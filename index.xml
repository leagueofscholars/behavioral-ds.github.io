<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Behavioral Data Science</title><link>https://www.behavioral-ds.ml/</link><atom:link href="https://www.behavioral-ds.ml/index.xml" rel="self" type="application/rss+xml"/><description>Behavioral Data Science</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2021</copyright><lastBuildDate>Tue, 09 Mar 2021 00:00:00 +0000</lastBuildDate><image><url>https://www.behavioral-ds.ml/img/logo.png</url><title>Behavioral Data Science</title><link>https://www.behavioral-ds.ml/</link></image><item><title>birdspotter: A toolkit for analyzing and labelling Twitter users</title><link>https://www.behavioral-ds.ml/softwaretool/birdspotter/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/softwaretool/birdspotter/</guid><description>&lt;img src="https://www.behavioral-ds.ml/img/birdspotter_logo.png" alt="Birdspotter Logo" width="200"/>
&lt;!-- Motivation -->
&lt;!-- Framing: Problem -> Solution -->
&lt;!-- Context -->
&lt;p>Social media platforms, although relatively new, host millions of users and billions of interactions daily. As tied as we are to these platforms, they profoundly impact our social institutions through phenomena such as disinformation, political polarization, and social bots.&lt;/p>
&lt;!-- Problem -->
&lt;p>Researchers are increasingly interested in trying to form an understanding of phenomena and their implications. Social scientists, political scientists, and data practitioners alike curate expansive datasets to combat these potentially adverse effects on our society; however, they lack the appropriate tooling.&lt;/p>
&lt;!-- Solution -->
&lt;p>&lt;code>birdspotter&lt;/code> is an &lt;strong>easy-to-use&lt;/strong> tool that models Twitter users&amp;rsquo; attributes and labels them. It comes prepackaged with a &lt;strong>state-of-the-art bot detector&lt;/strong> and an &lt;strong>influence quantification&lt;/strong> system based on tweet dynamics. &lt;code>birdspotter&lt;/code> features a generalized user labeler, which can be retrained easily with the engineered features to address a variety of use cases. Also, &lt;a href="http://birdspotter.ml/">birdspotter.ml&lt;/a> is a web application that can be utilized to explore datasets and derive a narrative around a dataset.&lt;/p>
&lt;p>In this post, I'll showcase the basic usage of &lt;code>birdspotter&lt;/code> and &lt;a href="http://birdspotter.ml/">birdspotter.ml&lt;/a>.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>The package can be installed in the canonical python way:&lt;/p>
&lt;pre>&lt;code class="language-{bash}">pip install birdspotter
&lt;/code>&lt;/pre>
&lt;h2 id="getting-a-dataset">Getting a dataset&lt;/h2>
&lt;p>The Twitter T&amp;amp;Cs restrict the sharing of tweet data directly online; however, they do allow the sharing of tweet-ids, which can be converted to full tweet data through a process called &lt;em>hydration&lt;/em>. Tools like &lt;a href="https://github.com/DocNow/twarc">twarc&lt;/a> can be used to hydrate a Tweet ID dataset. The resulting dataset will be in &lt;code>jsonl&lt;/code> (line delimited &lt;code>json&lt;/code>) format, which &lt;code>birdspotter&lt;/code> accepts directly.&lt;/p>
&lt;p>In the below examples, we use two datasets; a collection of COVID-19 related tweets from January 31st, 2020 [1], and a collection of tweets about politicians on Twitter [2].&lt;/p>
&lt;p>The politicians&amp;rsquo; dataset was acquired through the following process (and a similar process was taken for the COVID-19 dataset):&lt;/p>
&lt;pre>&lt;code class="language-{bash}">pip install twarc
wget http://twitterpoliticians.org./downloads/base/all_tweet_ids.csv
twarc hydrate all_tweet_ids.csv &amp;gt; tweets.jsonl
&lt;/code>&lt;/pre>
&lt;h2 id="basic-usage">Basic Usage&lt;/h2>
&lt;p>The code below imports the main class &lt;code>Birdspotter&lt;/code>, extracts the tweets from their standard format, labels the users with the default bot detector and influence, and reformats the retweet cascades into a tidier format.&lt;/p>
&lt;pre>&lt;code class="language-{python}">## Import birdspotter
from birdspotter import BirdSpotter
## Extracts the tweets from the raw jsonl [https://github.com/echen102/COVID-19-TweetIDs]
bs = BirdSpotter('covid19.jsonl')
## Uses the default bot labeller and influence quantification systems
bs.getLabeledUsers()
## Formats the retweet cascades, such that expected retweet structures can extracted
bs.getCascadesDataFrame()
## Access the botness labels and influence scores
bs.featureDataframe[['botness', 'influence']]
&lt;/code>&lt;/pre>
&lt;p>From here, the dataset is readily profile-able:&lt;/p>
&lt;pre>&lt;code class="language-{python}">botness_dist = sns.histplot(data=bs.featureDataframe, x=&amp;quot;botness&amp;quot;)
influence_eccdf = sns.ecdfplot(data=bs.featureDataframe, x=&amp;quot;influence&amp;quot;, complementary=True).set(xscale=&amp;quot;log&amp;quot;, yscale=&amp;quot;log&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://www.behavioral-ds.ml/img/covid_profile.png" alt="COVID Dataset Profile: (Left) The distribution of bot scores of users; (Right) The ECCDF of influence scores of users, showing a long-tailed (rich-gets-richer) paradigm">&lt;/p>
&lt;h2 id="the-visualizer">The visualizer&lt;/h2>
&lt;p>An alternative way to profile a dataset is the use &lt;a href="http://birdspotter.ml">&lt;code>birdspotter.ml&lt;/code>&lt;/a>, which facilitates dataset exploration and narrative construction.&lt;/p>
&lt;p>&lt;img src="https://www.behavioral-ds.ml/img/auspol_teaser.png" alt="birdspotter.ml visualizer: The various components shown include the scatterplot panel (Left), the user information panel (Top Right), and the retweet cascades panel (Bottom Right)">&lt;/p>
&lt;p>The visualizer features a scatterplot (on the left) of influence and botness for a sample of users and the population density. The colors represent the hashtags (a proxy for the topic) that the users most tweet about in the dataset. Users within the scatterplot are hoverable and selectable, and their information populates in the components on the right.&lt;/p>
&lt;p>The top right component shows information and metrics about the selected user and links the user's profile.&lt;/p>
&lt;p>The bottom right component shows the retweet cascades where a user has participated and highlights their participation. The points represent the follower counts (social capital) of users and their retweets/tweets&amp;rsquo; timing. The points are also hoverable and selectable.&lt;/p>
&lt;h2 id="customising-the-labeller">Customising the labeller&lt;/h2>
&lt;p>By default, the labeler is trained as a bot detection system, comparable to the state-of-the-art &lt;a href="https://botometer.osome.iu.edu/">&lt;code>botometer&lt;/code>&lt;/a>. Notable, &lt;code>birdspotter&lt;/code> is provided in an offline package and can be applied at scale, while &lt;code>botometer&lt;/code> is accessible only via an online API, which is often prohibitively rate-limited.&lt;/p>
&lt;p>&lt;code>birdspotter&lt;/code> is a versatile tool and can be utilized by practitioners for a variety of use-cases. For example, we could train the labeler to identify political leaning. This process is a bit involved, so we summarise it below;&lt;/p>
&lt;ol>
&lt;li>We hydrate some tweets from the Twitter Parlimentarian Database&lt;/li>
&lt;li>We filter the tweets to include only &lt;strong>Australian Politicians&lt;/strong>.&lt;/li>
&lt;li>We &lt;strong>label right-wing partied politicians positively&lt;/strong>, and others negatively (with &lt;code>bs_pol.getBotAnnotationTemplate&lt;/code> for example)&lt;/li>
&lt;li>We &lt;strong>retrain &lt;code>birdspotter&lt;/code>&lt;/strong> with these new labels and label all users (i.e., including users the politicians retweeted) using the new model&lt;/li>
&lt;/ol>
&lt;!-- ```{python class.source = 'fold-hide'} -->
&lt;!-- # This is the guts of the code; it does what is described above -->
&lt;!-- politicians = pd.read_csv('./full_member_info.csv', encoding='utf16') -->
&lt;!-- politicians_aus = politicians[politicians['country'] == 'Australia'] -->
&lt;!-- politicians_aus_available = politicians_aus[~politicians_aus['uid'].isnull()] -->
&lt;!-- def classify_party(party_id): -->
&lt;!-- mapping = { -->
&lt;!-- 464 : 1, # Liberal Party of Australia -->
&lt;!-- 465 : -1, # Australian Labor Party -->
&lt;!-- 467 : 1, # The Nationals -->
&lt;!-- 468 : 0, # Nick Xenophon Team -->
&lt;!-- 469 : -1, # Australian Greens -->
&lt;!-- 471 : np.nan, -->
&lt;!-- 475 : 1, # Katter's Australian Party -->
&lt;!-- } -->
&lt;!-- return mapping[party_id] -->
&lt;!-- politicians_aus_available['isright'] = politicians_aus_available['party_id'].apply(classify_party) -->
&lt;!-- politicians_aus_available['user_id'] = politicians_aus_available['uid'].astype(int).astype(str) -->
&lt;!-- politicians_aus_available = politicians_aus_available.set_index('user_id') -->
&lt;!-- with open('./tweets.jsonl', 'r') as rf, open('./aus_tweets.jsonl', 'w') as wf: -->
&lt;!-- for line in tqdm(rf): -->
&lt;!-- try: -->
&lt;!-- j = json.loads(line) -->
&lt;!-- if j['user']['id_str'] in politicians_aus_available['uid'].astype(int).astype(str).values: -->
&lt;!-- wf.write(json.dumps(j) + '\n') -->
&lt;!-- except Exception as e: -->
&lt;!-- print(j) -->
&lt;!-- print(e) -->
&lt;!-- break -->
&lt;!-- bs = BirdSpotter('aus_tweets.jsonl') -->
&lt;!-- bs.getLabeledUsers() -->
&lt;!-- bs.getCascadesDataFrame() -->
&lt;!-- with open('bs_aus_module.pk', 'wb') as wf: -->
&lt;!-- pk.dump(bs,wf, protocol=4) -->
&lt;!-- bs.featureDataframe['isright'] = politicians_aus_available['isright'] -->
&lt;!-- ground_truth = bs.featureDataframe[~bs.featureDataframe['isright'].isnull()][['isright']] -->
&lt;!-- ground_truth['isbot'] = ground_truth['isright'] == 1 -->
&lt;!-- ground_truth = ground_truth[~ground_truth.index.duplicated()] -->
&lt;!-- data = bs.featureDataframe.copy()[bs.featureDataframe.index.isin(ground_truth.index)] -->
&lt;!-- data = data[~data.index.duplicated()] -->
&lt;!-- del data['isright'] -->
&lt;!-- del data['botness'] -->
&lt;!-- del data['influence'] -->
&lt;!-- del data['cascade_membership'] -->
&lt;!-- data = data[list(data.columns[data.dtypes != 'object'])] -->
&lt;!-- data['isbot'] = ground_truth['isbot'].loc[data.index] -->
&lt;!-- with open('pol_training_data.pickle', 'wb') as wf: -->
&lt;!-- pk.dump(data,wf, protocol=4) -->
&lt;!-- from birdspotter import BirdSpotter -->
&lt;!-- import pickle as pk -->
&lt;!-- # bs_pol = BirdSpotter('aus_tweets.jsonl') -->
&lt;!-- with open('bs_aus_module.pk', 'rb') as rf: -->
&lt;!-- bs_pol = pk.load(rf) -->
&lt;!-- print("Loaded module") -->
&lt;!-- bs_pol.trainClassifierModel('pol_training_data.pickle') -->
&lt;!-- print("finished training") -->
&lt;!-- del bs_pol.featureDataframe['botness'] -->
&lt;!-- print("removed botness column") -->
&lt;!-- bs_pol.getBotness() -->
&lt;!-- bs_pol.getLabeledUsers() -->
&lt;!-- print("got labels") -->
&lt;!-- with open('pol_booster.pickle', 'wb') as wf: -->
&lt;!-- pk.dump(bs_pol.booster, wf, protocol=4) -->
&lt;!-- print("pickled booster") -->
&lt;!-- with open('aus_pol_bs_module.pickle', 'wb') as wf: -->
&lt;!-- pk.dump(bs_pol, wf, protocol=4) -->
&lt;!-- with open('pol_booster.pickle', 'wb') as wf: -->
&lt;!-- pk.dump(bs.booster, wf, protocol=4) -->
&lt;!-- ``` -->
&lt;!-- This is context: -->
&lt;!-- I want to start with the opportunity namely the analysis of large amounts of population data tranparently showing the interactions and discourse of people, allowing practictioners to model important applications in society. I also want to highlight the research issues which require investigation, namely social bots, misinformation, polarization, etc. -->
&lt;!-- This is content -->
&lt;!-- I then want to move into the problem, namely that there is a lack of tooling to analyse these huge swaths of data -->
&lt;pre>&lt;code class="language-{python}">bs_pol = BirdSpotter('aus_tweets.jsonl')
bs_pol.trainClassifierModel('pol_training_data.pickle')
bs_pol.getLabeledUsers()
&lt;/code>&lt;/pre>
&lt;p>On this limited of Australian politicians dataset, a 10-fold CV of &lt;code>birdspotter&lt;/code> garners an average AUC (Area under ROC) of 0.986.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>&lt;code>birdspotter&lt;/code> aims to democratize social analyzes that were once the domain of machine learning experts, generating insights and understanding of online phenomena and mitigating their potentially adverse effects on our society. This post shows how &lt;code>birdspotter&lt;/code> can be used in both a simple and advanced way to recover such insights.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] Chen, E. et al. 2020. Tracking social media discourse about the covid-19 pandemic: Development of a public coronavirus twitter data set. JMIR Public Health and Surveillance. 6, 2 (2020), e19273.&lt;/p>
&lt;p>[2] Vliet, L. van et al. 2020. The twitter parliamentarian database: Analyzing twitter politics across 26 countries. PloS one. 15, 9 (2020), e0237073.&lt;/p></description></item><item><title>Job Transitions in a Time of Automation and Labour Market Crises</title><link>https://www.behavioral-ds.ml/blogpost/job_transitions/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/blogpost/job_transitions/</guid><description>&lt;p>&lt;strong>Summary:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>We build a machine learning-based &lt;em>Job Transitions Recommender System&lt;/em> that can accurately predict the probability of transitioning between occupations. We showcase the system for workers forced to transition between jobs.&lt;/li>
&lt;li>The system is based on a novel data-driven method to measure the similarity between occupations based on their underlying skill profiles and real-time job ads.&lt;/li>
&lt;li>We also build a leading indicator of Artificial Intelligence adoption in Australian industries, outlining gaps, opportunities, and trends.&lt;/li>
&lt;li>For full technical details, please read the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>People are forced to change jobs as new technologies automate labour, production is moved abroad, and economic crises unfold. Successfully transitioning between jobs, however, requires leveraging current skills and acquiring others, which can falter if the skills gap is too large.&lt;/p>
&lt;p>In our latest research, &lt;a href="authors/nik-dawson/">Nik Dawson&lt;/a> and &lt;a href="authors/ma-rizoiu/">Marian-Andrei Rizoiu&lt;/a> (together with collaborator &lt;a href="https://research.unsw.edu.au/people/professor-mary-anne-williams">Mary-Anne Williams&lt;/a>) and I developed a novel method to measure the &amp;lsquo;distance&amp;rsquo; between &lt;em>sets of skills&lt;/em> using real-time job ads data. We then use these measures to build a recommender system that accurately predicts the probability of transitioning from one occupation to every other possible occupation. Intuitively, two occupations have a high probability of successfully transitioning when their skill sets are highly similar (i.e. the distance is small). For example, an Accountant has a high probability of transitioning to become a Financial Analyst because their skill sets are similar; whereas a Speech Therapist has a low transition probability to becoming a Financial Analyst as their skill sets are very different. This isn't to say that it's not possible. Rather, the skills gap is large, so the probability of successfully transitioning is diminished.&lt;/p>
&lt;h2 id="the-skill-space-method">The SKILL SPACE Method&lt;/h2>
&lt;p>&lt;strong>Distance between skills&lt;/strong>&lt;/p>
&lt;p>In order to measure the distance between occupations from their underlying skill sets, we first measure the distance between individual skills in job ads for each calendar year from 2012-2020. To achieve this bottom-up approach, we first use a method from Trade Economics, called ‘Revealed Comparative Advantage&amp;rsquo; (RCA), to identify how important an individual skill is to a job ad (i.e. comparative advantage). Then, after some normalisation, we calculate the pairwise similarity between every skill for each year. The image below shows the skill distance embeddings for the top 500 skills by posting frequency in 2018.&lt;/p>
&lt;p>&lt;img src="img/fig1A.png" alt="Figure 1A.">&lt;/p>
&lt;p>Here, each marker represents an individual skill that is coloured according to one of 13 clusters of highly similar skills. As seen in the Software Development cluster (see inset), highly similar skills cluster closely together, such as ‘Python&amp;rsquo; and ‘C++&amp;rsquo;. The skills map also provides useful insights, highlighting that specialised skills (such as ‘Software Development&amp;rsquo; and ‘Healthcare&amp;rsquo;) tend to lay toward the edges of the embedding, whereas more general and transferable skills lay toward the middle, acting as a ‘bridge&amp;rsquo; to specialist skills.&lt;/p>
&lt;p>&lt;strong>Distance between occupations&lt;/strong>&lt;/p>
&lt;p>Next, we use the pairwise skill distances to measure the distance between &lt;em>sets of skills&lt;/em>. In this example, we define sets of skills by their occupational groupings. But they can just as easily be defined by other groupings, such as companies, industries, or personalised skill sets. We calculate the distance between skill sets as the weighted average similarity between the individual skills in each set, where the weights correspond to the skill importance in their respective sets. The figure below visualises the distance between Australian occupations in 2018.&lt;/p>
&lt;p>&lt;img src="img/fig1B.png" alt="Figure 1B.">&lt;/p>
&lt;p>Each occupation is represented by a marker and coloured on a scale according to their automation susceptibility, as calculated by &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0040162516302244">Frey and Osborne&lt;/a> - dark blue represents low-risk probability and dark red shows high risk probability over the coming two decades. As seen in the magnified inset, similar occupations lie close together on the map. Further, occupations in low risk of automation tend to be characterised by non-routine, interpersonal, and/or high cognitive labour tasks; whereas occupations in high risk of automation tend to require routine, manual, and/or low cognitive labour tasks. For example, in the inset of the Figure above, a ‘Sheetmetal Trades Worker&amp;rsquo; is deemed to be at high risk of labour automation (82% probability) due to high levels of routine and manual labour tasks required by the occupation. However, the skill set demands of a ‘Sheetmetal Trades Worker&amp;rsquo; are highly similar to an ‘Industrial Designer&amp;rsquo;, which is considered at low risk of labour automation over the coming two decades (4% probability). Therefore, an ‘Industrial Designer&amp;rsquo; represents a transition opportunity for a ‘Sheetmetal Trades Worker&amp;rsquo; that leverages existing skills and protects against potential risks of technological labour automation.&lt;/p>
&lt;h2 id="constructing-a-job-transitions-recommender-system">Constructing a Job Transitions Recommender System&lt;/h2>
&lt;p>The SKILL SPACE Method described above achieves high levels of accuracy in predicting job transitions. However, these are symmetric measures and we know that &lt;strong>job transitions are &lt;em>asymmetric&lt;/em>&lt;/strong> - it is more difficult to transition between jobs in one direction than the other. Therefore, transitions are determined by more than the symmetric distance between skill sets; other factors, such as educational requirements and experience demands, contribute to these asymmetries.&lt;/p>
&lt;p>We account for the asymmetries between job transitions by training a machine learning classifier model that combines the SKILL SPACE distance measures with other labour market variables from job ads data and employment statistics (see the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a> for full details). Our machine learning model is trained against a dataset of ‘actual&amp;rsquo; (i.e. real life) job transitions from an Australian longitudinal household survey. We then apply the trained model to predict the probability for every possible occupational transition in the dataset - described as the transition probability between a ‘source&amp;rsquo; and a ‘target&amp;rsquo; occupation. This creates the &lt;em>Transitions Map&lt;/em>, for which a subset of 20 occupations can be seen in the figure below.&lt;/p>
&lt;p>&lt;img src="img/fig2.png" alt="Figure 2.">&lt;/p>
&lt;p>The coloured heatmap shows the transition probabilities (‘source&amp;rsquo; occupations are in columns and ‘targets&amp;rsquo; are in rows). Dark blue represents higher transition probabilities and lighter blue shows lower probabilities, where the asymmetries between occupation pairs are clearly observed. For example, a ‘Finance Manager&amp;rsquo; has a higher probability of transitioning to become an ‘Accounting Clerk&amp;rsquo; than the reverse direction. Moreover, transitioning to some occupations is generally easier (for example, ‘Bar Attendants and Baristas&amp;rsquo;) than others (‘Life Scientists&amp;rsquo;). The dendrogram (the lines on the left and top of the chart) illustrates the hierarchical clusters of occupations where there is a clear divide between service-oriented professions and manual labour occupations.&lt;/p>
&lt;p>Our model achieves high levels of performance, &lt;strong>accurately predicting 76% of occupational transitions&lt;/strong>.&lt;/p>
&lt;h2 id="recommending-jobs-and-skills">Recommending Jobs and Skills&lt;/h2>
&lt;p>The &lt;em>Transitions Map&lt;/em> provides the basis for making qualified job transition recommendations. We call this the &lt;em>Job Transitions Recommender System&lt;/em>. In the figure below, we highlight the example of `Domestic Cleaners&amp;rsquo;, an occupation that has experienced significant declines in labour demand and employment levels during COVID-19 in Australia.&lt;/p>
&lt;p>&lt;img src="img/fig4.png" alt="Figure 4.">&lt;/p>
&lt;p>First, we use the &lt;em>Transitions Map&lt;/em> to recommend the occupations with the highest transition probabilities; these are the named occupations on the right side of the flow diagram, ordered in descending order of transition probability. Segment widths show the labour demand for each of the recommended occupations during the beginning of the COVID-19 period (measured by posting frequency). The segment colours represent the percentage change of posting frequency during March and April 2019 compared to the same months in 2020; dark red indicates a big decrease in job ad posts and dark blue indicates a big increase. The first six transition recommendations for ‘Domestic Cleaners&amp;rsquo; all experienced negative demand, which is unsurprising given that ‘non-essential&amp;rsquo; services were restricted in Australia during this period. However, the seventh recommendation, ‘Aged and Disabled Carers&amp;rsquo;, had significantly grown in demand during the beginning of the COVID-19 period and there was a high number of jobs advertised. Given that it is generally favorable to transition to high demand jobs, we selected ‘Aged and Disabled Carers&amp;rsquo; as the target occupation for this example.&lt;/p>
&lt;p>&lt;strong>Skill recommendations for target occupations&lt;/strong>&lt;/p>
&lt;p>We then take the &lt;em>Job Transitions Recommender System&lt;/em> a step further by incorporating skill recommendations. Transitioning to a new occupation generally requires developing new skills under time and resource constraints. Therefore, workers must prioritise which skills to develop. We argue that a worker should invest in developing a skill when (1) the &lt;strong>skill is important to the target occupation&lt;/strong> &lt;em>and&lt;/em> (2) the &lt;strong>distance to acquire the skill is large&lt;/strong> (that is, it is relatively difficult to acquire). In the case of the ‘Domestic Cleaner&amp;rsquo; in the figure above, the top recommended skills to assist in the transition to the ‘Aged and Disabled Carer&amp;rsquo; occupation are specialised patient care skills, such as ‘Patient Hygiene Assistance&amp;rsquo;. Conversely, the reasons &lt;em>not&lt;/em> to develop a skill are when (1) the &lt;strong>skill is not important&lt;/strong> &lt;em>or&lt;/em> (2) the &lt;strong>distance is small to the target occupation&lt;/strong>. The figure shows that while some ‘Aged and Disabled Carer&amp;rsquo; jobs require ‘Business Analysis&amp;rsquo; and ‘Finance&amp;rsquo; skills, these skills are of low importance for the ‘Aged and Disabled Carer&amp;rsquo; occupation, so they should not be prioritised. Similarly, skills such as ‘Ironing&amp;rsquo; and ‘Laundry&amp;rsquo; are required by ‘Aged and Disabled Carer&amp;rsquo; jobs but the distance is small, so it is likely that either a ‘Domestic Cleaner&amp;rsquo; already possesses these skills or they can easily acquire them.&lt;/p>
&lt;h2 id="developing-a-leading-indicator-of-ai-adoption">Developing a Leading Indicator of AI Adoption&lt;/h2>
&lt;p>The SKILL SPACE method can also be flexibly applied to other adjacent problems, such as identifying the extent of specific skill gaps of firms within industries and potential adoption of new technologies. Here, we develop a leading indicator for emerging technology adoption and potential labour market disruptions based on skills data, using Artificial Intelligence (AI) as an example. We select AI because of its potential impacts on transforming labour tasks and accelerating job transitions. By applying SKILL SPACE, we were able to measure the yearly similarities between an adaptive set of AI skills against each of the 19 Australian industries from 2013-2019 (see the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a> for technical details). As industry skill sets become more similar to AI skills, the skills gap is diminished and firms within industries are more likely to possess the skills to make productive use of AI technologies. The growth of AI skill similarity within industries is shown by the coloured areas of the radar chart below.&lt;/p>
&lt;p>&lt;img src="img/fig5.png" alt="Figure 5.">&lt;/p>
&lt;p>All industries have increased their similarity levels to AI skills. This highlights the growing importance of AI skills across the Australian labour market. However, the rates of similarity are unequally distributed. Some industries, such as ‘Finance and Insurance Services&amp;rsquo; and ‘Information Media and Telecommunications&amp;rsquo; command much higher rates of AI skill similarity. This indicates that not only are firms within these industries increasingly demanding AI skills but also that the AI skills gaps within these industries are much smaller. Therefore, it is likely that firms within these industries are adopting AI and making productive use of these technologies.&lt;/p>
&lt;p>Also noteworthy are the differences in growth rates toward AI skill similarity. As the figure above clearly shows, AI skill similarity has rapidly grown for some industries and more modestly for others. For instance, ‘Retail Trade&amp;rsquo; has experienced the highest levels of growth in similarity to AI skills, increasing by 407% from 2013 to 2019. The majority of this growth has occurred recently, which coincides with the launch of Amazon Australia in 2017. Since then, Amazon has swiftly hired thousands in Australia. This indicator can assist policy-makers and businesses to robustly monitor the growth of AI skills (or other emerging technologies), which acts as a proxy for AI adoption within industries.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>While the future of work remains unclear, change is inevitable. New technologies, economic crises, and other factors will continue to shift labour demands causing workers to move between jobs. If labour transitions occur efficiently, significant productivity and equity benefits arise at all levels of the labour market; if transitions are slow, or fail, significant costs are borne to both the State and the individual. Therefore, it's in the interests of workers, firms, and governments that labour transitions are efficient and effective. The methods and systems we put forward here could significantly improve the achievement of these goals.&lt;/p>
&lt;p>For the full technical details, please read the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>&lt;/p>
&lt;p>We thank &lt;a href="https://www.linkedin.com/in/bleditaska?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3Bvr7rCfoNTVGsSR9hhmd%2FhQ%3D%3D">Bledi Taska&lt;/a> and &lt;a href="https://ca.linkedin.com/in/dmiskulin">Davor Miskulin&lt;/a> from Burning Glass Technologies for generously providing the job advertisements data for this research and for their valuable feedback. We also thank &lt;a href="https://fr.linkedin.com/in/broecke-stijn-36ba2048">Stijn Broecke&lt;/a> and other colleagues from the OECD for their ongoing input and guidance in the development of this work.&lt;/p></description></item><item><title>User Analysis on reshare cascades about COVID-19</title><link>https://www.behavioral-ds.ml/blogpost/user_analysis/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/blogpost/user_analysis/</guid><description>&lt;p>We demonstrate in this blog post a tutorial on applying the tools for analyzing online information diffusions about Twitter users, &lt;a href="https://github.com/behavioral-ds/BirdSpotter">&lt;code>birdspotter&lt;/code>&lt;/a> and &lt;a href="https://github.com/behavioral-ds/evently">&lt;code>evently&lt;/code>&lt;/a>.&lt;/p>
&lt;h2 id="dataset">Dataset&lt;/h2>
&lt;p>In this tutorial, we apply two tools for analyzing Twitter users, on a COVID-19 retweet dataset. The dataset
is curated by Chen, et al. One can obtain a copy of the tweet IDs from
their &lt;a href="https://github.com/echen102/COVID-19-TweetIDs">project&lt;/a>. We
only use the 31st of Janury sample of the whole dataset for
demonstration purpose. The tweets can be recovered by &lt;code>hydration&lt;/code>
from their IDs. We note that some tweets might have been deleted and in
the end we manage to get 69.2% (1,489,877) of the original tweets.&lt;/p>
&lt;h2 id="tools">Tools&lt;/h2>
&lt;p>While &lt;code>BirdSpotter&lt;/code> captures the social influence and botness of Twitter
users, &lt;code>evently&lt;/code> specifically models the temporal dynamics of online
information diffusion. We leverage information provided by the tools to
study the users in the COVID19 dataset.&lt;/p>
&lt;pre>&lt;code class="language-r">library(evently)
library(reticulate)
birdspotter &amp;lt;- import('birdspotter')
&lt;/code>&lt;/pre>
&lt;h2 id="preprocessing-tweets">Preprocessing tweets&lt;/h2>
&lt;p>At this step, we seek to extract diffusion cascades from the &lt;code>COVID-19&lt;/code>
dataset for analyzing user influence and botness. A diffusion cascade
consist of an initial tweet posted by a Twitter user and followed then
by a sereis of retweets. A function provided by &lt;code>evently&lt;/code> allows one to
obtain cascades from JSON formatted raw tweets. On the other hand, we
initialize a &lt;code>BirdSpotter&lt;/code> instance and compute the influence and
botness scores for all users in the
dataset.&lt;/p>
&lt;pre>&lt;code class="language-r">cascades &amp;lt;- parse_raw_tweets_to_cascades('corona_2020_01_31.jsonl', keep_user = T, keep_absolute_time = T)
bs &amp;lt;- birdspotter$BirdSpotter('corona_2020_01_31.jsonl')
labeled_users &amp;lt;- bs$getLabeledUsers()[, c('user_id', 'botness', 'influence')]
&lt;/code>&lt;/pre>
&lt;p>As we cannot publish &lt;code>corona_2020_01_31.jsonl&lt;/code> due to Twitter TOC, we
have stored the results and load them below&lt;/p>
&lt;pre>&lt;code class="language-r">load('corona_2020_01_31.rda')
labeled_users &amp;lt;- read.csv('corona_31_botness_influence.csv', stringsAsFactors = F,
colClasses=c(&amp;quot;character&amp;quot;,rep(&amp;quot;numeric&amp;quot;,3)))
&lt;/code>&lt;/pre>
&lt;p>We note that all user IDs have been encrypted. After obtaining the
results, let’s first conduct some simple measurements on users and
cascades.&lt;/p>
&lt;pre>&lt;code class="language-r">library(ggplot2)
# check the density of these two values
mean_bot &amp;lt;- mean(labeled_users$botness, na.rm = T)
ggplot(labeled_users, aes(botness)) +
stat_density(geom = 'line') +
geom_vline(xintercept = mean_bot, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_bot, y = 2, label= sprintf('mean: %s', round(mean_bot, 2))), color= 'red', angle=90, vjust=-0.11)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_inf &amp;lt;- mean(labeled_users$influence)
ggplot(labeled_users) +
stat_ecdf(aes(influence, 1 - ..y..)) +
scale_x_log10() +
scale_y_log10() +
ylab('CCDF') +
geom_vline(xintercept = mean_inf, linetype=2, color = 'red') +geom_text(data=data.frame(), aes(x = mean_inf, y = 1e-3, label= sprintf('mean: %s', round(mean_inf, 2))), color= 'red', angle=90, vjust=-0.11)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-2.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_value &amp;lt;- mean(sapply(cascades, nrow))
ggplot(data.frame(size = sapply(cascades, nrow))) +
stat_ecdf(aes(size, 1 - ..y..)) +
scale_x_log10() + scale_y_log10() +
geom_vline(xintercept = mean_value, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) +
xlab('cascade size') +
ylab('CCDF')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-3.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_value2 &amp;lt;- mean(sapply(cascades, function(c) c$time[nrow(c)]))
ggplot(data.frame(time = sapply(cascades, function(c) c$time[nrow(c)]))) +
stat_ecdf(aes(time, 1 - ..y..)) +
scale_x_continuous(trans = 'log1p', breaks = c(0, 100, 10000, 1000000), labels = c('0', '1e2', '1e4', '1e6')) +
scale_y_log10() +
geom_vline(xintercept = mean_value2, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_value2, y = 1e-3, label= sprintf('mean: %s', round(mean_value2, 2))), color= 'red', angle=90, vjust=-0.11) +
xlab('cascade final event time')+
ylab('CCDF')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-4.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_value &amp;lt;- mean(labeled_users$activity)
ggplot(data.frame(size = labeled_users$activity)) +
stat_ecdf(aes(size, 1 - ..y..)) +
scale_x_log10() +
scale_y_log10() +
geom_vline(xintercept = mean_value, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('user activity')+ ylab('CCDF')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-5.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="retrain-the-bot-detector">Retrain the bot detector&lt;/h2>
&lt;p>If one find the botness scores are not accurate, &lt;code>birdspotter&lt;/code> provides
a relabeling tool and a retrain API to learn from the given relabeled
dataset&lt;/p>
&lt;pre>&lt;code class="language-r"># output a file for mannual labeling
bs$getBotAnnotationTemplate('users_to_label.csv')
# Once annotated the botness detector can be trained with
bs$trainClassifierModel('users_to_label.csv')
&lt;/code>&lt;/pre>
&lt;h2 id="fit-user-posted-cacsades-with-evently">Fit user posted cacsades with &lt;code>evently&lt;/code>&lt;/h2>
&lt;p>We model a group of cascades initiated by a particular user jointly and
treat the fitted model as a characterization of the user. In this
example, we select two users for comparison.&lt;/p>
&lt;pre>&lt;code class="language-r">selected_users &amp;lt;- c('369686755237813560', '174266868073402929')
# fit Hawkes process on cascades initiated by the selected users
user_cascades_fitted &amp;lt;- lapply(selected_users, function(user) {
# select cascades that are initiated by the &amp;quot;selected_user&amp;quot;
selected_cascades &amp;lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades)
# obtain the observation times;
# note 1580515200 is 1st Feb when the observation stopped
# as we only observed until the end of 31st Jan
times &amp;lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1])
# fit a model on the selected cascades;
fit_series(data = selected_cascades, model_type = 'mPL', observation_time = times, cores = 10)
})
user_cascades_SEISMIC_fitted &amp;lt;- lapply(selected_users, function(user) {
selected_cascades &amp;lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades)
times &amp;lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1])
fit_series(data = selected_cascades, model_type = 'SEISMIC',
observation_time = times)
})
# check the fitted kernel functions
plot_kernel_function(user_cascades_fitted) +
scale_color_discrete(labels = c(&amp;quot;@BobOngHugots&amp;quot;, &amp;quot;@Jaefans_Global&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-6-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>The plot shows the fitted kernel functions of these two users which
reflect their time-decaying influence of attracting followers to reshare
their posts. We then demonstrate how to simulate new cascades&lt;/p>
&lt;pre>&lt;code class="language-r">set.seed(134841)
user_magnitude &amp;lt;- Filter(function(cascade) cascade$user[[1]] == selected_users[[1]], cascades)[[1]]$magnitude[1]
# simulate a new cascade from @BobOngHugots
sim_cascade &amp;lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude)
plot_event_series(cascade = sim_cascade, model = user_cascades_fitted[[1]])
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-7-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">selected_cascade &amp;lt;- Filter(function(cascade) cascade$user[1] == selected_users[[1]], cascades)[[1]]
selected_time &amp;lt;- user_cascades_fitted[[1]]$observation_time[1]
# simulate a cascade with a &amp;quot;selected_cascade&amp;quot; from @BobOngHugots
sim_cascade &amp;lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude,
init_history = selected_cascade)
sprintf('%s new events simulated after cascade',
nrow(sim_cascade[[1]]) - nrow(selected_cascade))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;25 new events simulated after cascade&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">predict_final_popularity(user_cascades_fitted[[1]],
selected_cascade, selected_time)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 458.303
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># predict with SEISMIC model, assume we have fitted the SEISMIC model
predict_final_popularity(user_cascades_SEISMIC_fitted[[1]],
selected_cascade, selected_time)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 729.923
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">get_branching_factor(user_cascades_fitted[[1]])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.7681281
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">get_viral_score(user_cascades_fitted[[1]])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 7.407763
&lt;/code>&lt;/pre>
&lt;h2 id="visualize-users-in-a-latent-space">Visualize users in a latent space&lt;/h2>
&lt;p>We show a visualization of top 300 users posted most tweets using the
features returned by &lt;code>evently&lt;/code> along with the botness and influence
scores from &lt;code>birdspotter&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-r"># obtain observation times here again
times &amp;lt;- 1580515200 - sapply(cascades, function(cas) cas$absolute_time[1])
# indicate the grouping of each cascade with the user who started the cascade
names(cascades) &amp;lt;- sapply(cascades, function(cas) cas$user[1])
# fit Hawkes processes on all cascades first
fitted_corona &amp;lt;- group_fit_series(cascades, model_type = 'mPL', observation_time = times)
&lt;/code>&lt;/pre>
&lt;p>The fitting procedure takes quite long so we again load the pre-fitted
models here&lt;/p>
&lt;pre>&lt;code class="language-r">load('fitted_models.rda')
# choose the top 300 users who started most cacsades
selected_users &amp;lt;- labeled_users$user_id[labeled_users$user_id %in%
names(sort(sapply(fitted_corona, length), decreasing = T)[seq(300)])]
# gather the stats for these users
user_influences &amp;lt;- labeled_users$influence[labeled_users$user_id %in% selected_users]
user_botness &amp;lt;- labeled_users$botness[labeled_users$user_id %in% selected_users]
fitted_corona_selected &amp;lt;- fitted_corona[selected_users]
# get the features
features &amp;lt;- generate_features(fitted_corona_selected)
# compute distances between users using manhattan distance
features &amp;lt;- features[, -1] # remove the user id column
distances &amp;lt;- dist(features, method = 'manhattan')
library(tsne)
positions &amp;lt;- tsne(distances, k = 2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## sigma summary: Min. : 0.34223375605395 |1st Qu. : 0.457223801885988 |Median : 0.489891425900637 |Mean : 0.500483006369232 |3rd Qu. : 0.538593613780411 |Max. : 0.676779919259545 |
## Epoch: Iteration #100 error is: 14.1961110881254
## Epoch: Iteration #200 error is: 0.490122133064818
## Epoch: Iteration #300 error is: 0.474257867010761
## Epoch: Iteration #400 error is: 0.472067779170087
## Epoch: Iteration #500 error is: 0.471844181155159
## Epoch: Iteration #600 error is: 0.471798834134577
## Epoch: Iteration #700 error is: 0.471783207059971
## Epoch: Iteration #800 error is: 0.471632929621924
## Epoch: Iteration #900 error is: 0.47087861882558
## Epoch: Iteration #1000 error is: 0.470873765976829
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">df &amp;lt;- data.frame(x = positions[,1], y = positions[,2],
influence = user_influences, botness = user_botness)
df &amp;lt;- cbind(df, data.frame(botornot = ifelse(df$botness &amp;gt; 0.6, 'Bot', 'Not Bot')))
ggplot(df, aes(x, y, color = influence, shape = botornot, size = botornot)) +
geom_point() +
scale_shape_manual(values = c(15,1)) +
scale_size_manual(values = c(1.5, 1.2)) +
scale_color_gradient(low = '#56B1F7', high = '#132B43', trans = 'log10') +
theme_void() + labs(size = NULL, shape = NULL) +
theme(legend.direction = 'horizontal', legend.position = c(0.8, 0.2),
legend.key.size = unit(.3, 'cm'), legend.text = element_text(size = 6),
legend.title = element_text(size = 6), legend.spacing = unit(.05, 'cm'))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-10-1.png" alt="">&lt;!-- -->&lt;/p></description></item><item><title>Causal Inference: A basic taster</title><link>https://www.behavioral-ds.ml/blogpost/causal_inference_taster/</link><pubDate>Thu, 12 Nov 2020 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/blogpost/causal_inference_taster/</guid><description>
&lt;script src="index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
&lt;script src="index_files/anchor-sections/anchor-sections.js">&lt;/script>
&lt;link rel="stylesheet" href="tufte.css" type="text/css" />
&lt;p>Most statistics students will be familiar with the phrase “correlation isn’t causation,” however, this doesn’t feature strongly in the remainder of their educations. To overcome this hurdle, the researchers’ best practice in experimental design is the randomized controlled trial. However, there are only specific experiments that we can perform. For example, to test the whether smoking causes cancer, we can’t force subjects to smoke. &lt;label for="tufte-mn-" class="margin-toggle">⊕&lt;/label>&lt;input type="checkbox" id="tufte-mn-" class="margin-toggle">&lt;span class="marginnote">In the 1950s the tobacco companies argued that there could be some confounding factor (a gene) which smokers and lung cancer patients shared.&lt;/span> In general, restricting ourselves to experimental studies to determine causation is incredibly limiting (especially for data scientists). We want to make the same causal conclusions from observational studies and those from experimental studies. We can do that by studying causal inference.&lt;/p>
&lt;div id="simpsons" class="section level2">
&lt;h2>Simpson’s Paradox&lt;/h2>
An example of the importance of understanding causal relationships is given by Simpson’s Paradox &lt;span class="citation">(&lt;a href="#ref-simpson1951interpretation" role="doc-biblioref">Simpson 1951&lt;/a>)&lt;/span>, which describes a peculiar phenomenon that can present in data sets, where a correlation between two variables is present in one direction but reverses in each stratum of the data. The paradox expressed best through an example:
&lt;label for="tufte-mn-" class="margin-toggle">⊕&lt;/label>&lt;input type="checkbox" id="tufte-mn-" class="margin-toggle">&lt;span class="marginnote">This appears to suggest that the more someone exercises, the higher their cholesterol is! This is absurd!&lt;/span>
&lt;div class="figure">&lt;span id="fig:simpsons-before">&lt;/span>
&lt;img src="./index_files/figure-html/simpsons-before-1.png" alt="The results of an experiment, where x-axis represents how much exercise an individual does in hours, and y-axis represents cholestral measurment for the same individual." width="672" />
&lt;p class="caption">
Figure 1: The results of an experiment, where x-axis represents how much exercise an individual does in hours, and y-axis represents cholestral measurment for the same individual.
&lt;/p>
&lt;/div>
Figure &lt;a href="#fig:simpsons-before">1&lt;/a> shows a positive correlation in an experiment that measures individuals’ exercise per week and cholesterol. At first glance, this seems absurd, but when we partition the data by another causal variable, this seems reasonable:
&lt;div class="figure">&lt;span id="fig:simpsons-after">&lt;/span>
&lt;img src="index_files/figure-html/simpsons-after-1.png" alt="The same results as the experiment above, partioned by age" width="672" />
&lt;p class="caption">
Figure 2: The same results as the experiment above, partioned by age
&lt;/p>
&lt;/div>
&lt;p>&lt;label for="tufte-mn-" class="margin-toggle">⊕&lt;/label>&lt;input type="checkbox" id="tufte-mn-" class="margin-toggle">&lt;span class="marginnote">(Also note, we have fabricated the data, although these relationships are quite plausible)&lt;/span>
Understanding the full causal story is essential. Without an entire causal narrative, we might recommend inappropriate interventions; for example, a doctor might prescribe less exercise to reduce cholesterol in the case above.&lt;/p>
&lt;p>To deduce such causal stories, we need to apply the methodology of causal inference.&lt;/p>
&lt;/div>
&lt;div id="structural-equation-models-and-causal-graphs" class="section level2">
&lt;h2>Structural Equation Models and Causal Graphs&lt;/h2>
&lt;p>A structural equation model (SEM) is a set of equations representing the relationship between variables. For example, the equations which generated the data from &lt;a href="#simpsons">the Simpson’s paradox example&lt;/a>, are given as:
&lt;span class="math display">\[
\begin{align*}
age &amp;amp;= U_1 \\
exercise &amp;amp;= \frac{1}{13}*age + U_2 \\
cholesteral &amp;amp;= -4*exercise + age + U_3
\end{align*}
\]&lt;/span>
We can think of &lt;span class="math inline">\(U_1\)&lt;/span>, &lt;span class="math inline">\(U_2\)&lt;/span>, and &lt;span class="math inline">\(U_3\)&lt;/span> as specific unobserved exogenous variables of an individual, which generate their endogenous variables (something like error terms).&lt;/p>
&lt;p>A causal graph is a DAG which describes the existence of relationships between variables in a model. An edge &lt;code>x -&amp;gt; y&lt;/code> represents the relationship &lt;code>x&lt;/code> directly causes &lt;code>y&lt;/code>. Consequently, causal graphs can represent SEMs:
&lt;img src="index_files/figure-html/unnamed-chunk-1-1.png" width="672" />&lt;/p>
&lt;p>Indeed this graph shows how age confounds the effect of exercise on cholesterol.&lt;/p>
&lt;/div>
&lt;div id="do-calculus" class="section level2">
&lt;h2>Do-calculus&lt;/h2>
&lt;p>&lt;span class="citation">&lt;a href="#ref-pearl1995causal" role="doc-biblioref">Pearl&lt;/a> (&lt;a href="#ref-pearl1995causal" role="doc-biblioref">1995&lt;/a>)&lt;/span> outline a method to remove this confounding (and other similar scenarios) using do-calculus. Outlining the specifics of do-calculus is beyond the scope of this blog post (but for interested readers, we suggest &lt;span class="citation">(&lt;a href="#ref-pearl2016causal" role="doc-biblioref">Pearl, Glymour, and Jewell 2016&lt;/a>)&lt;/span>). In brief, do-calculus introduces the &lt;span class="math inline">\(do()\)&lt;/span> operator, which acts as an intervention and fixes a variable to a particular constant. For example, consider a similar binary situation to &lt;a href="#simpsons">the Simpson’s paradox example&lt;/a>, where &lt;em>exer&lt;/em> is a binary variable true if the individual is active, &lt;em>chol&lt;/em> is a binary variable true if the individual has high cholesterol, and &lt;em>age&lt;/em> is a binary variable true if the individual is over 60.&lt;/p>
&lt;pre class="r">&lt;code>bin_simpsons_data &amp;lt;- simpsons_data %&amp;gt;%
mutate(age = age &amp;gt; 60) %&amp;gt;% # Binarize the age, so those over 60 are True, and under 60 are False
mutate(exer = exercise&amp;gt;mean(exercise)) %&amp;gt;% # Binarize the exercise level, so those above the average are True, and under are False
mutate(chol = cholesteral&amp;gt;mean(cholesteral)) # Binarize the cholesteral level, so those above the average are True, and under are False&lt;/code>&lt;/pre>
&lt;p>We ask the same experimental question; does exercise reduce cholesterol. A naive approach would be to compute the effect as &lt;span class="math inline">\(P(chol | exer = 1) - P(chol | exer = 0)=\)&lt;/span> 0.168, where &lt;span class="math inline">\(P(chol | exer)\)&lt;/span> is computed by filtering the data according to &lt;em>exer&lt;/em>. Taking this approach, we would erroneously observe that the effect was positive since those who exercise more are also old and more likely to have high cholesterol.&lt;/p>
&lt;p>The experimental best practice approach would be to perform a randomized controlled trial (RCT). A random selection of individuals are assigned to &lt;em>do&lt;/em> a high exercise regiment and the others &lt;em>do&lt;/em> a low exercise regiment (regardless of age). The RCT implicitly removes the natural tendency of exercise to vary with age and allows researchers to observe the causal effect of exercise on cholesterol. When using data generated in such a fashion, increases/decreases in the probability of having high cholesterol caused by exercise are given by &lt;span class="math inline">\(P_{RCT}(chol | exer = 1) - P_{RCT}(chol | exer = 0)\)&lt;/span>. This metric is known as the Average Causal Effect (ACE), sometimes called the Average Treatment Effect. Note that by conditioning on &lt;span class="math inline">\(exer=x\)&lt;/span>, with data generated by an RCT, researchers are essentially limiting the data used to estimate &lt;span class="math inline">\(P_{RCT}(chol | exer = x)\)&lt;/span>, to individuals who were &lt;strong>forced&lt;/strong> to &lt;em>do&lt;/em> an exercise regiment &lt;span class="math inline">\(x\)&lt;/span>. The &lt;em>do&lt;/em> here represents forcing individuals to take an intervention value, regardless of their natural tendency, and this is captured by the &lt;span class="math inline">\(do()\)&lt;/span> operator. In this case, &lt;span class="math inline">\(P(chol | do(exer = x)) = P_{RCT}(chol | exer = x)\)&lt;/span>, since the data was generated with an RCT. However, RCTs can be prohibitively expensive (both in time and money) and might not be necessary to tease out a causal effect.&lt;/p>
&lt;p>We would still like to estimate the ACE, &lt;span class="math inline">\(P(chol | do(exer = 1)) - P(chol | do(exer = 0))\)&lt;/span>, by using data that wasn’t generated from an RCT. By using the &lt;span class="math inline">\(do()\)&lt;/span> operator here, we aim to disassociate &lt;em>exer&lt;/em> from its natural tendency with &lt;em>age&lt;/em> and effectively perform a graph surgery:&lt;/p>
&lt;p>&lt;img src="index_files/figure-html/unnamed-chunk-4-1.png" width="672" />&lt;/p>
&lt;p>&lt;span class="citation">&lt;a href="#ref-pearl2016causal" role="doc-biblioref">Pearl, Glymour, and Jewell&lt;/a> (&lt;a href="#ref-pearl2016causal" role="doc-biblioref">2016&lt;/a>)&lt;/span> provide an adjustment formula for just this scenario:
&lt;span class="math display">\[
P(y|do(x)) = \sum_z \frac{P(X=x, Y=y, PA=z)}{P(X=x| PA=z)}
\]&lt;/span>
where &lt;span class="math inline">\(X\)&lt;/span> represents the variable we are acting on, &lt;span class="math inline">\(Y\)&lt;/span> the variable we measure results from, and &lt;span class="math inline">\(PA\)&lt;/span> the parents of &lt;span class="math inline">\(X\)&lt;/span> and &lt;span class="math inline">\(Y\)&lt;/span> or more generally any nodes that satisfy the back-door criterion (which we will introduce later). Note this allows us to derive the causal effect, as if we had generated data with an RCT, using only probabilities estimated from data not generated by an RCT.&lt;/p>
&lt;p>As such we compute our ACE for the binary scenario:&lt;/p>
&lt;pre class="r">&lt;code># The Joint Distribution P(age, exer, chol) i.e. P(x,y,z)
p_aec &amp;lt;- bin_simpsons_data %&amp;gt;%
count(age, exer, chol) %&amp;gt;%
mutate(freq = n/sum(n))
# The Marginal Distribution P(age) i.e. P(z)
p_a &amp;lt;- bin_simpsons_data %&amp;gt;%
count(age) %&amp;gt;%
mutate(freq = n/sum(n))
# The Marginal Distribution P(age, exer) i.e. P(x, z)
p_ea &amp;lt;- bin_simpsons_data %&amp;gt;%
count(age, exer) %&amp;gt;%
mutate(freq = n/sum(n))
# The Conditional Mariginal Distribution P(exer | age) i.e. P(x | z)
p_e_a &amp;lt;- p_a %&amp;gt;%
right_join(p_ea, by=&amp;quot;age&amp;quot;) %&amp;gt;%
mutate(freq = freq.y/freq.x) %&amp;gt;%
select(age, exer, freq)
# The Intervention Distribution P(chol | do(exer)) i.e. P(y | do(x))
probabilities &amp;lt;- data.table(p_aec %&amp;gt;%
left_join(p_e_a, by=c(&amp;quot;age&amp;quot;, &amp;quot;exer&amp;quot;)) %&amp;gt;%
mutate(freq = freq.x/freq.y) %&amp;gt;%
select(age, exer, chol, freq) %&amp;gt;%
filter(chol) # We are only concerned with what cause high cholestral
)
# The average causal effect of exer on chol
ACE &amp;lt;- sum(probabilities[exer==T, freq]) - sum(probabilities[exer==F, freq]) &lt;/code>&lt;/pre>
&lt;p>This procedure leads to a negative ACE of -0.175, which shows the causal effect of going from high to low exercise on the probability of getting high cholesterol.&lt;/p>
&lt;p>A natural question that follows from this example is, under what conditions can we use such adjustments to achieve an identifiable causal effect.&lt;/p>
&lt;/div>
&lt;div id="d-seperation" class="section level2">
&lt;h2>d-seperation&lt;/h2>
&lt;p>To understand common scenarios where the effect of variable &lt;span class="math inline">\(X\)&lt;/span> on &lt;span class="math inline">\(Y\)&lt;/span> is identifiable within a causal graph, we must first introduce the concept of d-separation, also known as blocking. A pair of variable &lt;span class="math inline">\(X\)&lt;/span> and &lt;span class="math inline">\(Y\)&lt;/span> are said to be blocked if they are conditionally independent, given a set of nodes &lt;span class="math inline">\(Z\)&lt;/span>. There are three graph types, which are essential for blocking:&lt;/p>
&lt;p>&lt;img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" />&lt;/p>
&lt;p>In the chain scenario, &lt;span class="math inline">\(X \sim Y\)&lt;/span> is blocked by conditioning on &lt;span class="math inline">\(Z={M}\)&lt;/span>. This is sometimes refered to as the mediation scenario, which we will address further in &lt;a href="#front-door">the front-door criterion&lt;/a>.&lt;/p>
&lt;p>&lt;img src="index_files/figure-html/unnamed-chunk-8-1.png" width="672" />&lt;/p>
&lt;p>In the fork scenario, &lt;span class="math inline">\(X \sim Y\)&lt;/span> is blocked by conditioning on &lt;span class="math inline">\(Z={Z}\)&lt;/span>. This is sometimes refered to as the confounder scenario, which is the situation in &lt;a href="#simpsons">the simpson’s paradox example&lt;/a>.&lt;/p>
&lt;p>&lt;img src="index_files/figure-html/unnamed-chunk-9-1.png" width="672" />&lt;/p>
&lt;p>Finally, in the collider scenario, &lt;span class="math inline">\(X \sim Y\)&lt;/span> is blocked by &lt;em>not&lt;/em> conditioning on &lt;span class="math inline">\(Z={M}\)&lt;/span>. The idea that &lt;span class="math inline">\(X\)&lt;/span> and &lt;span class="math inline">\(Y\)&lt;/span>, which are independent, to begin with, can become conditionally dependant is unintuitive. One way to think about this is that we are sharing information received from $ Y $ with $ X $ through $ M $ when we condition on $ M $. For a more thorough investigation into this phenomenon, refer to &lt;span class="citation">(&lt;a href="#ref-pearl2016causal" role="doc-biblioref">Pearl, Glymour, and Jewell 2016&lt;/a>)&lt;/span>.&lt;/p>
&lt;p>A path is said to be blocked by &lt;span class="math inline">\(Z\)&lt;/span> if it contains a chain or fork with its middle node in &lt;span class="math inline">\(Z\)&lt;/span> or a collider with its middle node not in &lt;span class="math inline">\(Z\)&lt;/span>.&lt;/p>
&lt;p>We are now ready to introduce the main criteria for which we can perform adjustments.&lt;/p>
&lt;/div>
&lt;div id="the-backdoor" class="section level2">
&lt;h2>The Backdoor&lt;/h2>
&lt;div class="definition">
&lt;span id="def:unnamed-chunk-10" class="definition">&lt;strong>Definition 1 (The Backdoor Criterion) &lt;/strong>&lt;/span>A set of nodes &lt;span class="math inline">\(Z\)&lt;/span>, given a DAG &lt;span class="math inline">\(G\)&lt;/span> and a pair of nodes &lt;span class="math inline">\((X,Y)\)&lt;/span>, is said to satisfy the backdoor criterion if no node in &lt;span class="math inline">\(Z\)&lt;/span> is a descendant of &lt;span class="math inline">\(X\)&lt;/span>, and &lt;span class="math inline">\(Z\)&lt;/span> blocks all paths between &lt;span class="math inline">\(X\)&lt;/span> and &lt;span class="math inline">\(Y\)&lt;/span>, which contain arrows into &lt;span class="math inline">\(X\)&lt;/span>.
&lt;/div>
&lt;p>If there exists are set of nodes why satisfy the backdoor criterion, then the effect of &lt;span class="math inline">\(X\)&lt;/span> on &lt;span class="math inline">\(Y\)&lt;/span> is identifiable and given by:
&lt;span class="math display">\[
P(y|do(x)) = \sum_z \frac{P(X=x, Y=y, Z=z)}{P(X=x| Z=z)}
\]&lt;/span>&lt;/p>
&lt;p>The backdoor criterion stops undue influence through the &lt;em>backdoor&lt;/em> paths; it leaves direct paths between &lt;span class="math inline">\(X\)&lt;/span> and &lt;span class="math inline">\(Y\)&lt;/span>, and it blocks spurious paths.&lt;/p>
&lt;p>It is clear that { &lt;em>age&lt;/em> } satisfies these conditions to be a backdoor adjustment set in the example above.&lt;/p>
&lt;p>&lt;img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" />&lt;/p>
&lt;/div>
&lt;div id="front-door" class="section level2">
&lt;h2>The Front-door&lt;/h2>
&lt;p>There are notably common scenarios where this doesn’t work. For example, consider a constructed causal mediation situation, as follows:
&lt;img src="index_files/figure-html/unnamed-chunk-12-1.png" width="672" />&lt;/p>
&lt;p>In this case we cannot use the backdoor criterion, to detect the effect of &lt;em>smoking&lt;/em> on &lt;em>cancer&lt;/em> because &lt;em>tar&lt;/em> is a descendant of &lt;em>smoking&lt;/em>, and there exists no direct link between &lt;em>smoking&lt;/em> and &lt;em>cancer&lt;/em>. We must use instead the frontdoor criterion:&lt;/p>
&lt;div class="definition">
&lt;span id="def:unnamed-chunk-13" class="definition">&lt;strong>Definition 2 (The Frontdoor Criterion) &lt;/strong>&lt;/span>A set of nodes &lt;span class="math inline">\(Z\)&lt;/span>, given a DAG &lt;span class="math inline">\(G\)&lt;/span> and a pair of nodes &lt;span class="math inline">\((X,Y)\)&lt;/span>, is said to satisfy the frontdoor criterion if; &lt;span class="math inline">\(Z\)&lt;/span> intercepts all direct paths from &lt;span class="math inline">\(X\)&lt;/span> to &lt;span class="math inline">\(Y\)&lt;/span>, all paths between &lt;span class="math inline">\(X\)&lt;/span> and &lt;span class="math inline">\(Z\)&lt;/span> are blocked, and all backdoor paths between &lt;span class="math inline">\(Y\)&lt;/span> and &lt;span class="math inline">\(Z\)&lt;/span> are blocked by &lt;span class="math inline">\(X\)&lt;/span>.
&lt;/div>
&lt;p>If there exists are set of nodes &lt;span class="math inline">\(Z\)&lt;/span> which satisfy the frontdoor criterion, and &lt;span class="math inline">\(P(x, z)&amp;gt;0\)&lt;/span>, then the effect of &lt;span class="math inline">\(X\)&lt;/span> on &lt;span class="math inline">\(Y\)&lt;/span> is identifiable and given by:
&lt;span class="math display">\[
P(y|do(x)) = \sum_z P(z|x) \sum_{x^\prime} P(y|x^\prime, z)P(x^\prime)
\]&lt;/span>
In our smoking scenario, we see that by adjusting for &lt;em>tar&lt;/em> , we can observe the effect of &lt;em>smoking&lt;/em> on &lt;em>cancer&lt;/em>.&lt;/p>
&lt;/div>
&lt;div id="conclusion" class="section level2">
&lt;h2>Conclusion&lt;/h2>
&lt;p>The above briefly outlines a core motivation for studying causal inference and causal stories. We summarise some of the underlying theory of causal inference and show practical methodology through the &lt;em>frontdoor&lt;/em> and &lt;em>backdoor&lt;/em> criterion for determining causal effects through entirely observational studies.&lt;/p>
&lt;p>There are notable aspects of causal inference we have omitted from this taster. The most gaping is the lack of an explanation for the powerful tool of counterfactuals. We have only presented binary examples here (aside from our motivating example); however, perhaps the most common and useful causal inference application is to continuous examples using regression with linear models. Ultimately, we decided this was beyond causal inference taster’s scope and were more deserving of their own articles. Again, for the interested reader, we recommend &lt;span class="citation">&lt;a href="#ref-pearl2016causal" role="doc-biblioref">Pearl, Glymour, and Jewell&lt;/a> (&lt;a href="#ref-pearl2016causal" role="doc-biblioref">2016&lt;/a>)&lt;/span>, which adds links to many other resources.&lt;/p>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-pearl1995causal" class="csl-entry">
Pearl, Judea. 1995. &lt;span>“Causal Diagrams for Empirical Research.”&lt;/span> &lt;em>Biometrika&lt;/em> 82 (4): 669–88.
&lt;/div>
&lt;div id="ref-pearl2016causal" class="csl-entry">
Pearl, Judea, Madelyn Glymour, and Nicholas P Jewell. 2016. &lt;em>Causal Inference in Statistics: A Primer&lt;/em>. John Wiley &amp;amp; Sons.
&lt;/div>
&lt;div id="ref-simpson1951interpretation" class="csl-entry">
Simpson, Edward H. 1951. &lt;span>“The Interpretation of Interaction in Contingency Tables.”&lt;/span> &lt;em>Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em> 13 (2): 238–41.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Epidemic Hawkes: an example project</title><link>https://www.behavioral-ds.ml/researchproject/project1/</link><pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/researchproject/project1/</guid><description>&lt;p>This is an example project description&lt;/p></description></item><item><title>evently: simulation, fitting of Hawkes processes</title><link>https://www.behavioral-ds.ml/softwaretool/evently/</link><pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/softwaretool/evently/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>This package is designed for simulating and fitting the Hawkes processes
and the HawkesN processes with several options of kernel functions.
Currently, it assumes univariate processes without background event
rates. Prior knowledge about the models is assumed in the following
tutorial and please refer to [1] and [2] for details about the
models.&lt;/p>
&lt;pre>&lt;code class="language-r">library(evently)
&lt;/code>&lt;/pre>
&lt;h2 id="installation-and-dependencies">Installation and dependencies&lt;/h2>
&lt;p>Several dependencies
(&lt;a href="https://cran.r-project.org/web/packages/poweRlaw/poweRlaw.pdf">poweRlaw&lt;/a>,
&lt;a href="https://ampl.com/">AMPL&lt;/a>,
&lt;a href="https://www.coin-or.org/Ipopt/documentation/">Ipopt&lt;/a>) are required for
running this package. These dependencies will be installed automatically
by R or by following instructions upon package load.&lt;/p>
&lt;p>Install the package by executing&lt;/p>
&lt;pre>&lt;code class="language-r">if (!require('devtools')) install.packages('devtools')
devtools::install_github('behavioral-ds/evently')
&lt;/code>&lt;/pre>
&lt;h2 id="simulating-cascades">Simulating cascades&lt;/h2>
&lt;p>Let’s first simulate 100 event cascades of the &lt;strong>Hawkes process with an
exponential kernel function&lt;/strong> (please refer to the &lt;a href="#available-models">Available
models&lt;/a> for models and their abbreviations in the
package) with a given parameter set, &lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%20%3D%200.9%2C%20%5Ctheta%20%3D%201" alt="\\kappa = 0.9, \\theta= 1" title="\kappa = 0.9, \theta = 1">. For each simulation, we only simulate
until 5 seconds. The resulted cascades are placed in a single &lt;code>list&lt;/code>
where each cascade is a &lt;code>data.frame&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-r">set.seed(4)
sim_no &amp;lt;- 100
data &amp;lt;- generate_hawkes_event_series(par = c(K = 0.9, theta = 1), model_type = 'EXP', Tmax = 5, sim_no = sim_no)
# alternatively, `generate_hawkes_event_series` also accepts a model class object
# e.g.
# model &amp;lt;- new_hawkes_model(par = c(K = 0.9, theta = 1), model_type = 'EXP')
# generate_hawkes_event_series(model = model, Tmax = 5, sim_no = sim_no)
head(data[[1]])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## magnitude time
## 1 1 0.0000000
## 2 1 0.5941959
## 3 1 1.4712411
## 4 1 1.6105430
## 5 1 1.7855535
## 6 1 1.8883869
&lt;/code>&lt;/pre>
&lt;p>A simulated process is represented by a &lt;code>data.frame&lt;/code> where each row is
an event. &lt;code>time&lt;/code> indicates the event happening time, while &lt;code>magnitude&lt;/code>
is the event mark information which is always 1 if &lt;code>model_type&lt;/code> is an
unmarked model. In the context of retweet diffusion cascades, the first
row is the original tweet and all following events are its retweets.
&lt;code>time&lt;/code> records the relative time (in second) of each retweet to the
original tweet and &lt;code>magnitude&lt;/code> is the follows’ count of the user who
retweeted.&lt;/p>
&lt;h2 id="fitting-a-model-on-data">Fitting a model on data&lt;/h2>
&lt;p>We can then fit on the cascades simulated in the previous section. After
providing the &lt;code>data&lt;/code> and &lt;code>model_type&lt;/code>, the fitting procedure will spawn
10 AMPL optimization procedures with different parameter
inistializations due to the non-convexity of some likelihood functions.
Among the 10 fitted model, the one giving the best likelihood value will
be returned. To make the fitting procedure faster, we can specify the
number of &lt;code>cores&lt;/code> to be used for fitting them in
parallel.&lt;/p>
&lt;pre>&lt;code class="language-r">fitted_model &amp;lt;- fit_series(data, model_type = 'EXP', observation_time = 5, cores = 10)
fitted_model
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Model: EXP
## No. of cascades: 100
## init_par
## K 7.92e+00; theta 1.32e+00
## par
## K 8.51e-01; theta 1.06e+00
## Neg Log Likelihood: 285.488
## lower_bound
## K 1.00e-100; theta 1.00e-100
## upper_bound
## K 1.00e+04; theta 3.00e+02
## convergence: 0
&lt;/code>&lt;/pre>
&lt;h2 id="available-models">Available models&lt;/h2>
&lt;p>There are 8 models available so far in this
package:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="center">Model&lt;/th>
&lt;th align="center">Abbreviation (model_type)&lt;/th>
&lt;th align="center">Intensity Function&lt;/th>
&lt;th align="center">Parameters&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="center">Hawkes process with an exponential kernel function&lt;/td>
&lt;td align="center">EXP&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Csum_%7Bt_i%20%3C%20t%7D%20%5Ctheta%20e%5E%7B-%5Ctheta%20%28t-t_i%29%7D" alt="\\kappa\\sum\_{t\_i \&lt; t} \\theta e^{-\\theta (t-t\_i)}" title="\kappa\sum_{t_i &amp;lt; t} \theta e^{-\theta (t-t_i)}">&lt;/td>
&lt;td align="center">K,theta&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">Hawkes process with a power-law kernel function&lt;/td>
&lt;td align="center">PL&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Csum_%7Bt_i%20%3C%20t%7D%20%28t-t_i%20%2B%20c%29%5E%7B-%281%2B%5Ctheta%29%7D" alt="\\kappa\\sum\_{t\_i \&lt; t} (t-t\_i + c)^{-(1+\\theta)}" title="\kappa\sum_{t_i &amp;lt; t} (t-t_i + c)^{-(1+\theta)}">&lt;/td>
&lt;td align="center">K,c,theta&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">HawkesN process with an exponential kernel function&lt;/td>
&lt;td align="center">EXPN&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Cfrac%7BN-N_t%7D%7BN%7D%5Csum_%7Bt_i%20%3C%20t%7D%20%5Ctheta%20e%5E%7B-%5Ctheta%20%28t-t_i%29%7D" alt="\\kappa\\frac{N-N\_t}{N}\\sum\_{t\_i \&lt; t} \\theta e^{-\\theta (t-t\_i)}" title="\kappa\frac{N-N_t}{N}\sum_{t_i &amp;lt; t} \theta e^{-\theta (t-t_i)}">&lt;/td>
&lt;td align="center">K,theta,N&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">HawkesN process with a power-law kernel function&lt;/td>
&lt;td align="center">PLN&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Cfrac%7BN-N_t%7D%7BN%7D%5Csum_%7Bt_i%20%3C%20t%7D%20%28t-t_i%20%2B%20c%29%5E%7B-%281%2B%5Ctheta%29%7D" alt="\\kappa\\frac{N-N\_t}{N}\\sum\_{t\_i \&lt; t} (t-t\_i + c)^{-(1+\\theta)}" title="\kappa\frac{N-N_t}{N}\sum_{t_i &amp;lt; t} (t-t_i + c)^{-(1+\theta)}">&lt;/td>
&lt;td align="center">K,c,theta,N&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">Marked Hawkes process with an exponential kernel function&lt;/td>
&lt;td align="center">mEXP&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Csum_%7Bt_i%20%3C%20t%7D%20%5Ctheta%20m_i%5E%7B%5Cbeta%7D%20e%5E%7B-%5Ctheta%20%28t-t_i%29%7D" alt="\\kappa\\sum\_{t\_i \&lt; t} \\theta m\_i^{\\beta} e^{-\\theta (t-t\_i)}" title="\kappa\sum_{t_i &amp;lt; t} \theta m_i^{\beta} e^{-\theta (t-t_i)}">&lt;/td>
&lt;td align="center">K,beta,theta&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">Marked Hawkes process with a power-law kernel function&lt;/td>
&lt;td align="center">mPL&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Csum_%7Bt_i%20%3C%20t%7D%20m_i%5E%7B%5Cbeta%7D%20%28t-t_i%20%2B%20c%29%5E%7B-%281%2B%5Ctheta%29%7D" alt="\\kappa\\sum\_{t\_i \&lt; t} m\_i^{\\beta} (t-t\_i + c)^{-(1+\\theta)}" title="\kappa\sum_{t_i &amp;lt; t} m_i^{\beta} (t-t_i + c)^{-(1+\theta)}">&lt;/td>
&lt;td align="center">K,beta,c,theta&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">Marked HawkesN process with an exponential kernel function&lt;/td>
&lt;td align="center">mEXPN&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Cfrac%7BN-N_t%7D%7BN%7D%5Csum_%7Bt_i%20%3C%20t%7D%20%5Ctheta%20m_i%5E%7B%5Cbeta%7D%20e%5E%7B-%5Ctheta%20%28t-t_i%29%7D" alt="\\kappa\\frac{N-N\_t}{N}\\sum\_{t\_i \&lt; t} \\theta m\_i^{\\beta} e^{-\\theta (t-t\_i)}" title="\kappa\frac{N-N_t}{N}\sum_{t_i &amp;lt; t} \theta m_i^{\beta} e^{-\theta (t-t_i)}">&lt;/td>
&lt;td align="center">K,beta,theta,N&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">Marked HawkesN process with a power-law kernel function&lt;/td>
&lt;td align="center">mPLN&lt;/td>
&lt;td align="center">&lt;img src="https://latex.codecogs.com/png.latex?%5Ckappa%5Cfrac%7BN-N_t%7D%7BN%7D%5Csum_%7Bt_i%20%3C%20t%7D%20m_i%5E%7B%5Cbeta%7D%28t-t_i%20%2B%20c%29%5E%7B-%281%2B%5Ctheta%29%7D" alt="\\kappa\\frac{N-N\_t}{N}\\sum\_{t\_i \&lt; t} m\_i^{\\beta}(t-t\_i + c)^{-(1+\\theta)}" title="\kappa\frac{N-N_t}{N}\sum_{t_i &amp;lt; t} m_i^{\beta}(t-t_i + c)^{-(1+\theta)}">&lt;/td>
&lt;td align="center">K,beta,c,theta,N&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="acknowledgement">Acknowledgement&lt;/h2>
&lt;p>The development of this package is supported by the Green Policy grant
from the National Security College, Crawford School, ANU.&lt;/p>
&lt;h2 id="license">License&lt;/h2>
&lt;p>Both dataset and code are distributed under the &lt;a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons
Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)
license&lt;/a>. If you
require a different license, please contact us at &lt;a href="mailto:Quyu.Kong@anu.edu.au">Quyu.Kong@anu.edu.au&lt;/a>
or &lt;a href="mailto:Marian-Andrei@rizoiu.eu">Marian-Andrei@rizoiu.eu&lt;/a>.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>[1] Rizoiu, M. A., Lee, Y., Mishra, S., &amp;amp; Xie, L. (2017, December). Hawkes processes for events in social media. In Frontiers of Multimedia Research (pp. 191-218). Association for Computing Machinery
and Morgan &amp;amp; Claypool.&lt;br>
[2] Rizoiu, M. A., Mishra, S., Kong, Q., Carman, M., &amp;amp; Xie, L.
(2018, April). SIR-Hawkes: Linking epidemic models and Hawkes processes to model diffusions in finite populations. In Proceedings of the 2018 World Wide Web Conference (pp. 419-428). International World Wide Web Conferences Steering Committee.&lt;br>
[3] Mishra, S., Rizoiu, M. A., &amp;amp; Xie, L. (2016, October). Feature
driven and point process approaches for popularity prediction. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 1069-1078). ACM.&lt;br>
[4] Kong, Q., Rizoiu, M. A., &amp;amp; Xie, L. (2019). Modeling Information
Cascades with Self-exciting Processes via Generalized Epidemic Models. arXiv preprint arXiv:1910.05451.&lt;/p></description></item><item><title>Blog</title><link>https://www.behavioral-ds.ml/post/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/post/</guid><description/></item><item><title>Contact</title><link>https://www.behavioral-ds.ml/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/contact/</guid><description/></item><item><title>News</title><link>https://www.behavioral-ds.ml/news/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/news/</guid><description/></item><item><title>People</title><link>https://www.behavioral-ds.ml/people/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/people/</guid><description/></item><item><title>Publications</title><link>https://www.behavioral-ds.ml/publication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/publication/</guid><description>&lt;p>
&lt;script src="https://www.behavioral-ds.ml/js/bibtex_js.js">&lt;/script>
&lt;bibtex src="publications.bib">&lt;/bibtex>&lt;/p>
&lt;div style="margin-right: 10px; margin-top: -50px; float:right">
&lt;div class="input-group">
&lt;span class="input-group-addon">&lt;i class="fa fa-search" style="padding-left: 5px;">&lt;/i>&lt;/span>
&lt;input type="text" class="bibtex_search" id="searchbar" placeholder="Search publications">
&lt;/div>
&lt;/div>
&lt;div class="row">
&lt;div class="col-sm-12">
&lt;div id="bibtex_display" style="padding: 0 10px;">&lt;/div>
&lt;div class="bibtex_structure">
&lt;div class="group year" extra="DESC number">
&lt;div class="row">
&lt;div id="year-title" class="col-sm-12">
&lt;div class="title">&lt;/div>
&lt;/div>
&lt;div class="col-sm-12">
&lt;div class="templates">&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="bibtex_template">
&lt;ul style="list-style-type:none">
&lt;li class="if author" style="font-weight: normal;">
&lt;b>&lt;span class="title">&lt;/span>&lt;/b>,
&lt;span class="author">&lt;/span>,
&lt;span class="if booktitle">
&lt;span class="booktitle">&lt;/span>,
&lt;/span>
&lt;span class="if journal">
&lt;span class="journal">&lt;/span>,
&lt;/span>
&lt;span class="year">&lt;/span>
&lt;span class="if url" style="margin-left: 5px; font-size:16px">
&lt;a class="url" target="_blank">
&lt;i class="fas fa-link" style="color:black;">&lt;/i>&lt;/a>
&lt;/span>
&lt;span class="if url_paper" style="margin-left: 5px; font-size:16px">
&lt;a class="url_paper" target="_blank">
&lt;i class="fas fa-file-alt" style="color:black;">&lt;/i>&lt;/a>
&lt;/span>
&lt;span class="if url_code" style="margin-left: 5px; font-size:16px">
&lt;a class="url_code" target="_blank">
&lt;i class="fab fa-github" style="color:black;">&lt;/i>&lt;/a>
&lt;/span>
&lt;span class="if url_slides" style="margin-left: 5px; font-size:16px">
&lt;a class="url_slides" target="_blank">
&lt;i class="far fa-newspaper" style="color:black;">&lt;/i>&lt;/a>
&lt;/span>
&lt;span class="if abstract" style="margin-left: 5px;">
&lt;div class="morepage" >
&lt;span class="bibtexkey">&lt;/span>
&lt;span class="abstract noread">&lt;/span>
&lt;/div>
&lt;/span>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Reading</title><link>https://www.behavioral-ds.ml/reading/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/reading/</guid><description/></item><item><title>Research</title><link>https://www.behavioral-ds.ml/research/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/research/</guid><description/></item><item><title>Tool</title><link>https://www.behavioral-ds.ml/tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/tool/</guid><description/></item></channel></rss>