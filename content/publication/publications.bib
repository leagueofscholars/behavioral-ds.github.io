@inproceedings{Kong2021b,
    abstract = {Qualitative research provides methodological guidelines for observing and studying communities and cultures on online social media platforms. However, such methods demand considerable manual effort from researchers and may be overly focused and narrowed to certain online groups. In this work, we propose a complete solution to accelerate qualitative analysis of problematic online speech -- with a specific focus on opinions emerging from online communities -- by leveraging machine learning algorithms. First, we employ qualitative methods of deep observation for understanding problematic online speech. This initial qualitative study constructs an ontology of problematic speech, which contains social media postings annotated with their underlying opinions. The qualitative study also dynamically constructs the set of opinions, simultaneous with labeling the postings. Next, we collect a large dataset from three online social media platforms (Facebook, Twitter and Youtube) using keywords. Finally, we introduce an iterative data exploration procedure to augment the dataset. It alternates between a data sampler, which balances exploration and exploitation of unlabeled data, the automatic labeling of the sampled data, the manual inspection by the qualitative mapping team and, finally, the retraining of the automatic opinion classifier. We present both qualitative and quantitative results. First, we present detailed case studies of the dynamics of problematic speech in a far-right Facebook group, exemplifying its mutation from conservative to extreme. Next, we show that our method successfully learns from the initial qualitatively labeled and narrowly focused dataset, and constructs a larger dataset. Using the latter, we examine the dynamics of opinion emergence and co-occurrence, and we hint at some of the pathways through which extreme opinions creep into the mainstream online discourse.},
    archivePrefix = {arXiv},
    arxivId = {2109.00302},
    author = {Kong, Quyu and Booth, Emily and Bailo, Francesco and Johns, Amelia and Rizoiu, Marian-Andrei},
    booktitle = {AAAI International Conference on Web and Social Media},
    eprint = {2109.00302},
    file = {:home/andrei/Mendeley Desktop/Kong et al/AAAI International Conference on Web and Social Media/Kong et al. - 2022 - Slipping to the Extreme A Mixed Method to Explain How Extreme Opinions Infiltrate Online Discussions.pdf:pdf},
    month = {sep},
    title = {{Slipping to the Extreme: A Mixed Method to Explain How Extreme Opinions Infiltrate Online Discussions}},
    URL_paper = {http://arxiv.org/pdf/2109.00302.pdf},
    URL_video = {https://www.youtube.com/watch?v=HwFq3ywanp4},
    year = {2022}
}

@inproceedings{Ahadi2022,
    abstract = {Higher education often aims to create job-ready graduates. Thus, the skills and knowledge taught in professional degrees are expected to align with the needs of the labor market. However, the dynamic nature of the job market makes it challenging to ensure that this alignment occurs. In this study, we show how Skills Analytics can be used to identify critical skills in the workforce, mapping these to the curriculum offerings of a university. This enables us to identify skill gaps between what is taught and what is needed in the job market. Methods are presented that allow universities to test the alignment of their curriculum offerings with the job market. Where gaps are identified, this would enable universities to update their curriculum more rapidly to produce graduates equipped with up-to-date skills required by the local job market. Our contributions include: a new method for ranking skills in curricula based on their relative importance in the job market; and proof of concept methods to find skills gaps between curriculum offerings and an identified job market that can lead to curriculum redesign and enhancements.},
    address = {Durham, United Kingdom},
    author = {Alireza Ahadi and Kirsty Kitto and Marian-Andrei Rizoiu and Katarzyna Musial},
    booktitle = {Proceedings of the 15th International Conference on Educational Data Mining},
    doi = {10.5281/zenodo.6853121},
    editor = {Antonija Mitrovic and Nigel Bosch},
    isbn = {978-1-7336736-3-1},
    month = {July},
    pages = {538--542},
    publisher = {International Educational Data Mining Society},
    title = {{Skills Taught vs Skills Sought: Using Skills Analytics to Identify the Gaps between Curriculum and Job Markets}},
    year = {2022},
    URL = {https://zenodo.org/record/6853121},
    URL_paper = {https://educationaldatamining.org/edm2022/proceedings/2022.EDM-posters.56/2022.EDM-posters.56.pdf}
}

@article{Dawson2021a,
    abstract = {Job security can never be taken for granted, especially in times of rapid, widespread and unexpected social and economic change. These changes can force workers to transition to new jobs. This may be because new technologies emerge or production is moved abroad. Perhaps it is a global crisis, such as COVID-19, which shutters industries and displaces labor en masse . Regardless of the impetus, people are faced with the challenge of moving between jobs to find new work. Successful transitions typically occur when workers leverage their existing skills in the new occupation. Here, we propose a novel method to measure the similarity between occupations using their underlying skills. We then build a recommender system for identifying optimal transition pathways between occupations using job advertisements (ads) data and a longitudinal household survey. Our results show that not only can we accurately predict occupational transitions (Accuracy = 76%), but we account for the asymmetric difficulties of moving between jobs (it is easier to move in one direction than the other). We also build an early warning indicator for new technology adoption (showcasing Artificial Intelligence), a major driver of rising job transitions. By using real-time data, our systems can respond to labor demand shifts as they occur (such as those caused by COVID-19). They can be leveraged by policy-makers, educators, and job seekers who are forced to confront the often distressing challenges of finding new jobs.},
    archivePrefix = {arXiv},
    arxivId = {2011.11801},
    author = {Dawson, Nikolas and Williams, Mary-Anne and Rizoiu, Marian-Andrei},
    doi = {10.1371/journal.pone.0254722},
    editor = {Fu, Shihe},
    eprint = {2011.11801},
    issn = {1932-6203},
    journal = {PLOS ONE},
    month = {aug},
    number = {8},
    pages = {e0254722},
    title = {{Skill-driven recommendations for job transition pathways}},
    url = {https://dx.plos.org/10.1371/journal.pone.0254722},
    URL_paper = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0254722&type=printable},
    volume = {16},
    year = {2021}
}


@article{McCarthy2021a,
    abstract = {Ever since the web began, the number of websites has been growing exponentially. These websites cover an ever-increasing range of online services that fill a variety of social and economic functions across a growing range of industries. Yet the networked nature of the web, combined with the economics of preferential attachment, increasing returns and global trade, suggest that over the long run a small number of competitive giants are likely to dominate each functional market segment, such as search, retail and social media. Here we perform a large scale longitudinal study to quantify the distribution of attention given in the online environment to competing organisations. In two large online social media datasets, containing more than 10 billion posts and spanning more than a decade, we tally the volume of external links posted towards the organisations' main domain name as a proxy for the online attention they receive. We also use the Common Crawl dataset—which contains the linkage patterns between more than a billion different websites—to study the patterns of link concentration over the past three years across the entire web. Lastly, we showcase the linking between economic, financial and market data by exploring the relationships between online attention on social media and the growth in enterprise value in the electric carmaker Tesla. Our analysis shows that despite the fact that we observe consistent growth in all the macro indicators—the total amount of online attention, in the number of organisations with an online presence, and in the functions they perform—we also observe that a smaller number of organisations account for an ever-increasing proportion of total user attention, usually with one large player dominating each function. These results highlight how evolution of the online economy involves innovation, diversity, and then competitive dominance.},
    archivePrefix = {arXiv},
    arxivId = {2003.07049},
    author = {McCarthy, Paul X. and Gong, Xian and Eghbal, Sina and Falster, Daniel S. and Rizoiu, Marian-Andrei},
    doi = {10.1371/journal.pone.0249993},
    editor = {Zollo, Fabiana},
    eprint = {2003.07049},
    issn = {1932-6203},
    journal = {PLOS ONE},
    month = {apr},
    number = {4},
    pages = {e0249993},
    title = {{Evolution of diversity and dominance of companies in online activity}},
    url = {https://dx.plos.org/10.1371/journal.pone.0249993},
%    URL_paper = {http://arxiv.org/abs/2003.07049},
    URL_paper = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0249993&type=printable},
    volume = {16},
    year = {2021}
}

@article{Unwin2021,
    abstract = {Developing new methods for modelling infectious diseases outbreaks is important for monitoring transmission and developing policy. In this paper we propose using semi-mechanistic Hawkes Processes for modelling malaria transmission in near-elimination settings. Hawkes Processes are well founded mathematical methods that enable us to combine the benefits of both statistical and mechanistic models to recreate and forecast disease transmission beyond just malaria outbreak scenarios. These methods have been successfully used in numerous applications such as social media and earthquake modelling, but are not yet widespread in epidemiology. By using domain-specific knowledge, we can both recreate transmission curves for malaria in China and Eswatini and disentangle the proportion of cases which are imported from those that are community based.},
    author = {Unwin, H Juliette T and Routledge, Isobel and Flaxman, Seth and Rizoiu, Marian-Andrei and Lai, Shengjie and Cohen, Justin and Weiss, Daniel J and Mishra, Swapnil and Bhatt, Samir},
    doi = {10.1371/journal.pcbi.1008830},
    editor = {Perkins, Alex},
    issn = {1553-7358},
    journal = {PLOS Computational Biology},
    month = {apr},
    number = {4},
    pages = {e1008830},
    title = {{Using Hawkes Processes to model imported and local malaria cases in near-elimination settings}},
    url = {https://dx.plos.org/10.1371/journal.pcbi.1008830},
    volume = {17},
    URL_paper = {https://www.medrxiv.org/content/10.1101/2020.07.17.20156174v1.full.pdf},
    URL_code = {https://github.com/mrc-ide/epihawkes},
    year = {2021}
}

@article{Dawson2021,
    abstract = {In Australia and beyond, journalism is reportedly an industry in crisis, a crisis exacerbated by COVID-19. However, the evidence revealing the crisis is often anecdotal or limited in scope. In this unprecedented longitudinal research, we draw on data from the Australian journalism jobs market from January 2012 until March 2020. Using Data Science and Machine Learning techniques, we analyse two distinct data sets: job advertisements (ads) data comprising 3698 journalist job ads from a corpus of over 8 million Australian job ads; and official employment data from the Australian Bureau of Statistics. Having matched and analysed both sources, we address both the demand for and supply of journalists in Australia over this critical period. The data show that the crisis is real, but there are also surprises. Counter-intuitively, the number of journalism job ads in Australia rose from 2012 until 2016, before falling into decline. Less surprisingly, for the entire period studied the figures reveal extreme volatility, characterised by large and erratic fluctuations. The data also clearly show that COVID-19 has significantly worsened the crisis. We then tease out more granular findings, including: that there are now more women than men journalists in Australia, but that gender inequity is worsening, with women journalists getting younger and worse-paid just as men journalists are, on average, getting older and better-paid; that, despite the crisis besetting the industry, the demand for journalism skills has increased; and that, perhaps concerningly, the skills sought by journalism job ads increasingly include ‘social media' and ‘generalist communications' skills.},
    archivePrefix = {arXiv},
    arxivId = {2008.12459},
    author = {Dawson, Nikolas and Molitorisz, Sacha and Rizoiu, Marian-Andrei and Fray, Peter},
    doi = {10.1177/1464884921996286},
    eprint = {2008.12459},
    issn = {1464-8849},
    journal = {Journalism},
    month = {feb},
    pages = {146488492199628},
    title = {{Layoffs, inequity and COVID-19: A longitudinal study of the journalism jobs crisis in Australia from 2012 to 2020}},
    url = {http://journals.sagepub.com/doi/10.1177/1464884921996286},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2008.12459.pdf}
}

@inproceedings{Largeron2021,
    abstract = {This paper studies the dynamics of opinion formation and polarization in social media. We investigate whether users' stance concerning contentious subjects is influenced by the online discussions they are exposed to and interactions with users supporting different stances. We set up a series of predictive exercises based on machine learning models. Users are described using several posting activities features capturing their overall activity levels, posting success, the reactions their posts attract from users of different stances, and the types of discussions in which they engage. Given the user description at present, the purpose is to predict their stance in the future. Using a dataset of Brexit discussions on the Reddit platform, we show that the activity features regularly outperform the textual baseline, confirming the link between exposure to discussion and opinion. We find that the most informative features relate to the stance composition of the discussion in which users prefer to engage.},
    author = {Largeron, Christine and Mardale, Andrei and Rizoiu, Marian-Andrei},
    booktitle = {Symposium on Intelligent Data Analysis (IDA'21)},
    doi = {10.1007/978-3-030-74251-5_22},
    pages = {275--286},
    title = {{Linking the Dynamics of User Stance to the Structure of Online Discussions}},
    url = {https://link.springer.com/10.1007/978-3-030-74251-5_22},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2101.09852.pdf},
    URL_code = {https://github.com/behavioral-ds/online-opinion-dynamics},
    URL_video = {https://www.youtube.com/watch?v=DPy3l0RdgPM}
}

@inproceedings{Ram2021a,
    abstract = {The impact of online social media on societal events and institutions is profound, and with the rapid increases in user uptake, we are just starting to understand its ramifications. Social scientists and practitioners who model online discourse as a proxy for real-world behavior often curate large social media datasets. A lack of available tooling aimed at non-data science experts frequently leaves this data (and the insights it holds) underutilized. Here, we propose birdspotter – a tool to analyze and label Twitter users –, and birdspotter.ml – an exploratory visualizer for the computed metrics. birdspotter provides an end-to-end analysis pipeline, from the processing of pre-collected Twitter data to general-purpose labeling of users and estimating their social influence, within a few lines of code. The package features tutorials and detailed documentation. We also illustrate how to train birdspotter into a fully-fledged bot detector that achieves better than state-of-the-art performances without making Twitter API calls, and we showcase its usage in an exploratory analysis of a topical COVID-19 dataset.},
    address = {New York, NY, USA},
    author = {Ram, Rohit and Kong, Quyu and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
    doi = {10.1145/3437963.3441695},
    isbn = {9781450382977},
    issn = {23318422},
    month = {mar},
    pages = {918--921},
    publisher = {ACM},
    title = {{Birdspotter: A Tool for Analyzing and Labeling Twitter Users}},
    url = {https://dl.acm.org/doi/10.1145/3437963.3441695},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2012.02370.pdf},
    URL_code = {https://github.com/behavioral-ds/BirdSpotter},
    URL_video = {https://youtu.be/52HwHAiK1rs}
}
@inproceedings{Kong2021a,
    abstract = {Modeling online discourse dynamics is a core activity in understanding the spread of information, both offline and online, and emergent online behavior. There is currently a disconnect between the practitioners of online social media analysis - usually social, political and communication scientists - and the accessibility to tools capable of handling large quantities of online data, and examining online users and their behavior. We present two tools,birdspotter and evently, for analyzing online users based on their involvement in retweet cascades. birdspotter provides a toolkit to measure social influence and botnets of Twitter users. While it leverages the multimodal information of tweets, such as text contents, evently augments the user measurement by modeling the temporal dynamics of information diffusions using self-exciting processes. Both tools are designed for users with a wide range of computer expertise and include tutorials and detailed documentation. We illustrate a case study of a topical dataset relating to COVID-19, using both tools for end-to-end analysis of online user behavior.},
    address = {New York, NY, USA},
    archivePrefix = {arXiv},
    arxivId = {2006.06167},
    author = {Kong, Quyu and Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
    doi = {10.1145/3437963.3441708},
    eprint = {2006.06167},
    isbn = {9781450382977},
    month = {mar},
    pages = {1097--1100},
    publisher = {ACM},
    title = {{Evently: Modeling and Analyzing Reshare Cascades with Hawkes Processes}},
    url = {https://dl.acm.org/doi/10.1145/3437963.3441708},
    year = {2021},
    URL_paper = {https://arxiv.org/pdf/2006.06167.pdf},
    URL_code = {https://github.com/behavioral-ds/evently},
    URL_video = {https://youtu.be/zSMHol0qsy4}
}

@phdthesis{Liu2021,
    abstract = {With the prosperity of the online labour market, more and more employers are willing to post recruitment advertisements on the websites. The demand of the labour market changes in a unpredictable speed and many new skills emerge then quickly reflect on the labour market. For the purpose of extracting the existing skills and even find new emerging skills, we leverage the popular natural language processing (NLP) framework, and build reliable model to accomplish this goal. In this research, we use job advertisements from five different English speaking countries including Australia, Canada, UK, Singapore, US and mainly use a natural language processing frame work called spaCy to build named entity recognition (NER) model to identify the skills in these job ads. We propose two kinds of model one is called proprietary model, the other is called joint model to this skills detecting task. We summarise the pros and cons for these two kinds of model respectively and give suggestions about what kind of model should be chosen to solve different problems. Additional to the spacy NER model, we also analyze the factors leading to wrong prediction and compare with the models trained by another NLP framework Flair. The contributions of our research consists of two aspects. One is that we built a reliable NER model whose prediction result is impressive using spaCy and we prove that spaCy is a more suitable choice than Flair when you have rigid time limitations for training, relatively high requirement for accuracy and does not demand powerful device. Furthermore, no prior work provides a NER model to finish this task, our work can play an important role in future related research not necessarily restricted in extracting skills from job ads.},
    author = {Liu, Yaozhong},
    school = {Australian National University},
    title = {{Labour dynamics in the age of automation: detecting emergent skills in labour markets from job ads description}},
    year = {2021},
    URL_paper = {../authors/yaozhong-liu/thesis.pdf}
}

@phdthesis{Law2021,
    abstract = {Recently, social media has been blamed for the increasingly polarised nature of polit- ical discourse in our society. The ability to measure and combat political polarisation on social media is of significant importance if we wish to prevent polarisation from degrading the functioning of democracy and social cohesion. Stance detection pro- vides a viable solution for addressing this problem, however so far no research has tested this technique on highly structured online discussions such as those found on the Reddit social media platform. In this thesis, we propose a pipeline for annotating Reddit submissions for stance via crowdsourced workers from Amazon Mechanical Turk (MTurk). We conduct ex- periments to determine the optimum approach and parameters for conducting stance labelling with MTurk and produce a dataset of 5895 labelled r/Brexit submissions. We analyse the dynamics around r/Brexit discussions relating user activity to the occurrence of political events. We evaluate various novel strategies for improving BERT model performance on stance detection. Finally, we implement a state of the art stance detection model for Reddit user stance towards Brexit that achieves an F1 Score of 0.5547 compared to 0.3203 obtained by our previous baseline model.},
    author = {Law, Andrew},
    school = {The Australian National University},
    title = {{Exposing the Stance of Reddit Users Towards Brexit}},
    year = {2021},
    URL_paper = {../authors/andrew-law/andrew_law_thesis.pdf}
}

@phdthesis{Khuu2021,
    abstract = {The popularisation of social media has led to widespread occurrences of echo cham- bers, selective exposure and misinformation. This is particularly concerning with regard to contentious topics, where lack of interaction with opposing views can lead to complacence or stubbornness. We build on past work in an attempt to deter- mine how exposure to differing opinions affects an individual's future opinion. We quickly discover that: 1. The problem goes far beyond a simple discrete classification task due to the subtleties of user sentiment and 2. Future stance information being conditional on users choosing to remain active in the discussion network. We address the first issue by proposing a continuous polarity metric to quantify the attitudes of users and find that individuals who choose to remain are polarised users who are stubborn in their beliefs. To resolve the second point we must first determine what makes users choose to leave. We find that future presence correlates with user interaction and social neighbourhood size. Finally, we propose a sequence model that takes into account individual interac- tions to predict future user behaviour.},
    author = {Khuu, Duy},
    title = {{Polarisation and Influence: Investigating Brexit Opinion Dynamics on Reddit}},
    URL_paper = {../authors/duy-khuu/duy-khuu-thesis.pdf},
    year = {2021}
}


@inproceedings{Rizoiu2013b,
    abstract = {The objective of the thesis is to explore how complex data can be treated using unsupervised machine learning techniques, in which additional information is injected to guide the exploratory process. Starting from specific problems, our contributions take into account the different dimensions of the complex data: their nature (image, text), the additional information attached to the data (labels, structure, concept ontologies) and the temporal dimension. A special attention is given to data representation and how additional information can be leveraged to improve this representation.},
    address = {Beijing, China},
    author = {Rizoiu, Marian-Andrei},
    booktitle = {International Joint Conference on Artificial Intelligence IJCAI'13},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_IJCAI-DC-2013.pdf},
    pages = {3239--3240},
    publisher = {AAAI Press},
    series = {IJCAI '13},
    title = {{Semi-Supervised Structuring of Complex Data}},
    year = {2013},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_IJCAI-DC-2013.pdf}
}

@inproceedings{RIZ12,
    abstract = {In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.},
    address = {Athens, Greece},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    booktitle = {2012 IEEE 24th International Conference on Tools with Artificial Intelligence},
    doi = {10.1109/ICTAI.2012.88},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_ICTAI-2012-preprint.pdf},
    isbn = {978-1-4799-0227-9},
    issn = {10823409},
    keywords = {contiguity penalty function,contiguity penalty function.,semi-supervised clustering,temporal clustering,temporal-aware dissimilarity measure},
    mendeley-groups = {Aigaion Import},
    month = {nov},
    pages = {610--617},
    publisher = {IEEE},
    series = {ICTAI '12},
    title = {{Structuring Typical Evolutions Using Temporal-Driven Constrained Clustering}},
    url = {http://ieeexplore.ieee.org/document/6495100/},
    volume = {1},
    year = {2012},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICTAI-2012-slides.pdf}
}
@article{Musat2012,
    abstract = {This work outlines a novel system that automatically extracts conceptual labels for statistically obtained topics. By creating a projection of the topic, which is a distribution over all the vocabulary words, over the WordNet ontology we succeed in associating concepts to the said groups of words. The most important contributions of this paper are connected to the validation of the role of these concepts as topical labels and the determination of correlations that emerge between the utility of these labels and the strength of the relation between the concepts and the topics.},
    author = {Muşat, Claudiu Cristian and Trǎuşan-Matu, Ştefan and Velcin, Julien and Rizoiu, Marian-Andrei},
%    URL_paper = {:home/andrei/Mendeley Desktop/Muşat et al/UPB Scientific Bulletin, Series C Electrical Engineering/Muşat et al. - 2012 - Automatic extraction of conceptual labels from topic models.pdf:pdf},
    journal = {UPB Scientific Bulletin, Series C: Electrical Engineering},
    keywords = {Conceptual processing,Labels,Topic models,WordNet},
    number = {2},
    pages = {57--68},
    title = {{Automatic extraction of conceptual labels from topic models}},
    volume = {74},
    year = {2012}
}
@inproceedings{MUS11,
    abstract = {The growing number of statistical topic models led to the need to better evaluate their output. Traditional evaluation means estimate the model's fitness to unseen data. It has recently been proven than the output of human judgment can greatly differ from these measures. Thus the need for methods that better emulate human judgment is stringent. In this paper we present a system that computes the conceptual relevance of individual topics from a given model on the basis of information drawn from a given concept hierarchy, in this case WordNet. The notion of conceptual relevance is regarded as the ability to attribute a concept to each topic and separate words related to the topic from the unrelated ones based on that concept. In multiple experiments we prove the correlation between the automatic evaluation method and the answers received from human evaluators, for various corpora and difficulty levels. By changing the evaluation focus from a statistical one to a conceptual one we were able to detect which topics are conceptually meaningful and rank them accordingly.},
    address = {Barcelona, Catalonia, Spain},
    author = {Muşat, Claudiu Cristian and Velcin, Julien and Trǎuşan-Matu, Ştefan and Rizoiu, Marian-Andrei},
    booktitle = {International Joint Conference on Artificial Intelligence, Proceedings of the Twenty-Second},
    doi = {10.5591/978-1-57735-516-8/IJCAI11-312},
    isbn = {978-1-57735-515-1},
    keywords = {natural language processing},
    pages = {1866--1871},
    publisher = {AAAI Press},
    series = {IJCAI 2011},
    title = {{Improving topic evaluation using conceptual knowledge}},
    url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewPDFInterstitial/3010/3754},
    volume = {3},
    year = {2011},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_IJCAI-2011.pdf},
}
@inproceedings{MUS11a,
    abstract = {We propose a system which employs conceptual knowledge to improve topic models by removing unrelated words from the simplified topic description. We use WordNet to detect which topical words are not conceptually similar to the others and then test our assumptions against human judgment. Results obtained on two different corpora in different test conditions show that the words detected as unrelated had a much greater probability than the others to be chosen by human evaluators as not being part of the topic at all. We prove that there is a strong correlation between the said probability and an automatically calculated topical fitness and we discuss the variation of the correlation depending on the method and data used.},
    address = {Warsaw, Poland},
    author = {Muşat, Claudiu Cristian and Velcin, Julien and Rizoiu, Marian-Andrei and Trǎuşan-Matu, Ştefan},
    booktitle = {International Symposium on Methodologies for Intelligent Systems},
    doi = {10.1007/978-3-642-22732-5_12},
    isbn = {978-3-642-22732-5},
    issn = {1860949X},
    keywords = {Evaluation,Improvement,Ontologies,Topic Models},
    pages = {133--142},
    publisher = {Springer},
    series = {ISMIS{\~{}}'11},
    title = {{Concept-Based Topic Model Improvement}},
    url = {http://link.springer.com/10.1007/978-3-642-22732-5{\_}12},
    volume = {369},
    year = {2011},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_ISMIS-2011.pdf},
}
@incollection{Rizoiu2011,
    abstract = {This chapter addresses the issue of topic extraction from text corpora for ontology learning. The first part provides an overview of some of the most significant solutions present today in the literature. These solutions deal mainly with the inferior layers of the Ontology Learning Layer Cake. They are related to the challenges of the Terms and Synonyms layers. The second part shows how these pieces can be bound together into an integrated system for extracting meaningful topics. While the extracted topics are not proper concepts as yet, they constitute a convincing approach towards concept building and therefore ontology learning. This chapter concludes by discussing the research undertaken for filling the gap between topics and concepts as well as perspectives that emerge today in the area of topic extraction.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien},
    booktitle = {Ontology Learning and Knowledge Discovery Using the Web},
    doi = {10.4018/978-1-60960-625-1.ch003},
    editor = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
    URL_paper = {https://eric.univ-lyon2.fr/~jvelcin/public/publis/bookchapter_rizoiu_2011.pdf},
    pages = {38--60},
    publisher = {IGI Global},
    title = {{Topic Extraction for Ontology Learning}},
    url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60960-625-1.ch003},
    year = {2011}
}
@inproceedings{RIZ10,
    abstract = {Organiser les donn{\'{e}}es textuelles et en tirer du sens est un d{\'{e}}fi majeur aujourd'hui. Ainsi, lorsque l'on souhaite analyser un d{\'{e}}bat en ligne ou un forum de discussion, on voudrait pouvoir rapidement voir quels sont les principaux th{\`{e}}mes abord{\'{e}}s et la mani{\`{e}}re dont la discussion se structure autour d'eux. Pour cela, et parce que un m{\^{e}}me texte peut {\^{e}}tre associ{\'{e}} {\`{a}} plusieurs th{\`{e}}mes, nous proposons une m{\'{e}}thode originale pour regrouper les donn{\'{e}}es textuelles en autorisant les chevauchements et pour nommer chaque groupe de mani{\`{e}}re lisible. La contribution principale de cet article est une m{\'{e}}thode globale qui permet de r{\'{e}}aliser toute la cha{\^{i}}ne, partant des donn{\'{e}}es textuelles brutes jusqu'{\`{a}} la caract{\'{e}}risation des groupes {\`{a}} un niveau s{\'{e}}mantique qui d{\'{e}}passe le simple ensemble de mots.},
    address = {Hammamet, Tunisie},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Chauchat, Jean-Hugues},
    booktitle = {Extraction et Gestion des Connaissances, (EGC 10) 10{\`{e}}me Conf{\'{e}}rence},
    keywords = {clustering,overlapping,text mining},
    organization = {C{\'{e}}padu{\`{e}}s},
    pages = {561--572},
    publisher = {Revue des Nouvelles Technologies de l'Information},
    series = {Revue des Nouvelles Technologies de l'Information},
    title = {{Regrouper les donn{\'{e}}es textuelles et nommer les groupes {\`{a}} l'aide des classes recouvrantes}},
    volume = {E-19},
    year = {2010},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_EGC-2010.pdf},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_EGC-2010-slides.pdf}
}

@article{MUS10,
    abstract = {Topic modeling is a growing research field and novel ways of interpreting and evaluating results are necessary. We propose a method for evaluating and improving the performance of topic models generating algorithms relying on WordNet data. We first propose a measure for determining a topic model's fitness factoring in its broadness and redundancy. Then, for each individual topic, the amount of relevant information it provides, along with its most important words and related concepts are determined by defining a cohesion function based on the topic's projection on WordNet concepts. The model as a whole is improved by eliminating each topic's outliers with respect to the ontology projection. We define a inter topic ontology based distance and we further use it to investigate the impact of removing redundant topics from a model with regard to the overlap between topics' ontological projections. Clustering similar topics into conceptually cohesive groups is tried as an alternative to pruning less relevant topics. Results show that evaluating and improving statistical models with WordNet is a promising research track that leads to more coherent topic models.},
    author = {Muşat, Claudiu Cristian and Rizoiu, Marian-Andrei and Trǎuşan-Matu, Ştefan},
    issn = {1843-4460},
    journal = {Romanian Journal of Human-Computer Interaction},
    number = {2},
    pages = {81--96},
    title = {{An Intra and Inter-Topic Evaluation and Cleansing Method}},
    volume = {3},
    year = {2010},
    url = {http://rochi.utcluj.ro/rrioc/en/rrioc-2010-2.html#An_Intra_and_Inter-Topic_Evaluation_and},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_RRIOC-2010.pdf}
}
@article{Rizoiu2013a,
    abstract = {Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like sky ∧ ¬building ∧ panorama would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.1007/s10844-013-0235-x},
    URL_paper = {http://arxiv.org/pdf/1512.05467.pdf},
    issn = {0925-9902},
    journal = {Journal of Intelligent Information Systems},
    keywords = {Algorithms for data and knowledge management,Clustering,Data mining,Feature evaluation,Heuristic methods,Nonparametric statistics,Pattern analysis,Representations,Unsupervised feature construction,clustering,data mining,feature evaluation,nonparametric statistics,representations,unsupervised feature construction},
    month = {jun},
    number = {3},
    pages = {501--527},
    title = {{Unsupervised feature construction for improving data representation and semantics}},
    url = {http://link.springer.com/10.1007/s10844-013-0235-x},
    volume = {40},
    year = {2013}
}

@article{Rizoiu2014b,
    abstract = {In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.1142/S0218213014600136},
    URL_paper = {http://arxiv.org/pdf/1601.02603.pdf},
    issn = {0218-2130},
    journal = {International Journal on Artificial Intelligence Tools},
    month = {aug},
    number = {04},
    pages = {1460013},
    title = {{How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions}},
    url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213014600136},
    volume = {23},
    year = {2014}
}
@article{Rizoiu2015a,
    abstract = {One of the prevalent learning tasks involving images is content-based image classification. This is a difficult task especially because the low-level features used to digitally describe images usually capture little information about the semantics of the images. In this paper, we tackle this difficulty by enriching the semantic content of the image representation by using external knowledge. The underlying hypothesis of our work is that creating a more semantically rich representation for images would yield higher machine learning performances, without the need to modify the learning algorithms themselves. The external semantic information is presented under the form of non-positional image labels, therefore positioning our work in a weakly supervised context. Two approaches are proposed: the first one leverages the labels into the visual vocabulary construction algorithm, the result being dedicated visual vocabularies. The second approach adds a filtering phase as a pre-processing of the vocabulary construction. Known positive and known negative sets are constructed and features that are unlikely to be associated with the objects denoted by the labels are filtered. We apply our proposition to the task of content-based image classification and we show that semantically enriching the image representation yields higher classification performances than the baseline representation.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.3233/IDA-140702},
    URL_paper = {http://arxiv.org/pdf/1512.04605.pdf},
    journal = {Intelligent Data Analysis},
    keywords = {bag-of-features representation,image numerical representation,semantic-enriched representation,semisupervised learning,visual vocabulary construction},
    number = {1},
    pages = {161--185},
    title = {{Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context}},
    volume = {19},
    year = {2015},
    url = {http://dx.doi.org/10.3233/IDA-140702}
}
@article{Rizoiu2015,
    abstract = {We present CommentWatcher, an open source tool aimed at analyzing discussions on web forums. Constructed as a web platform, CommentWatcher features automatic mass fetching of user posts from forum on multiple sites, extracting topics, visualizing the topics as an expression cloud and exploring their temporal evolution. The underlying social network of users is simultaneously constructed using the citation relations between users and visualized as a graph structure. Our platform addresses the issues of the diversity and dynamics of structures of webpages hosting the forums by implementing a parser architecture that is independent of the HTML structure of webpages. This allows easy on-the-fly adding of new websites. Two types of users are targeted: end users who seek to study the discussed topics and their temporal evolution, and researchers in need of establishing a forum benchmark dataset and comparing the performances of analysis tools.},
    annote = {NULL},
    archivePrefix = {arXiv},
    arxivId = {1504.07459},
    author = {Rizoiu, Marian-Andrei and Guille, Adrien and Velcin, Julien},
    eprint = {1504.07459},
    URL_paper = {http://arxiv.org/pdf/1504.07459.pdf},
    journal = {arXiv preprint},
    month = {apr},
    title = {{CommentWatcher: An Open Source Web-based platform for analyzing discussions on web forums}},
    url = {http://arxiv.org/abs/1504.07459},
    year = {2015},
    URL_code = {https://github.com/behavioral-ds/CommentWatcher}
}
@inproceedings{Kim2015a,
    abstract = {Evolutionary clustering aims at capturing the temporal evolution of clusters. This issue is particularly important in the context of social media data that are naturally temporally driven. In this paper, we propose a new probabilistic model-based evolutionary clustering technique. The Temporal Multinomial Mixture (TMM) is an extension of classical mixture model that optimizes feature co-occurrences in the trade-off with temporal smoothness. Our model is evaluated for two recent case studies on opinion aggregation over time. We compare four different probabilistic clustering models and we show the superiority of our proposal in the task of instance-oriented clustering.},
    address = {Vienna, Austria.},
    archivePrefix = {arXiv},
    arxivId = {1601.02300},
    author = {Kim, Young-Min and Velcin, Julien and Bonnevay, St{\'{e}}phane and Rizoiu, Marian-Andrei},
    booktitle = {European Conference on Information Retrieval, Proceedings of the 37th},
    doi = {10.1007/978-3-319-16354-3_66},
    eprint = {1601.02300},
    URL_paper = {http://arxiv.org/pdf/1601.02300.pdf},
    isbn = {9783319163536},
    issn = {16113349},
    keywords = {evolutionary clustering,mixture model,temporal analysis.},
    pages = {593--604},
    publisher = {Springer International Publishing Switzerland},
    series = {ECIR '15},
    title = {{Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering}},
    url = {http://link.springer.com/10.1007/978-3-319-16354-3{\_}66},
    volume = {9022},
    year = {2015}
}
@article{Rizoiu2016a,
    abstract = {We propose ClusPath, a novel algorithm for detecting general evolution tendencies in a population of entities. We show how abstract notions, such as the Swedish socio-economical model (in a political dataset) or the companies fiscal optimization (in an eco- nomical dataset) can be inferred from low-level descriptive features. Such high-level regularities in the evolution of entities are detected by combining spatial and temporal features into a spatio-temporal dissimilarity measure and using semi-supervised clustering techniques. The relations between the evolution phases are modeled using a graph structure, inferred si- multaneously with the partition, by using a “slow changing world” assumption. The idea is to ensure a smooth passage for entities along their evolution paths, which catches the long- term trends in the dataset. Additionally, we also provide a method, based on an evolutionary algorithm, to tune the parameters of ClusPath to new, unseen datasets. This method assesses the fitness of a solution using four opposed quality measures and proposes a balanced com- promise.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Bonnevay, St{\'{e}}phane and Lallich, St{\'{e}}phane},
    doi = {10.1007/s10618-015-0445-7},
    URL_paper = {http://arxiv.org/pdf/1512.03501.pdf},
    issn = {1384-5810},
    journal = {Data Mining and Knowledge Discovery},
    keywords = {Pareto front estimation,detection of long-term trends,evolutionary clustering,semi-supervised clustering,temporal cluster graph,temporal clustering},
    month = {sep},
    number = {5},
    pages = {1324--1349},
    series = {EKML/PKDD{\~{}}'14},
    title = {{ClusPath: a temporal-driven clustering to infer typical evolution paths}},
    url = {http://link.springer.com/10.1007/s10618-015-0445-7},
    volume = {30},
    year = {2016},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_PKDD-2016_slides.pdf},
    URL_code = {https://github.com/behavioral-ds/cluspath-distrib}
}


@article{kong2020cikm,
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Xie, Lexing},
    title={Describing and Predicting Online Items with Reshare Cascades via Dual Mixture Self-exciting Processes},
    booktitle={ACM International Conference on Information and Knowledge Management (CIKM'20)},
    pages={645--654},
    year={2020},
    url_Paper = {https://arxiv.org/pdf/2001.11132.pdf},
    url_Code = {https://github.com/qykong/dual-mixture-hawkes-processes},
    abstract = {It is well-known that online behavior is long-tailed, with most cascaded actions being short and a few being very long. A prominent drawback in generative models for online events is the inability to describe unpopular items well. This work addresses these shortcomings by proposing dual mixture self-exciting processes to jointly learn from groups of cascades. We first start from the observation that maximum likelihood estimates for content virality and influence decay are separable in a Hawkes process. Next, our proposed model, which leverages a Borel mixture model and a kernel mixture model, jointly models the unfolding of a heterogeneous set of cascades. When applied to cascades of the same online items, the model directly characterizes their spread dynamics and supplies interpretable quantities, such as content virality and content influence decay, as well as methods for predicting the final content popularities. On two retweet cascade datasets --- one relating to YouTube videos and the second relating to controversial news articles --- we show that our models capture the differences between online items at the granularity of items, publishers and categories. In particular, we are able to distinguish between far-right, conspiracy, controversial and reputable online news articles based on how they diffuse through social media, achieving an F1 score of 0.945. On holdout datasets, we show that the dual mixture model provides, for reshare diffusion cascades especially unpopular ones, better generalization performance and, for online items, accurate item popularity predictions.},
    url = {https://doi.org/10.1145/3340531.3411861}
}

@inproceedings{kong2020modeling,
    title={Modeling Information Cascades with Self-exciting Processes via Generalized Epidemic Models},
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle={ACM International Conference on Web Search and Data Mining (WSDM'20)},
    year={2020},
    abstract = {Epidemic models and self-exciting processes are two types of models used to describe diffusion phenomena online and offline. These models were originally developed in different scientific communities, and their commonalities are under-explored. This work establishes, for the first time, a general connection between the two model classes via three new mathematical components. The first is a generalized version of stochastic Susceptible-Infected-Recovered (SIR) model with arbitrary recovery time distributions; the second is the relationship between the (latent and arbitrary) recovery time distribution, recovery hazard function, and the infection kernel of self-exciting processes; the third includes methods for simulating, fitting, evaluating and predicting the generalized process. On three large Twitter diffusion datasets, we conduct goodness-of-fit tests and holdout log-likelihood evaluation of self-exciting processes with three infection kernels --- exponential, power-law and Tsallis Q-exponential. We show that the modeling performance of the infection kernels varies with respect to the temporal structures of diffusions, and also with respect to user behavior, such as the likelihood of being bots. We further improve the prediction of popularity by combining two models that are identified as complementary by the goodness-of-fit tests.},
    url_Paper = {https://arxiv.org/abs/1910.05451},
    url_Code = {https://github.com/qykong/generalized-sir-and-hawkes},
    url = {https://doi.org/10.1145/3336191.3371821}
}

@inproceedings{wu2019estimating,
    address = {Austin, TX, USA},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW '19)},
    title = {Estimating Attention Flow in Online Video Networks},
    year = {2019},
    abstract = {Online videos have shown tremendous increase in Internet traffic. Most video hosting sites implement recommender systems, which connect the videos into a directed network and conceptually act as a source of pathways for users to navigate. At present, little is known about how human attention is allocated over such large-scale networks, and about the impacts of the recommender systems. In this paper, we first construct the Vevo network -- a YouTube video network with 60,740 music videos interconnected by the recommendation links, and we collect their associated viewing dynamics. This results in a total of 310 million views every day over a period of 9 weeks. Next, we present large-scale measurements that connect the structure of the recommendation network and the video attention dynamics. We use the bow-tie structure to characterize the Vevo network and we find that its core component (23.1% of the videos), which occupies most of the attention (82.6% of the views), is made out of videos that are mainly recommended among themselves. This is indicative of the links between video recommendation and the inequality of attention allocation. Finally, we address the task of estimating the attention flow in the video recommendation network. We propose a model that accounts for the network effects for predicting video popularity, and we show it consistently outperforms the baselines. This model also identifies a group of artists gaining attention because of the recommendation network. Altogether, our observations and our models provide a new set of tools to better understand the impacts of recommender systems on collective social attention.},
    url = {https://dl.acm.org/doi/10.1145/3359285},
    url_Abstract = {https://arxiv.org/abs/1908.07123},
    url_Paper = {https://arxiv.org/pdf/1908.07123.pdf},
    url_Data = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TORICY},
    url_Code = {https://github.com/avalanchesiqi/networked-popularity},
    url_Slides = {http://users.cecs.anu.edu.au/~siqi.wu/files/cscw2019slides.pdf},
    url_Blog = {https://medium.com/acm-cscw/how-does-the-network-of-youtube-music-videos-drive-attention-42130144b59b}
}

@inproceedings{zhang2019efficient,
    author    = {Rui Zhang and Christian Walder and Marian-Andrei Rizoiu and Lexing Xie},
    title     = {Efﬁcient Non-parametric Bayesian Hawkes Processes},
    booktitle={The 28th International Joint Conference on Artificial Intelligence (IJCAI-19)},
    year={2019},
    address = {Macau, China},
    abstract = {In this paper, we develop an efﬁcient non-parametric Bayesian estimation of the kernel function of Hawkes processes. The non-parametric Bayesian approach is important because it provides ﬂexible Hawkes kernels and quantiﬁes their uncertainty. Our method is based on the cluster representation of Hawkes processes. Utilizing the stationarity of the Hawkes process, we efﬁciently sample random branching structures and thus, we split the Hawkes process into clusters of Poisson processes. We derive two algorithms — a block Gibbs sampler and a maximum a posteriori estimator based on expectation maximization — and we show that our methods have a linear time complexity, both theoretically and empirically. On synthetic data, we show our methods to be able to infer ﬂexible Hawkes triggering kernels. On two large scale Twitter diffusion datasets, we show that our methods outperform the current state-of-the-art in goodness-of-ﬁt and that the time complexity is linear in the size of the dataset. We also observe that on diffusions related to online videos, the learned kernels reﬂect the perceived longevity for different content types such as music or pets videos.},
    url_Paper = {https://arxiv.org/abs/1810.03730},
    url_Code = {https://github.com/RuiZhang2016/Efficient-Nonparametric-Bayesian-Hawkes-Processes}
}

@Article{Kim2019,
    author={Kim, Dongwoo and Graham, Timothy and Wan, Zimin and Rizoiu, Marian-Andrei},
    title={Analysing user identity via time-sensitive semantic edit distance (t-SED): a case study of Russian trolls on Twitter},
    journal={Journal of Computational Social Science},
    year={2019},
    month={Jul},
    day={01},
    volume={2},
    number={2},
    pages={331--351},
    abstract={In the digital era, individuals are increasingly profiled and grouped based on the traces that they leave behind in online social networks such as Twitter and Facebook. In this paper, we develop and evaluate a novel text analysis approach for studying user identity and social roles by redefining identity as a sequence of timestamped items (e.g., tweet texts). We operationalise this idea by developing a novel text distance metric, the time-sensitive semantic edit distance (t-SED), which accounts for the temporal context across multiple traces. To evaluate this method, we undertake a case study of Russian online-troll activity within US political discourse. The novel metric allows us to classify the social roles of trolls based on their traces, in this case tweets, into one of the predefined categories left-leaning, right-leaning, and news feed. We show the effectiveness of the t-SED metric to measure the similarities between tweets while accounting for the temporal context, and we use novel data visualisation techniques and qualitative analysis to uncover new empirical insights into Russian troll activity that have not been identified in the previous work. In addition, we highlight a connection with the field of actor--network theory and the related hypotheses of Gabriel Tarde, and we discuss how social sequence analysis using t-SED may provide new avenues for tackling a longstanding problem in social theory: how to analyse society without separating reality into micro vs. macro-levels.},
    issn={2432-2725},
    doi={10.1007/s42001-019-00051-x},
    url={https://doi.org/10.1007/s42001-019-00051-x},
    url_paper = {https://arxiv.org/pdf/1901.05228.pdf}
}

@inproceedings{kong2019linking,
    address = {Melbourne, VIC, Australia},
    author = {Kong, Quyu},
    booktitle = {ACM International Conference on Web Search and Data Mining (WSDM '19), Doctoral Consortium},
    title = {{Linking Epidemic Models and Hawkes Point Processes for Modeling Information Diffusion}},
    year = {2019},
    abstract = {Epidemic models and Hawkes point process models are two common model classes for information diffusion. Recent work has revealed the equivalence between the two for information diffusion modeling. This allows tools created for one class of models to be applied to another. However, epidemic models and Hawkes point processes can be connected in more ways. This thesis aims to develop a rich set of mathematical equivalences and extensions, and use them to ask and answer questions in social media and beyond. Specifically, we show our plan of generalizing the equivalence of the two model classes by extending it to Hawkes point process models with arbitrary memory kernels. We then outline a rich set of quantities describing diffusion, including diffusion size and extinction probability, introduced in the fields where the models are originally designed. Lastly, we discuss some novel applications of these quantities in a range of problems such as popularity prediction and popularity intervention.},
    url_Paper = {http://cm.cecs.anu.edu.au/documents/kong_wsdm2019_dc.pdf}
}

@inproceedings{Rizoiu2018a,
    abstract = {Serious concerns have been raised about the role of `socialbots' in manipulating public opinion and influencing the outcome of elections by retweeting partisan content to increase its reach. Here we analyze the role and influence of socialbots on Twitter by determining how they contribute to retweet diffusions. We collect a large dataset of tweets during the 1st U.S. presidential debate in 2016 and we analyze its 1.5 million users from three perspectives: user influence, political behavior (partisanship and engagement) and botness. First, we define a measure of user influence based on the user's active contributions to information diffusions, i.e. their tweets and retweets. Given that Twitter does not expose the retweet structure -- it associates all retweets with the original tweet -- we model the latent diffusion structure using only tweet time and user features, and we implement a scalable novel approach to estimate influence over all possible unfoldings. Next, we use partisan hashtag analysis to quantify user political polarization and engagement. Finally, we use the BotOrNot API to measure user botness (the likelihood of being a bot). We build a two-dimensional "polarization map" that allows for a nuanced analysis of the interplay between botness, partisanship and influence. We find that not only are socialbots more active on Twitter -- starting more retweet cascades and retweeting more -- but they are 2.5 times more influential than humans, and more politically engaged. Moreover, pro-Republican bots are both more influential and more politically engaged than their pro-Democrat counterparts. However we caution against blanket statements that software designed to appear human dominates politics-related activity on Twitter. Firstly, it is known that accounts controlled by teams of humans (e.g. organizational accounts) are often identified as bots. Secondly, we find that many highly influential Twitter users are in fact pro-Democrat and that most pro-Republican users are mid-influential and likely to be human (low botness).},
    address = {Stanford, CA, USA},
    author = {Rizoiu, Marian-Andrei and Graham, Timothy and Zhang, Rui and Zhang, Yifei and Ackland, Robert and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    title = {{{\#}DebateNight: The Role and Influence of Socialbots on Twitter During the 1st 2016 U.S. Presidential Debate}},
    year = {2018},
    url_Abstract = {https://arxiv.org/abs/1802.09808},
    url_Paper = {https://arxiv.org/pdf/1802.09808.pdf},
    url_Code = {https://github.com/computationalmedia/cascade-influence},
    url_Slides = {http://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICWSM-2018_slides.pdf}
}

@inproceedings{wu2018beyond,
    address = {Stanford, CA, USA},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    title = {Beyond Views: Measuring and Predicting Engagement in Online Videos},
    year = {2018},
    abstract = {The share of videos in the internet traffic has been growing, therefore understanding how videos capture attention on a global scale is also of growing importance. Most current research focus on modeling the number of views, but we argue that video engagement, or time spent watching is a more appropriate measure for resource allocation problems in attention, networking, and promotion activities. In this paper, we present a first large-scale measurement of video-level aggregate engagement from publicly available data streams, on a collection of 5.3 million YouTube videos published over two months in 2016. We study a set of metrics including time and the average percentage of a video watched. We define a new metric, relative engagement, that is calibrated against video properties and strongly correlate with recognized notions of quality. Moreover, we find that engagement measures of a video are stable over time, thus separating the concerns for modeling engagement and those for popularity -- the latter is known to be unstable over time and driven by external promotions. We also find engagement metrics predictable from a cold-start setup, having most of its variance explained by video context, topics and channel information -- R2=0.77. Our observations imply several prospective uses of engagement metrics -- choosing engaging topics for video production, or promoting engaging videos in recommender systems.},
    url_Abstract = {https://arxiv.org/abs/1709.02541},
    url_Paper = {https://arxiv.org/pdf/1709.02541.pdf},
    url_Code = {https://github.com/avalanchesiqi/youtube-engagement},
    url_Slides = {http://cm.cecs.anu.edu.au/documents/wu_icwsm2018_slides.pdf}
}

@inproceedings{Mishra2018rnn-mas,
    address = {Stanford, CA, USA},
    author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    pages = {1--10},
    title = {{Modeling Popularity in Asynchronous Social Media Streams with Recurrent Neural Networks}},
    year = {2018},
    abstract = {Understanding and predicting the popularity of online items is an important open problem in social media analysis. Considerable progress has been made recently in data-driven predictions, and in linking popularity to external promotions. However, the existing methods typically focus on a single source of external influence, whereas for many types of online content such as YouTube videos or news articles, attention is driven by multiple heterogeneous sources simultaneously - e.g. microblogs or traditional media coverage. Here, we propose RNN-MAS, a recurrent neural network for modeling asynchronous streams. It is a sequence generator that connects multiple streams of different granularity via joint inference. We show RNN-MAS not only to outperform the current state-of-the-art Youtube popularity prediction system by 17%, but also to capture complex dynamics, such as seasonal trends of unseen influence. We define two new metrics: promotion score quantifies the gain in popularity from one unit of promotion for a Youtube video; the loudness level captures the effects of a particular user tweeting about the video. We use the loudness level to compare the effects of a video being promoted by a single highly-followed user (in the top 1% most followed users) against being promoted by a group of mid-followed users. We find that results depend on the type of content being promoted: superusers are more successful in promoting Howto and Gaming videos, whereas the cohort of regular users are more influential for Activism videos. This work provides more accurate and explainable popularity predictions, as well as computational tools for content producers and marketers to allocate resources for promotion campaigns.},
    url_Abstract = {https://arxiv.org/abs/1804.02101},
    url_Paper = {https://arxiv.org/pdf/1804.02101.pdf},
    url_Code = {https://github.com/computationalmedia/rnn-mas}
}

@incollection{Rizoiu2018HPE,
    author = {Rizoiu, Marian-Andrei and Lee, Young and Mishra, Swapnil and Xie, Lexing},
    title = {{A Tutorial on Hawkes Processes for Events in Social Media}},
    booktitle = {Frontiers of Multimedia Research},
    editor = {Chang, Shih-Fu},
    year = {2018},
    isbn = {978-1-97000-107-5},
    pages = {191--218},
    numpages = {28},
    abstract = {This chapter provides an accessible introduction for point processes, and especially Hawkes processes, for modeling discrete, inter-dependent events over continuous time. We start by reviewing the definitions and the key concepts in point processes. We then introduce the Hawkes process, its event intensity function, as well as schemes for event simulation and parameter estimation. We also describe a practical example drawn from social media data - we show how to model retweet cascades using a Hawkes self-exciting process. We presents a design of the memory kernel, and results on estimating parameters and predicting popularity. The code and sample event data are available as an online appendix.},
    url = {https://doi.org/10.1145/3122865.3122874},
    url_Abstract = {https://arxiv.org/abs/1708.06401},
    url_Paper = {https://arxiv.org/pdf/1708.06401.pdf},
    url_Code= {https://github.com/s-mishra/featuredriven-hawkes},
    doi = {10.1145/3122865.3122874},
    acmid = {3122874},
    publisher = {Association for Computing Machinery and Morgan \& Claypool},
    address = {New York, NY, USA}
}

@inproceedings{rizoiu2018sir,
    title={{SIR-Hawkes}: Linking Epidemic Models and {Hawkes} Processes to Model Diffusions in Finite Populations},
    author={Rizoiu, Marian-Andrei and Mishra, Swapnil and Kong, Quyu and Carman, Mark and Xie, Lexing},
    address = {Lyon, France},
    booktitle = {Proceedings of the 2018 World Wide Web Conference},
    series = {WWW '18},
    year={2018},
    abstract = {Among the statistical tools for online information diffusion modeling, both epidemic models and Hawkes point processes are popular choices. The former originate from epidemiology, and consider information as a viral contagion which spreads into a population of online users. The latter have roots in geophysics and finance, view individual actions as discrete events in continuous time, and modulate the rate of events according to the self-exciting nature of event sequences. Here, we establish a novel connection between these two frameworks. Namely, the rate of events in an extended Hawkes model is identical to the rate of new infections in the Susceptible-Infected-Recovered (SIR) model after marginalizing out recovery events -- which are unobserved in a Hawkes process. This result paves the way to apply tools developed for SIR to Hawkes, and vice versa. It also leads to HawkesN, a generalization of the Hawkes model which accounts for a finite population size. Finally, we derive the distribution of cascade sizes for HawkesN, inspired by methods in stochastic SIR. Such distributions provide nuanced explanations to the general unpredictability of popularity: the distribution for diffusion cascade sizes tends to have two modes, one corresponding to large cascade sizes and another one around zero.},
    doi = {10.1145/3178876.3186108},
    eprint = {1711.01679},
    isbn = {9781450356398},
    pages = {419--428},
    url_Abstract = {https://arxiv.org/abs/1711.01679},
    url_Paper = {https://arxiv.org/pdf/1711.01679.pdf},
    url_Code = {https://github.com/computationalmedia/sir-hawkes},
    url_Slides = {http://www.rizoiu.eu/documents/research/presentations/RIZOIU_WWW-2018_slides.pdf}
}

@inproceedings{kong2018will,
    title={Will This Video Go Viral? Explaining and Predicting the Popularity of Youtube Videos},
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Wu, Siqi and Xie, Lexing},
    address = {Lyon, France},
    booktitle = {Companion Proceedings of the The Web Conference 2018 - Demos},
    series = {WWW '18},
    year={2018},
    abstract = {What makes content go viral? Which videos become popular and why others don't? Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. Hipie -- an interactive visualization system -- is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs the Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and to predict future popularity, to identify viral videos, and to estimate responses to online promotion. },
    doi = {10.1145/3184558.3186972},
    url_Abstract = {https://arxiv.org/abs/1801.04117},
    url_Paper = {https://arxiv.org/pdf/1801.04117.pdf},
    url_Code = {https://github.com/computationalmedia/hipie}
}

@inproceedings{Rizoiu2017HIP,
    address = {Perth, Australia},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing and Sanner, Scott and Cebrian, Manuel and Yu, Honglin and {Van Hentenryck}, Pascal},
    booktitle = {World Wide Web 2017, International Conference on},
    pages = {735--744},
    title = {Expecting to be {HIP}: Hawkes Intensity Processes for Social Media Popularity},
    year = {2017},
    doi = {10.1145/3038912.3052650},
    isbn = {9781450349130},
    abstract = {Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor -- the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors -- sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6% from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.},
    url_Abstract={https://arxiv.org/abs/1602.06033},
    url_Paper = {https://arxiv.org/pdf/1602.06033},
    url_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_WWW-2017_slides.pdf},
    url_code = {https://github.com/andrei-rizoiu/hip-popularity}
}

@inproceedings{Rizoiu2017promo,
    address = {Montreal, Canada},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {The International AAAI Conference on Web and Social Media (ICWSM)},
    pages = {182--191},
    title = {Online Popularity under Promotion: Viral Potential, Forecasting, and the Economics of Time},
    year = {2017},
    abstract = {Modeling the popularity dynamics of an online item is an important open problem in computational social science. This paper presents an in-depth study of popularity dynamics under external promotions, especially in predicting popularity jumps of online videos, and determining effective and efficient schedules to promote online content. The recently-proposed Hawkes Intensity Process (HIP) models popularity as a non-linear interplay between exogenous stimuli and the endogenous reaction. We propose two novel metrics based on HIP: to describe popularity gain per unit of promotion, and to quantify the time it takes for such effects to unfold. We make increasingly accurate forecasts of future popularity by including information about the intrinsic properties of the video, promotions it receives, and the non-linear effects of popularity ranking. We illustrate by simulation the interplay between the unfolding of popularity over time, and the time-sensitive value of resources. Lastly, our model lends a novel explanation of the commonly adopted periodic and constant promotion strategy in advertising, as increasing the perceived viral potential. This study provides quantitative guidelines about setting promotion schedules considering content virality, timing, and economics.},
    url_Abstract={https://arxiv.org/abs/1703.01012},
    url_Paper = {https://arxiv.org/pdf/1703.01012},
    url_code = {https://github.com/andrei-rizoiu/hip-popularity},
    url_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICWSM-2017_slides.pdf}
}

@article{Graham2017,
    abstract = {Despite its wide-ranging influence in social science, the field of actor-network theory (ANT) has proven difficult to operationalise quantitatively. Although social network analysis (SNA) and ANT appear to share certain affinities (e.g., the term ‘network'), attempts to develop an ANT approach to SNA (and vice versa) have stumbled upon fundamental problems or ‘discontinuities' between them (Venturini, Munk, and Jacomy, 2016). These problems constitute serious obstacles for progressing ANT research using digital data. In this paper, we propose hypergraphs as one way forward to operationalising ANT. Broadly, we term this method ‘social hypergraph analysis' (SHA). We outline SHA in this paper and apply it to analyse social media data, using a case study of the anti-vaccination debate on Twitter.},
    author = {Graham, Timothy and Ackland, Robert and Rizoiu, Marian-Andrei and Swift, Ben},
    journal = {67th Annual Conference of the International Communication Association},
    title = {{"Social Hypergraph Analysis": Towards an operationalisation of Actor-Network Theory Using hypergraphs}},
    year = {2017},
    url_Paper = {https://eprints.qut.edu.au/127948/1/127948.pdf}
}

@inproceedings{Mishra2016,
    title = {{Feature Driven and Point Process Approaches for Popularity Prediction}},
    author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {Proceedings of the 25th ACM International Conference on Information and Knowledge Management},
    series = {CIKM '16},
    address = {Indianapolis, IN, USA},
    doi = {10.1145/2983323.2983812},
    keywords = {social media; self-exciting point process; information diffusion; cascade prediction},
    year = {2016},
    abstract = {Predicting popularity, or the total volume of information outbreaks, is an important subproblem for understanding collective behavior in networks. Each of the two main types of recent approaches to the problem, feature-driven and generative models, have desired qualities and clear limitations. This paper bridges the gap between these solutions with a new hybrid approach and a new performance benchmark. We model each social cascade with a marked Hawkes self-exciting point process, and estimate the content virality, memory decay, and user influence. We then learn a predictive layer for popularity prediction using a collection of cascade history. To our surprise, Hawkes process with a predictive overlay outperform recent feature-driven and generative approaches on existing tweet data [44] and a new public benchmark on news tweets. We also found that a basic set of user features and event time summary statistics performs competitively in both classification and regression tasks, and that adding point process information to the feature set further improves predictions. From these observations, we argue that future work on popularity prediction should compare across feature-driven and generative modeling approaches in both classification and regression tasks.},
    url_Abstract={https://arxiv.org/abs/1608.04862},
    url_Paper = {https://arxiv.org/pdf/1608.04862.pdf},
    url_Presentation_Page = {http://cm.cecs.anu.edu.au/post/fdhawkesforpopularity/},
    url_Slides = {http://cm.cecs.anu.edu.au/documents/smishra_cikm16_presentation.pdf},
    url_code = {https://git.io/v6rIN}
}

@inproceedings{Rizoiu2016,
    abstract = {The cumulative effect of collective participation online has an important and adverse impact on individual privacy. As an online system evolves over time, new digital traces of individual behavior may uncover previously hidden statistical links between an individual’s past actions and her private traits. Furthermore, this de-anonymization trend may not be observable when analyzing short or medium time-span snapshots of data. To quantify this effect, we analyze the evolution of individual privacy loss by studying the 13-year long edit history of Wikipedia, including more than 117,523 different users performing 188,805,088 edits. We trace each Wikipedia’s contributor using apparently harmless features, such as the number of edits performed on predefined broad categories in a given time period (e.g. Mathematics, Culture or Nature). We show that even at this unspecific level of identification, it is possible to use off-the-shelf machine learning algorithms to uncover usually undisclosed private traits, such as gender, religion or education. We provide empirical evidence that the prediction accuracy for almost all private traits consistently improves over time. Moreover, we observe that the system also shows improved prediction for users who participated in the system during “safe” periods — periods where a given individual’s private traits could not be — showing that de-anonymization threats are hard to foresee as online systems evolve. Insights from this work should help users, system designers, and policy makers understand and debate the design and long-term effects of online content creation systems.},
    address = {San Francisco, CA, USA},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing and Caetano, Tiberio and Cebrian, Manuel},
    booktitle = {Proceedings of the 9th ACM International Conference on Web Search and Data Mining},
    series = {WSDM '16},
    doi = {10.1145/2835776.2835798},
    keywords = {de-anonymization,online privacy,temporal loss of privacy},
    title = {{Evolution of Privacy Loss on Wikipedia}},
    year = {2016},
    url_Slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_WSDM-2016_slides.pdf}, % use this to distribute the slides of the presentation
    %url_Link = {http://my.repo.com/awesome.paper.pdf} % use this to distribute the link to your paper (e.g. journal entry)
    url_Paper = {http://arxiv.org/pdf/1512.03523.pdf}, % use this to distribute the link ot the paper. It can be either a http:// link or a local link relative to the bib file (somthing like publications/paper.pdf)
    url_Presentation_Page = {http://cm.cecs.anu.edu.au/post/wikiprivacy/}
}

@inproceedings{Mihaita2019,
    abstract = {Congestion prediction represents a major priority for traffic management centres around the world to ensure timely incident response handling. The increasing amounts of generated traffic data have been used to train machine learning predictors for traffic, however this is a challenging task due to inter-dependencies of traffic flow both in time and space. Recently, deep learning techniques have shown significant prediction improvements over traditional models, however open questions remain around their applicability, accuracy and parameter tuning. This paper proposes an advanced deep learning framework for simultaneously predicting the traffic flow on a large number of monitoring stations along a highly circulated motorway in Sydney, Australia, including exit and entry loop count stations, and over varying training and prediction time horizons. The spatial and temporal features extracted from the 36.34 million data points are used in various deep learning architectures that exploit their spatial structure (convolutional neuronal networks), their temporal dynamics (recurrent neuronal networks), or both through a hybrid spatio-temporal modelling (CNN-LSTM). We show that our deep learning models consistently outperform traditional methods, and we conduct a comparative analysis of the optimal time horizon of historical data required to predict traffic flow at different time points in the future.},
    address = {Auckland, New Zealand},
    archivePrefix = {arXiv},
    arxivId = {1907.06356},
    author = {Mihaita, Adriana-Simona and Li, Haowen and He, Zongyang and Rizoiu, Marian-Andrei},
    booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
    doi = {10.1109/ITSC.2019.8916852},
    eprint = {1907.06356},
    URL_paper = {https://arxiv.org/pdf/1907.06356.pdf},
    isbn = {978-1-5386-7024-8},
    keywords = {,BPNN,CNN,Deep learning,LSTM,Motorway flow predicting,Short- versus long-term prediction},
    month = {oct},
    pages = {1683--1690},
    publisher = {IEEE},
    title = {{Motorway Traffic Flow Prediction using Advanced Deep Learning}},
    url = {https://ieeexplore.ieee.org/document/8916852/},
    year = {2019}
}
@article{Kern2020,
    abstract = {Work is thought to be more enjoyable and beneficial to individuals and society when there is congruence between one's personality and one's occupation. We provide large-scale evidence that occupations have distinctive psychological profiles, which can successfully be predicted from linguistic information unobtrusively collected through social media. Based on 128,279 Twitter users representing 3,513 occupations, we automatically assess user personalities and visually map the personality profiles of different professions. Similar occupations cluster together, pointing to specific sets of jobs that one might be well suited for. Observations that contradict existing classifications may point to emerging occupations relevant to the 21st century workplace. Findings illustrate how social media can be used to match people to their ideal occupation.},
    author = {Kern, Margaret L. and McCarthy, Paul X. and Chakrabarty, Deepanjan and Rizoiu, Marian-Andrei},
    doi = {10.1073/pnas.1917942116},
    URL_paper = {https://www.pnas.org/content/pnas/116/52/26459.full.pdf},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences},
    month = {dec},
    number = {52},
    pages = {26459--26464},
    title = {{Social media-predicted personality traits and values can help match people to their ideal jobs}},
    url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1917942116},
    volume = {116},
    year = {2019},
    URL_code = {https://github.com/behavioral-ds/VocationMap}
}
@inproceedings{Mihaita2019a,
    abstract = {Predicting traffic incident duration is a major challenge for many traffic centres around the world. Most research studies focus on predicting the incident duration on motorways rather than arterial roads, due to a high network complexity and lack of data. In this paper we propose a bi-level framework for predicting the accident duration on arterial road networks in Sydney, based on operational requirements of incident clearance target which is less than 45 minutes. Using incident baseline information, we first deploy a classification method using various ensemble tree models in order to predict whether a new incident will be cleared in less than 45min or not. If the incident was classified as short-term, then various regression models are developed for predicting the actual incident duration in minutes by incorporating various traffic flow features. After outlier removal and intensive model hyper-parameter tuning through randomized search and cross-validation, we show that the extreme gradient boost approach outperformed all models, including the gradient-boosted decision-trees by almost 53{\%}. Finally, we perform a feature importance evaluation for incident duration prediction and show that the best prediction results are obtained when leveraging the real-time traffic flow in vicinity road sections to the reported accident location.},
    address = {Singapore},
    archivePrefix = {arXiv},
    arxivId = {1905.12254},
    author = {Mihaita, Adriana-Simona and Liu, Zheyuan and Cai, Chen and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 26th ITS World Congress},
    eprint = {1905.12254},
    URL_paper = {http://arxiv.org/pdf/1905.12254.pdf},
    month = {may},
    pages = {1--12},
    title = {{Arterial incident duration prediction using a bi-level framework of extreme gradient-tree boosting}},
    url = {http://arxiv.org/abs/1905.12254},
    year = {2019}
}
@inproceedings{Ram2019,
    abstract = {Social influence has become an incredibly powerful tool with the advent of so-cial media; as such, measuring and understanding it has become an increasingly importantendeavor.   Understanding  and  measuring  online  social  influence  are  essential  for  sociolo-gists  studying  modern  events,  such  as  understanding  how  ‘influencers’  promote  productseffectively,  how Queenslanders swung the last Australian election or how anti-vaccinationsentiment spreads.  Social influence has been studied thoroughly in both the computationaland  social  science  contexts,  however  there  is  often  a  disconnect  between  the  two  fields.Computational techniques are often not grounded in social theory, nor are they validatedby humans.  Conversely, social scientists rarely exploit the vast amounts of data from socialmedia and they lack the tractable computational models to quantify influence efficiently.},
    address = {Adelaide, Australia},
    author = {Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {Australian Social Network Analysis Conference (ASNAC'19)},
    URL_paper = {https://opus.lib.uts.edu.au/bitstream/10453/137931/1/main.pdf},
    pages = {2},
    title = {{A social science-grounded approach for quantifying online social influence}},
    year = {2019}
}

@inproceedings{Dawson2019,
    abstract = {Labour demand and skill shortages have historically been difficult to assess given the high costs of conducting representative surveys and the inherent delays of these indicators. This is particularly consequential for fast developing skills and occupations, such as those relating to Data Science and Analytics (DSA). This paper develops a data-driven solution to detecting skill shortages from online job advertisements (ads) data. We first propose a method to generate sets of highly similar skills based on a set of seed skills from job ads. This provides researchers with a novel method to adaptively select occupations based on granular skills data. Next, we apply this adaptive skills similarity technique to a dataset of over 6.7 million Australian job ads in order to identify occupations with the highest proportions of DSA skills. This uncovers 306,577 DSA job ads across 23 occupational classes from 2012-2019. Finally, we propose five variables for detecting skill shortages from online job ads: (1) posting frequency; (2) salary levels; (3) education requirements; (4) experience demands; and (5) job ad posting predictability. This contributes further evidence to the goal of detecting skills shortages in real-time. In conducting this analysis, we also find strong evidence of skills shortages in Australia for highly technical DSA skills and occupations. These results provide insights to Data Science researchers, educators, and policy-makers from other advanced economies about the types of skills that should be cultivated to meet growing DSA labour demands in the future.},
    address = {Los Angeles, CA, USA},
    archivePrefix = {arXiv},
    arxivId = {1911.02302},
    author = {Dawson, Nikolas and Rizoiu, Marian-Andrei and Johnston, Benjamin and Williams, Mary-Anne},
    booktitle = {2019 IEEE International Conference on Big Data (Big Data)},
    doi = {10.1109/BigData47090.2019.9005967},
    eprint = {1911.02302},
    URL_paper = {http://arxiv.org/pdf/1911.02302.pdf},
    isbn = {978-1-7281-0858-2},
    keywords = {Big Data,Data Science,Labour Demand,Online Job Advertisements,Skill Shortages},
    month = {dec},
    pages = {1637--1643},
    publisher = {IEEE},
    title = {{Adaptively selecting occupations to detect skill shortages from online job ads}},
    url = {https://ieeexplore.ieee.org/document/9005967/},
    year = {2019}
}
@inproceedings{Zhang2020,
    abstract = {The Hawkes process has been widely applied to modeling self-exciting events, including neuron spikes, earthquakes and tweets. To avoid designing parametric kernel functions and to be able to quantify the prediction confidence, non-parametric Bayesian Hawkes processes have been proposed. However the inference of such models suffers from unscalability or slow convergence. In this paper, we first propose a new non-parametric Bayesian Hawkes process whose triggering kernel is modeled as a squared sparse Gaussian process. Second, we present the variational inference scheme for the model optimization, which has the advantage of linear time complexity by leveraging the stationarity of the triggering kernel. Third, we contribute a tighter lower bound than the evidence lower bound of the marginal likelihood for the model selection. Finally, we exploit synthetic data and large-scale social media data to validate the efficiency of our method and the practical utility of our approximate marginal likelihood. We show that our approach outperforms state-of-the-art non-parametric Bayesian and non-Bayesian methods.},
    address = {New York, New York, USA},
    annote = {(CoRE: A*, H5: , a.r.: )},
    archivePrefix = {arXiv},
    arxivId = {1905.10496},
    author = {Zhang, Rui and Walder, Christian and Rizoiu, Marian-Andrei},
    booktitle = {AAAI Conference on Artificial Intelligence (AAAI'20)},
    eprint = {1905.10496},
    URL_paper = {http://arxiv.org/pdf/1905.10496.pdf},
    month = {may},
    title = {{Variational Inference for Sparse Gaussian Process Modulated Hawkes Process}},
    url = {https://doi.org/10.1609/aaai.v34i04.6160},
    year = {2020}
}
@inproceedings{Zhang2020a,
    abstract = {Approximate inference techniques are a cornerstone of the study of Gaussian Processes. Despite this, most work approximately optimises divergence measures (Kullback-Leibler (KL), $\alpha$-divergence, etc.) which lack the basic desiderata for the task at hand, while chiefly offering merely technical convenience. We develop a new approximate inference method for the Gaussian process which overcomes the technical challenges of abandoning these convenient divergences. Our method — dubbed Quantile Propagation (QP) — is similar to expectation propagation (EP) but minimizes the L 2 Wasserstein distance (WD). The WD exhibits all the required properties of a distance metric, while respecting the geometry of the underlying sample space. We show that QP matches quantile functions rather than moments as in EP and has the same mean update but a smaller variance update than EP, thereby alleviating the over-estimation of the posterior variance exhibited by EP. Crucially, despite the significant complexity in even evaluating the WD, our QP has the same favorable locality property as EP, and thereby admits an efficient algorithm. Experiments on classification and Poisson regression tasks demonstrate that QP outperforms both EP and variational Bayes.},
    author = {Zhang, Rui and Walder, Christian and Bonilla, Edwin V. and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {Conference on Neural Information Processing Systems (NeurIPS'20)},
    title = {{Quantile Propagation for Wasserstein-Approximate Gaussian Processes}},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
    pages = {21566--21578},
    publisher = {Curran Associates, Inc.},
    volume = {33},
    year = {2020},
    url_paper = {https://papers.nips.cc/paper/2020/file/f5e62af885293cf4d511ceef31e61c80-Paper.pdf},
    url_code = {https://github.com/RuiZhang2016/Quantile-Propagation-for-Wasserstein-Approximate-Gaussian-Processes}
}
@inproceedings{Mihaita2020,
    abstract = {Traffic flow prediction, particularly in areas that experience highly dynamic flows such as motorways, is a major issue faced in traffic management. Due to increasingly large volumes of data sets being generated every minute, deep learning methods have been used extensively in the latest years for both short and long term prediction. However, such models, despite their efficiency, need large amounts of historical information to be provided, and they take a considerable amount of time and computing resources to train, validate and test. This paper presents two new spatial-temporal approaches for building accurate short-term prediction along a popular motorway in Sydney, by making use of the graph structure of the motorway network (including exits and entries). The methods are built on proximity-based approaches, denoted backtracking and interpolation, which uses the most recent and closest traffic flow information for each of the target counting stations along the motorway. The results indicate that for short-term predictions (less than 10 minutes into the future), the proposed graph-based approaches outperform state-of-the-art deep learning models, such as long-term short memory, convolutional neuronal networks or hybrid models.},
    address = {Rhodes, Greece},
    author = {Mihaita, Adriana-Simona and Papachatgis, Zac and Rizoiu, Marian-Andrei},
    booktitle = {23rd IEEE International Conference on Intelligent Transportation Systems (ITSC'20)},
    URL_paper = {https://arxiv.org/pdf/2006.14824},
    pages = {1--8},
    title = {{Graph modelling approaches for motorway traffic flow prediction}},
    year = {2020}
}

@inproceedings{Wu2020,
    abstract = {A comprehensive understanding of data bias is the cornerstone of mitigating biases in social media research. This paper presents in-depth measurements of the effects of Twitter data sampling across different timescales and different subjects (entities, networks, and cascades). By constructing two complete tweet streams, we show that Twitter rate limit message is an accurate measure for the volume of missing tweets. Despite sampling rates having clear temporal variations, we find that the Bernoulli process with a uniform rate well approximates Twitter data sampling, and it allows to estimate the ground-truth entity frequency and ranking with the observed sample data. In terms of network analysis, we observe significant structure changes in both the user-hashtag bipartite graph and the retweet network. Finally, we measure the retweet cascades. We identify risks for information diffusion models that rely on tweet inter-arrival times and user influence. This work calls attention to the social data bias caused by data collection, and proposes methods to measure the systematic biases introduced by sampling.},
    archivePrefix = {arXiv},
    arxivId = {2003.09557},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '20)},
    eprint = {2003.09557},
    month = {mar},
    pages = {1--10},
    title = {{Variation across Scales: Measurement Fidelity under Twitter Data Sampling}},
    URL_paper = {https://arxiv.org/pdf/2003.09557.pdf},
    URL_code = {https://github.com/avalanchesiqi/twitter-sampling},
    url = {https://ojs.aaai.org/index.php/ICWSM/article/view/7337},
    year = {2020}
}

@inproceedings{Dawson2020,
    abstract = {Skill shortages are a drain on society. They hamper economic opportunities for individuals, slow growth for firms, and impede labor productivity in aggregate. Therefore, the ability to understand and predict skill shortages in advance is critical for policy-makers and educators to help alleviate their adverse effects. This research implements a high-performing Machine Learning approach to predict occupational skill shortages. In addition, we demonstrate methods to analyze the underlying skill demands of occupations in shortage and the most important features for predicting skill shortages. For this work, we compile a unique dataset of both Labor Demand and Labor Supply occupational data in Australia from 2012 to 2018. This includes data from 7.7 million job advertisements (ads) and 20 official labor force measures. We use these data as explanatory variables and leverage the XGBoost classifier to predict yearly skills shortage classifications for 132 standardized occupations. The models we construct achieve macro-F1 average performance scores of up to 83 per cent. Our results show that job ads data and employment statistics were the highest performing feature sets for predicting year-to-year skills shortage changes for occupations. We also find that features such as `Hours Worked', years of `Education', years of `Experience', and median `Salary' are highly important features for predicting occupational skill shortages. This research provides a robust data-driven approach for predicting and analyzing skill shortages, which can assist policy-makers, educators, and businesses to prepare for the future of work.},
    archivePrefix = {arXiv},
    arxivId = {2004.01311},
    author = {Dawson, Nikolas and Rizoiu, Marian-Andrei and Johnston, Benjamin and Williams, Mary-Anne},
    booktitle = {Workshop on Human-in-the-Loop Methods and Future of Work in BigData (HMData'20)},
    eprint = {2004.01311},
    month = {dec},
    pages = {1--10},
    title = {{Predicting Skill Shortages in Labor Markets: A Machine Learning Approach}},
    URL_paper = {https://arxiv.org/pdf/2004.01311.pdf},
    year = {2020}
}

@inproceedings{Xu2022,
    abstract = {The rapid advances in automation technologies, such as artificial intelligence (AI) and robotics, pose an increasing risk of automation for occupations, with a likely significant impact on the labour market. Recent social-economic studies suggest that nearly 50% of occupations are at high risk of being automated in the next decade. However, the lack of granular data and empirically informed models have limited the accuracy of these studies and made it challenging to predict which jobs will be automated. In this paper, we study the automation risk of occupations by performing a classification task between automated and non-automated occupations. The available information is 910 occupations' task statements, skills and interactions categorised by Standard Occupational Classification (SOC). To fully utilize this information, we propose a graph-based semi-supervised classification method named Automated Occupation Classification based on Graph Convolutional Networks (AOC-GCN) to identify the automated risk for occupations. This model integrates a heterogeneous graph to capture occupations' local and global contexts. The results show that our proposed method outperforms the baseline models by considering the information of both internal features of occupations and their external interactions. This study could help policymakers identify potential automated occupations and support individuals' decision-making before entering the job market.},
    author = {Xu, Dawei and Yang, Haoran and Rizoiu, Marian-Andrei and Xu, Guandong},
    booktitle = {International Conference on Advanced Data Mining and Applications (ADMA 2022)},
    pages = {1--16},
    title = {{Being Automated or Not? Risk Identification of Occupations with Graph Neural Networks}},
    URL_paper = {https://arxiv.org/pdf/2209.02182.pdf},
    year = {2022}
}

