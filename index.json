[{"authors":["ma-rizoiu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4f906e7173ba13f37fe457c2f8f5aa2a","permalink":"https://www.behavioral-ds.science/authors/ma-rizoiu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ma-rizoiu/","section":"authors","summary":"","tags":null,"title":"Marian-Andrei Rizoiu","type":"authors"},{"authors":["quyu-kong"],"categories":null,"content":"","date":1573516800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1573516800,"objectID":"ad55124a082344b02dd6b633f167c02c","permalink":"https://www.behavioral-ds.science/authors/quyu-kong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/quyu-kong/","section":"authors","summary":"","tags":null,"title":"Quyu Kong","type":"authors"},{"authors":["rohit-ram"],"categories":null,"content":"","date":1615248000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1615248000,"objectID":"3bc23ee679b3318384aadb6ddcf712af","permalink":"https://www.behavioral-ds.science/authors/rohit-ram/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rohit-ram/","section":"authors","summary":"","tags":null,"title":"Rohit Ram","type":"authors"},{"authors":["alexander-soen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2f1bc0185b27a60a128583d3386e5d82","permalink":"https://www.behavioral-ds.science/authors/alexander-soen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alexander-soen/","section":"authors","summary":"","tags":null,"title":"Alexander Soen","type":"authors"},{"authors":["andrew-law"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d546df2262edacdbaab70b01ca4b420e","permalink":"https://www.behavioral-ds.science/authors/andrew-law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/andrew-law/","section":"authors","summary":"","tags":null,"title":"Andrew Law","type":"authors"},{"authors":["callum-pastuszak"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6577ba28ed581660e635939587c94dd4","permalink":"https://www.behavioral-ds.science/authors/callum-pastuszak/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/callum-pastuszak/","section":"authors","summary":"","tags":null,"title":"Callum Pastuszak","type":"authors"},{"authors":["dima-galat"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ec3e6f1b3b5f9cf5f63d7c94288bf326","permalink":"https://www.behavioral-ds.science/authors/dima-galat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dima-galat/","section":"authors","summary":"","tags":null,"title":"Dima Galat","type":"authors"},{"authors":["duy-khuu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"98e7c238ddbadc52ff4f0f2adb8689a0","permalink":"https://www.behavioral-ds.science/authors/duy-khuu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/duy-khuu/","section":"authors","summary":"","tags":null,"title":"Duy Khuu","type":"authors"},{"authors":["kriti-tripathi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"0e195dcb27b476a5295ec2a6c5b09223","permalink":"https://www.behavioral-ds.science/authors/kriti-tripathi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kriti-tripathi/","section":"authors","summary":"","tags":null,"title":"Kriti Tripathi","type":"authors"},{"authors":["lanqin-yuan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"64452371efd2e158a89061b2df61e343","permalink":"https://www.behavioral-ds.science/authors/lanqin-yuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lanqin-yuan/","section":"authors","summary":"","tags":null,"title":"Lanqin (Frankie) Yuan","type":"authors"},{"authors":["leanne-dong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f7ca5f02fcd2077eab139c9effda524d","permalink":"https://www.behavioral-ds.science/authors/leanne-dong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/leanne-dong/","section":"authors","summary":"","tags":null,"title":"Leanne Dong","type":"authors"},{"authors":["nik-dawson"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"336eca5812f78357754a398d8d048657","permalink":"https://www.behavioral-ds.science/authors/nik-dawson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nik-dawson/","section":"authors","summary":"","tags":null,"title":"Nik Dawson","type":"authors"},{"authors":["pio-calderon"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dbf59d75c981b3cf8c3d81d407382bf1","permalink":"https://www.behavioral-ds.science/authors/pio-calderon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pio-calderon/","section":"authors","summary":"","tags":null,"title":"Pio Calderon","type":"authors"},{"authors":["quyu-kong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"99578dd32d3d54feba0441767c95358d","permalink":"https://www.behavioral-ds.science/authors/rui-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rui-zhang/","section":"authors","summary":"","tags":null,"title":"Rui Zhang","type":"authors"},{"authors":["thomas-willingham"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"8b5bfcac50bc6a8e4f6b4f4b87d53e49","permalink":"https://www.behavioral-ds.science/authors/thomas-willingham/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/thomas-willingham/","section":"authors","summary":"","tags":null,"title":"Thomas Willingham","type":"authors"},{"authors":["yaozhong-liu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d4dd97b06e8eb332e7faa76e1de0d262","permalink":"https://www.behavioral-ds.science/authors/yaozhong-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yaozhong-liu/","section":"authors","summary":"","tags":null,"title":"Yaozhong Liu","type":"authors"},{"authors":["Marian-Andrei Rizoiu"],"categories":["Presentation"],"content":" Recording of the presentation \u0026ldquo;Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes\u0026rdquo; (Tim Graham, Marian-Andrei Rizoiu, Axel Bruns, Dan Angus), presented at the European Communication Conference (ECREA) 2021, 8 Sep. 2021.\nDiscovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes Tim Graham, Digital Media Research Centre, Queensland University of Technology\nMarian-Andrei Rizoiu, Data Science Institute, University of Technology Sydney\nAxel Bruns, Digital Media Research Centre, Queensland University of Technology\nDan Angus, Digital Media Research Centre, Queensland University of Technology\n‘Fake news’ and broader ‘information disorders’ [6] such as mis- and disinformation have emerged as global issues that threaten to undermine democracy and authentic political communication on social media [1]. Increasingly sophisticated coordination strategies have intensified the scale and scope of the impact that disinformation has on public opinion and democratic trust. Howard et al. [3] found that coordinated disinformation operations are now occurring in 48 countries, and in 2019 the European External Action Service detected and exposed over 1,000 cases of disinformation within the European Union [2]. Whilst disinformation has attracted much scholarly attention, most studies to date have focussed on the diffusion and impact of individual content (e.g. ‘fake news’ articles) and the activity of individual accounts (e.g. bots and trolls).\nAn emerging problem is to understand message coordination strategies, where content authored and distributed by agents (e.g. Twitter trolls) is governed and scheduled by some unknown principal actor [4]. We know that coordinated promotion (e.g. sharing, liking, retweeting) of ‘fake news’ articles by trolls and social bots can greatly increase and amplify the negative effects of these attempts to sow discord and manipulate public conversations about election candidates and partisan issues such as immigration and climate change. Likewise, it is evident that disinformation campaigns unfold via ‘collaborative work’ that co-opts and cultivates organic systems in order to produce desired effects such as increased polarisation, distrust in news media and confusion of the audience [7]. This makes identifying ‘inauthentic’ versus ‘organic’ activity ever more difficult, as they are intricately enmeshed in real-world disinformation campaigns.\nIn this paper, we tackle the problem of inferring the coordinated promotion schedules of ‘fake news’ articles using a novel approach known as Hawkes Intensity Processes (HIP; see [5]). We analyse the diffusion of articles from ten major sources of hyperpartisan information and ‘fake news’ within over 16.5 million tweets that linked to content from these sites during July to September 2019. Using HIP, we uncover not only coordination strategies but also the promotion schedules of ‘fake news’ content, where agents (in this case Twitter accounts) are being centrally managed by principals (e.g. state operatives, government officials, etc.) in order to strategically promote ‘fake news’ content and maximise its virality and longevity in the social memory. This paper provides preliminary results from this ongoing research, highlighting the current challenges as well as open problems and gaps for future work.\nReferences [1] Benkler, Y., Faris, R., \u0026amp; Roberts, H. (2018). Network propaganda: Manipulation, disinformation, and radicalization in American politics. Oxford University Press.\n[2] European Commission. (2019). Action plan against disinformation: Report in progress. Retrieved 20 November 2019 from: https://ec.europa.eu/commission/sites/beta-political/files/factsheet_disinfo_elex_140619_final.pdf.\n[3] Howard, P. N., \u0026amp; Kollanyi, B. (2016). Bots, #StrongerIn, and #Brexit: Computational Propaganda during the UK-EU Referendum. SSRN Electronic Journal. doi:10.2139/ssrn.2798311\n[4] Keller, F. B., Schoch, D., Stier, S., \u0026amp; Yang, J. (2019). Political Astroturfing on Twitter: How to Coordinate a Disinformation Campaign. Political Communication, 1-25.\n[5] Rizoiu, M. A., Xie, L., Sanner, S., Cebrian, M., Yu, H., \u0026amp; Van Hentenryck, P. (2017, April). Expecting to be hip: Hawkes intensity processes for social media popularity. In Proceedings of the 26th International Conference on World Wide Web (pp. 735-744). International World Wide Web Conferences Steering Committee.\n[6] Wardle, C., \u0026amp; Derakhshan, H. (2017). Information disorder: Toward an interdisciplinary framework for research and policymaking. Council of Europe Report DGI (2017) 09.\n[7] Wilson, T., Zhou, K., \u0026amp; Starbird, K. (2018). Assembling Strategic Narratives: Information Operations as Collaborative Work within an Online Community. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 183.\nThe panel: \u0026ldquo;Coordinated Inauthentic Behaviour in Social Media: New Methods and Findings\u0026rdquo; Panel Rationale Social media platforms are increasingly forced to address what Facebook now describes as ‘coordinated inauthentic behaviour’ (Gleicher 2018): online influence operations that seek to trick platform algorithms into promoting and recommending ‘problematic information’ (Jack 2017), to mislead the human users of such platforms into accepting and sharing such content, and thereby also to affect broader issue frames and news agendas in mainstream media coverage. Concerns about such coordinated inauthentic behaviour extend earlier fears about the influence of malignant social bots, but also transcend them: drawing on social bots as well as human labour, coordinated inauthentic behaviour is likely to involve a combination of manual and automated activity. This additional human factor also complicates the detection of such coordinated activities, and their distinction from genuine, organic, authentic coordinated actions.\nThis cross-national and interdisciplinary panel approaches the study of coordinated inauthentic behaviour from a number of directions. It outlines novel and innovative detection and analysis approaches for a number of leading social media platforms, and presents their results in the context of domestic and international political debates across several national contexts. Further, it also considers how mainstream journalism might report on and respond to such activities in order to protect news audiences from being affected by coordinated inauthentic behaviours.\nThe first two papers in this panel focus especially on coordinated inauthentic link-sharing practices. Paper 1 introduces Hawkes Intensity Processes (HIP), a novel technique for inferring the coordinated content promotion schedules of automated social media accounts, and applies this to a major dataset of 16.5 million tweets containing links to ten major sites identified as sources of hyperpartisan content and ‘fake news’. In doing so, it uncovers new networks of inauthentic Twitter actors. Paper 2 investigates similar coordinated link-sharing activity on Facebook in Italy during the 2018 Italian and 2019 European elections. It uncovers evidence for the involvement of dozens of pages, groups, and public profiles in such media manipulation attempts. Paper 3 complements this work by focussing especially on the temporal posting patterns in such coordinated activity. It employs the recurrence plotting technique to identify traces of inauthentic actors’ use of automated scheduling tools in systematically posting content to a network of apparently unrelated pages, focussing here especially on a group of far-right pages on Facebook. Paper 4 examines ten coordinated disinformation campaigns across the globe (e.g., Hong Kong, Russia, USA, Spain and Germany) and identifies important traits that help distinguish between those participating in the disinformation campaign and the regular users they try to imitate. Paper 5, finally, shifts our attention to a core target of such coordinated inauthentic behaviour: the journalists and editors whose perception of current political moods such influence operations often aim to affect. Drawing on a series of in-depth interviews with Danish news workers and related stakeholders, it examines their understanding of and responses to coordinated mis- and disinformation campaigns.\nCollectively, these studies contribute substantially to advancing the methodological toolkit and extending the empirical evidence base for the study of coordinated inauthentic behaviour, while also not losing sight of the stakeholders that such work seeks to support. They offer an independent assessment of the nature and extent of the problem across several leading social media platforms, complementing the platform providers’ own investigations into such activities and identifying possible responses to such concerns for both social and mainstream media actors.\nThe papers presented are (presenters bolded):  Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes (Tim Graham, Marian-Andrei Rizoiu, Axel Bruns, Dan Angus) It Takes a Village to Manipulate the Media: Coordinated Link Sharing Behaviour during 2018 and 2019 Italian Elections (Fabio Giglietto, Nicola Righetti, Luca Rossi, Giada Marino) Recurrence Plotting for Detecting Duplicate Online Posting Activities (Dan Angus, Tim Graham, Tobias Keller, Brenda Moon, Axel Bruns) Astroturfing in Hong Kong and Elsewhere: Patterns of Coordination in Hidden Twitter Campaigns (Franziska B. Keller, Sebastian Stier, David Schoch, JungHwan Yang)  ","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"ecad0b03705e94049ba7ef40ca802fb4","permalink":"https://www.behavioral-ds.science/blogpost/coordinated_disinfo_hawkes/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/blogpost/coordinated_disinfo_hawkes/","section":"blogpost","summary":"ECREA 2021 presentation: \"Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes\" ","tags":null,"title":"Discovering coordinated disinformation via Hawkes processes","type":"blogpost"},{"authors":["Marian-Andrei Rizoiu"],"categories":["Presentation"],"content":" Recording of the presentation \u0026ldquo;Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes\u0026rdquo; (Tim Graham, Marian-Andrei Rizoiu, Axel Bruns, Dan Angus), presented at the European Communication Conference (ECREA) 2021, 8 Sep. 2021.\nDiscovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes Tim Graham, Digital Media Research Centre, Queensland University of Technology\nMarian-Andrei Rizoiu, Data Science Institute, University of Technology Sydney\nAxel Bruns, Digital Media Research Centre, Queensland University of Technology\nDan Angus, Digital Media Research Centre, Queensland University of Technology\n‘Fake news’ and broader ‘information disorders’ [6] such as mis- and disinformation have emerged as global issues that threaten to undermine democracy and authentic political communication on social media [1]. Increasingly sophisticated coordination strategies have intensified the scale and scope of the impact that disinformation has on public opinion and democratic trust. Howard et al. [3] found that coordinated disinformation operations are now occurring in 48 countries, and in 2019 the European External Action Service detected and exposed over 1,000 cases of disinformation within the European Union [2]. Whilst disinformation has attracted much scholarly attention, most studies to date have focussed on the diffusion and impact of individual content (e.g. ‘fake news’ articles) and the activity of individual accounts (e.g. bots and trolls).\nAn emerging problem is to understand message coordination strategies, where content authored and distributed by agents (e.g. Twitter trolls) is governed and scheduled by some unknown principal actor [4]. We know that coordinated promotion (e.g. sharing, liking, retweeting) of ‘fake news’ articles by trolls and social bots can greatly increase and amplify the negative effects of these attempts to sow discord and manipulate public conversations about election candidates and partisan issues such as immigration and climate change. Likewise, it is evident that disinformation campaigns unfold via ‘collaborative work’ that co-opts and cultivates organic systems in order to produce desired effects such as increased polarisation, distrust in news media and confusion of the audience [7]. This makes identifying ‘inauthentic’ versus ‘organic’ activity ever more difficult, as they are intricately enmeshed in real-world disinformation campaigns.\nIn this paper, we tackle the problem of inferring the coordinated promotion schedules of ‘fake news’ articles using a novel approach known as Hawkes Intensity Processes (HIP; see [5]). We analyse the diffusion of articles from ten major sources of hyperpartisan information and ‘fake news’ within over 16.5 million tweets that linked to content from these sites during July to September 2019. Using HIP, we uncover not only coordination strategies but also the promotion schedules of ‘fake news’ content, where agents (in this case Twitter accounts) are being centrally managed by principals (e.g. state operatives, government officials, etc.) in order to strategically promote ‘fake news’ content and maximise its virality and longevity in the social memory. This paper provides preliminary results from this ongoing research, highlighting the current challenges as well as open problems and gaps for future work.\nReferences [1] Benkler, Y., Faris, R., \u0026amp; Roberts, H. (2018). Network propaganda: Manipulation, disinformation, and radicalization in American politics. Oxford University Press.\n[2] European Commission. (2019). Action plan against disinformation: Report in progress. Retrieved 20 November 2019 from: https://ec.europa.eu/commission/sites/beta-political/files/factsheet_disinfo_elex_140619_final.pdf.\n[3] Howard, P. N., \u0026amp; Kollanyi, B. (2016). Bots, #StrongerIn, and #Brexit: Computational Propaganda during the UK-EU Referendum. SSRN Electronic Journal. doi:10.2139/ssrn.2798311\n[4] Keller, F. B., Schoch, D., Stier, S., \u0026amp; Yang, J. (2019). Political Astroturfing on Twitter: How to Coordinate a Disinformation Campaign. Political Communication, 1-25.\n[5] Rizoiu, M. A., Xie, L., Sanner, S., Cebrian, M., Yu, H., \u0026amp; Van Hentenryck, P. (2017, April). Expecting to be hip: Hawkes intensity processes for social media popularity. In Proceedings of the 26th International Conference on World Wide Web (pp. 735-744). International World Wide Web Conferences Steering Committee.\n[6] Wardle, C., \u0026amp; Derakhshan, H. (2017). Information disorder: Toward an interdisciplinary framework for research and policymaking. Council of Europe Report DGI (2017) 09.\n[7] Wilson, T., Zhou, K., \u0026amp; Starbird, K. (2018). Assembling Strategic Narratives: Information Operations as Collaborative Work within an Online Community. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 183.\nThe panel: \u0026ldquo;Coordinated Inauthentic Behaviour in Social Media: New Methods and Findings\u0026rdquo; Panel Rationale Social media platforms are increasingly forced to address what Facebook now describes as ‘coordinated inauthentic behaviour’ (Gleicher 2018): online influence operations that seek to trick platform algorithms into promoting and recommending ‘problematic information’ (Jack 2017), to mislead the human users of such platforms into accepting and sharing such content, and thereby also to affect broader issue frames and news agendas in mainstream media coverage. Concerns about such coordinated inauthentic behaviour extend earlier fears about the influence of malignant social bots, but also transcend them: drawing on social bots as well as human labour, coordinated inauthentic behaviour is likely to involve a combination of manual and automated activity. This additional human factor also complicates the detection of such coordinated activities, and their distinction from genuine, organic, authentic coordinated actions.\nThis cross-national and interdisciplinary panel approaches the study of coordinated inauthentic behaviour from a number of directions. It outlines novel and innovative detection and analysis approaches for a number of leading social media platforms, and presents their results in the context of domestic and international political debates across several national contexts. Further, it also considers how mainstream journalism might report on and respond to such activities in order to protect news audiences from being affected by coordinated inauthentic behaviours.\nThe first two papers in this panel focus especially on coordinated inauthentic link-sharing practices. Paper 1 introduces Hawkes Intensity Processes (HIP), a novel technique for inferring the coordinated content promotion schedules of automated social media accounts, and applies this to a major dataset of 16.5 million tweets containing links to ten major sites identified as sources of hyperpartisan content and ‘fake news’. In doing so, it uncovers new networks of inauthentic Twitter actors. Paper 2 investigates similar coordinated link-sharing activity on Facebook in Italy during the 2018 Italian and 2019 European elections. It uncovers evidence for the involvement of dozens of pages, groups, and public profiles in such media manipulation attempts. Paper 3 complements this work by focussing especially on the temporal posting patterns in such coordinated activity. It employs the recurrence plotting technique to identify traces of inauthentic actors’ use of automated scheduling tools in systematically posting content to a network of apparently unrelated pages, focussing here especially on a group of far-right pages on Facebook. Paper 4 examines ten coordinated disinformation campaigns across the globe (e.g., Hong Kong, Russia, USA, Spain and Germany) and identifies important traits that help distinguish between those participating in the disinformation campaign and the regular users they try to imitate. Paper 5, finally, shifts our attention to a core target of such coordinated inauthentic behaviour: the journalists and editors whose perception of current political moods such influence operations often aim to affect. Drawing on a series of in-depth interviews with Danish news workers and related stakeholders, it examines their understanding of and responses to coordinated mis- and disinformation campaigns.\nCollectively, these studies contribute substantially to advancing the methodological toolkit and extending the empirical evidence base for the study of coordinated inauthentic behaviour, while also not losing sight of the stakeholders that such work seeks to support. They offer an independent assessment of the nature and extent of the problem across several leading social media platforms, complementing the platform providers’ own investigations into such activities and identifying possible responses to such concerns for both social and mainstream media actors.\nThe papers presented are (presenters bolded):  Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes (Tim Graham, Marian-Andrei Rizoiu, Axel Bruns, Dan Angus) It Takes a Village to Manipulate the Media: Coordinated Link Sharing Behaviour during 2018 and 2019 Italian Elections (Fabio Giglietto, Nicola Righetti, Luca Rossi, Giada Marino) Recurrence Plotting for Detecting Duplicate Online Posting Activities (Dan Angus, Tim Graham, Tobias Keller, Brenda Moon, Axel Bruns) Astroturfing in Hong Kong and Elsewhere: Patterns of Coordination in Hidden Twitter Campaigns (Franziska B. Keller, Sebastian Stier, David Schoch, JungHwan Yang)  ","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"142936ae2a68743e779b9db702fe09af","permalink":"https://www.behavioral-ds.science/theme2_content/coordinated_disinfo_hawkes/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/theme2_content/coordinated_disinfo_hawkes/","section":"theme2_content","summary":"ECREA 2021 presentation: \"Discovering the Strategies and Promotion Schedules of Coordinated Disinformation via Hawkes Intensity Processes\" ","tags":null,"title":"Discovering coordinated disinformation via Hawkes processes","type":"theme2_content"},{"authors":null,"categories":null,"content":"Our group has access to considerable computation and storage resources. This document describes these and how to access them. Each resource has its own use scenario, and some may be more adapted to specific problems than others (for example, if you want to set up a Twitter crawler, use a Nectar Virtual Machine for better uptime and to avoid security issues on the compute servers). We have four compute resources and each has attached a storage resource.\nCompute servers and frameworks iHPC servers Link to resource: ihpc.research.uts.edu.au (requires UTS VPN)\nQuick description: iHPC stands for Interactive High-Performance Computing. In a nutshell, these are very large machines (around 60 cores, and terabytes of memory). The machines are hosted in UTS and require both a UTS affiliation and special access. The servers are grouped into clusters, some of which require special access.\nOur group has a private cluster called hephaestos which currently contains two machines (see print screen below). Our private cluster is accessible to BDS members only.\nGetting access: Request an iHPC account (see ihpc.research.uts.edu.au for instructions). Mention the Behavioral Data Science group and Andrei in your request to be automatically added to the hephaestos access lists. Make sure you read the documentation!\nUse case: the iHPC is designed for prototyping that requires large machines (Jupyter Notebook or RStudio), or for intermediary-size compute (for larger size see UTS HPC and NCI). You can use iHPC using its Graphical User Interface (GUI) via NoMachine. You can also connect via console using:\nssh \u0026lt;user\u0026gt;@hephaestos1.eng.uts.edu.au\nFor use with Jupyter Notebook or RStudio Server, you can redirect ports via SSH tunneling. Assuming the Jupyter Notebook/RStudio Server listens on port 8282, you can redirect that port by connecting to hephaestos1 using:\nssh -L 8282:localhost:8282 \u0026lt;user\u0026gt;@hephaestos1.eng.uts.edu.au\nafter which you can connect the browser to your local machine to https://localhost:8282 and access the kernel on the distant machines. See here for more details and examples. \nNeCTAR Virtual Machines Link to resource: https://dashboard.rc.nectar.org.au/\nQuick description: NeCTAR is an infrastructure for virtual machines (i.e. machines that can be spawned for a particular purpose), similar to Amazon Web Services. This is quite useful for crawling data with no downtime, hosting visualizers, and even computing. More details here: https://ardc.edu.au/services/nectar-research-cloud/\nOur group has an allocation of 25 instances, 120 vCPUs, and 480 GB of RAM.\nGetting access: First activate your UTS/ANU/Data61 account here: https://dashboard.rc.nectar.org.au/. Then flip Andrei the email address associated with the account, and I'll add you to the project. Note: once you have access, you need to switch projects using the project selector in the top-left corner to access the behavioral-ds allocation.\nUse case: You can easily spawn virtual machines, install stuff on them, play and destroy them. Each virtual machine acts as a computer, therefore you can install web interfaces, use it for compute (the larger flavors), and connect via SSH. The port forwarding trick described for iHPC also works for NeCTAR VMs. \nUTS High-Performance Computing Cluster (HPCC) Link to resource: https://hpc.research.uts.edu.au/\nUsage status: https://hpc.research.uts.edu.au/status/\nQuick description: HPCC is a typical high-performance computing cluster (a.k.a. super-computers), in which jobs run non-interactively. See here a schema of HPC clusters. UTS HPCC uses PBS as the scheduler, and it is designed to be fully compatible with NCI. Its mission is to be a training or development site for larger HPC projects destined for NCI.\nGetting access: You need to request access to the HPCC. Email eResearch-IT@uts.edu.au to introduce yourself and your requirements. Make sure you mention Andrei and that you are part of the Behavioral DS group. Once you have access read the HPC Getting Started pages.\nUse case: As HPC systems stand, UTS HPCC is a small system, but very useful for mid- to largish-scale computations. You can access 600 cores and up to 6TB of memory. Usage-wise, UTS HPCC stands in between iHPC and the NCI. That is, once your compute grows too large for iHPC, you use HPCC to scale it up and prepare your scripts for NCI. \nNational Computation Infrastructure (NCI) supercomputer \u0026ndash; Gadi Link to resource: https://nci.org.au/\nUsage status: https://nci.org.au/our-systems/status\nQuick description: NCI’s Gadi is Australia’s supercomputer. Gadi has 3,200 compute nodes, 155,000 CPU cores, 567 Terabytes of memory, and 640 GPUs (as of 07/09/2021). It has a peak performance of 9 petaflops (by comparison, the largest supercomputer, Fugaku in Japan, has a peak performance of 442 petaflops).\nGadi is a cross-institutional shared resource, and its usage requires credit (in NCI terms service units (SU)). Our group has a varying allocation of SUs to be used for computing, which can be dynamically increased. Speak to Andrei if you believe you may require more than we have or even if you may use up all the exiting allocation (not leaving enough for the others).\nGetting access: You will need to request an NCI account using your UTS/ANU/Data61 email address. Use our project code gh47: Tracking disinformation campaigns across social media (Marian-Andrei Rizoiu) during the joining process. If you already have an account, simply request to join the project. You will subsequently use this project code for all computing. Make sure you have a read of the Gadi User guide.\n**Use case: Use Gadi for the largest scale computes, which require years of sequential compute time. Ideally, you designed already your scripts using the UTS HPCC. Note that Gadi is less responsive than HPCC (and less forgiving with not respecting the limits and instructions).\nThere are several queues available for Gadi, depending on your usage requirements. These include normal (day-to-day compute), express (small batches that can be executed fast), hugemem and megamem (large memory requirement), and gpuvolta (for deep learning). \nStorage iHPC data folder (1TB) iHPC home folders are limited to 32 GB. However, each iHPC user has a data folder of 1TB accessible at /data/\u0026lt;username\u0026gt;. Put here all large files, including your anaconda and datasets. This data folder is on Network File System (NFS) and accessible from any iHPC machine.\niHPC project mount (20TB) For shared files and datasets, our group has a project allocation of 20TB, accessible at /projects/BehavioralDS/. All files in this folder are accessible by all group members.\nNectar storage (12TB) NeCTAR VMs have very little storage space (~30GB of disk). You can also create external volumes that can be attached to the VMs (each 50GB). For larger storage, our group has a 12TB NFS volume that needs to be attached using the following procedure.\n Send the IP address or hostname of the machine that you want to mount the Space storage to Intersect Operations (help@intersect.org.au). Copy Andrei to the email!\n  Install NFS and automount related packages (for Ubuntu):\n  apt-get install nfs-common autofs rpcbind\nsystemctl enable rpcbind\nsystemctl start rpcbind\nsystemctl enable autofs\nsystemctl start autofs\nCreate Directories  mkdir -p /data/mounts\nCreate /etc/auto.data  echo \u0026quot;Q3530 -fstype=nfs,nfsvers=3 10.255.122.28:/gpfs/general00/pool9000/Q3530/Q3530\u0026quot; \u0026gt;\u0026gt; /etc/auto.data\nModify /etc/auto.master  Run the following command to insert mount point to /etc/auto.master file\necho \u0026quot;/data/mounts /etc/auto.data\u0026quot; \u0026gt;\u0026gt; /etc/auto.master\nRestart autofs and create symlink  Restart autofs service to apply the above changes\nsystemctl restart autofs\nln -s /data/mounts/Q3530 /data/Q3530 \nNCI storage (4TB) Our group has a storage allocation of 4TB on NCI. It can be accessed at /g/data/gh47/. If you want to use this allocation in your PBS jobs, you need to explicitly ask for it in the submission script. For example, to indicate that credit should be used from gh47 and to access both the scratch and the /g/data allocation, one would add the following lines in the submission script:\n#PBS -P gh47\n#PBS -l storage=scratch/gh47+g/data/gh47\n","date":1632441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632441600,"objectID":"8e8ab4769ab938b9c2d74e60b2dee5af","permalink":"https://www.behavioral-ds.science/extra/compute_resources/","publishdate":"2021-09-24T00:00:00Z","relpermalink":"/extra/compute_resources/","section":"extra","summary":"Our group has access to considerable computation and storage resources. This document describes these and how to access them. Each resource has its own use scenario, and some may be more adapted to specific problems than others (for example, if you want to set up a Twitter crawler, use a Nectar Virtual Machine for better uptime and to avoid security issues on the compute servers). We have four compute resources and each has attached a storage resource.","tags":null,"title":"Compute and storage resources","type":"extra"},{"authors":null,"categories":null,"content":"(this document is shamelessly adapted from Robert West’s excellent dlab onboarding page)\nIf you’re new to the Behavioral Data Science lab (BDS), first of all: welcome! This page will help you get started by providing useful information on how to get set up and ready to roll. If you’re new not only to BDS but to UTS as a whole, see the bottom of this page for some additional useful pointers. \nCompute resources As a very first step, carefully read this introduction to our computing and storage resources: iHPC, UTS HPCC, Nectar, and NCI. It will tell you everything that’s necessary to get cracking and crunching with that exciting data! \n\nCommunication Group’s webpage Our group’s webpage is https://www.behavioral-ds.science/. It lists the members, our news, our publications, and blog entries. We also use it to keep track of our reading group schedule. Our website is collaborative, and every member needs to update it (for example to add their personal profile, or to fill in the reading details). The webpage is hosted on Github pages, and we update it by committing to the repository.\nTo add your profile to the lab's people page, you need to fork the lab's repository on GitHub, edit it is to add your profile, and create a pull request. More instructions are in the repository’s README. \nSlack Join the BDS team on Slack. We use Slack for all communications. The group’s philosophy is that we all are aware of the work of others, and we pitch in when we feel like it. The workspace is organized in public channels, one for each project.\nEach BDS member is part of the channels they work on (usually one project). All research-oriented discussions go in the project channel (please keep the DM for only admin and personal issues). Feel free to roam around and peek at any public channel.\nThere are several public channels that you might want to peak at:\n #general \u0026ndash; general announcements, usually admin- and social group-related #o-data \u0026ndash; datasets #compute-power \u0026ndash; computation resources and issues #reading \u0026ndash; what to read, and announcements about the reading group #random \u0026ndash; non-work banter and water cooler conversation   Meetings Here is some good advice on how to make meetings with your advisor as effective and successful as possible. Please read it.\nWhere possible, in-person (offline) meetings are preferred. For online and mixed-mode meetings we use Zoom. When online, we will use Zoom’s Shared Whiteboard to take collective notes. If you have a Zoom-compatible tablet with a pen, it is recommended to install Zoom as drawing with a pen is considerably easier than with a mouse. Alternative environments (like Miro or Google Jamboard) can also be used. \nReading group We have a reading group that runs for one hour each week. The meeting runs mixed-mode, on Zoom and in-person in Sydney. During the meeting, we read papers and run group-oriented activities (such as three-minute thesis presentations and conference dry-runs). The meeting is run each week by a different member (see the online roster). When it is your turn, you can populate the meeting details as described above (see group’s webpage). \nDocumenting your research Keeping notes It is important that you keep thorough notes of your research. This will make it easier to remember the things you’ve done, the things you still need to do, important references you’ve come across, etc. It will also let you share your thoughts with the rest of the group, and it will make you a better, more disciplined researcher.\nWe recommend two ways of keeping notes, you will need to choose one. The first is a rolling dated Overleaf document, where each week is a section and under which you note your work and progress, including figures and interpretations. Keep the most recent on top (i.e. you add at the beginning of the document, not at the end). The second way is the app Notion, which has free academic plans with your UTS student number email address. Create a master page that you will share with Andrei. On the Master page link your reading list, meeting notes etc. (see an example print-screen). Note that we suggest using a dedicated tool for managing your reading and related work (such as Mendeley or Zotero).\nWriting papers You can find some tips and tools for writing papers in LaTeX on dlab’s tips for writing. \nBlog posts Blog posts are a great way of publicizing your research results beyond good old (and slooow…) papers. They are also a channel for making what you’re doing accessible to the general public, rather than just to other academics.\nTherefore, every paper published at BDS is expected to be accompanied by a blog post that is written in non-technical terms and easily accessible to non-experts, to be published on BDS’s research page. You will need to select a category or shout if none is adequate. \nNew to UTS? The following pointers might help you get started if you’ve just arrived at UTS. \nWifi On campus, the best wireless network to use is UTS-Wifi. You can use it with your UTS student username and password. For Linux, check the below screen for configuration.\nVPN To access UTS resources, such as the iHPC compute servers, from outside of the UTS network, you need to use the UTS VPN. \nUTS student/staff card During your first days at UTS, you’ll get your UTS student card, which will serve to access the library and the UTS buildings. Once you have the card, make sure you let the Data Science admins know so that you get access to DSI HDR dedicated spaces (room CB02.11.141). \nUTS Honorary appointments Group members who are not UTS staff nor students need to obtain honorary UTS appointments. An honorary staff has a UTS online account and can access internal systems, including the compute resources (note: not all compute resources require a UTS affiliation, check details above).\nTo get an Honorary appointment, Andrei needs to initiate the process with UTS admins, after which you will receive email instructions by email. Please make sure you fill in the online form ASAP, as the downstream admin process can take weeks. UTS Honorary members are considered staff in UTS systems.\n","date":1632441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632441600,"objectID":"61cfbf47d93ad57ea59cbbf75b9650c4","permalink":"https://www.behavioral-ds.science/extra/onboarding/","publishdate":"2021-09-24T00:00:00Z","relpermalink":"/extra/onboarding/","section":"extra","summary":"(this document is shamelessly adapted from Robert West’s excellent dlab onboarding page)\nIf you’re new to the Behavioral Data Science lab (BDS), first of all: welcome! This page will help you get started by providing useful information on how to get set up and ready to roll. If you’re new not only to BDS but to UTS as a whole, see the bottom of this page for some additional useful pointers.","tags":null,"title":"Onboarding","type":"extra"},{"authors":null,"categories":null,"content":"Summary  Rare PhD opportunity for research on the Future of Work and Labour Markets applying Data Science and Machine Learning techniques Mentored by leading researchers and industrials in this space Access to datasets, strong industry partnerships, and international collaborations with top universities   The Research Topic Advances in automation technologies and labour market shocks, such as COVID-19, have elevated the importance of labour mobility issues. Therefore, identifying viable and desirable job transition pathways for individuals has been a growing area of interest. The current state of the art methods [Moro, Frank et al, Nat.Comm.’21] [Dawson et al, PLOS ONE ‘21] take a global view to estimate the dynamics of labour markets using factors such as skill similarity, location, education, experience, and industry labour flows. While these have been shown as important features for explaining job transition pathways, they implicitly assume that the changes in the occupation of an individual depend solely on their current occupation. Other works [Kern et al, PNAS’19] have shown that worker psychometric profiles (measured using Big5 feature) closely relate to their occupations. In other words, people have predispositions towards certain occupations. This project aims to build personalised recommender systems for occupation transitions that account for worker personality, previous experience and education. \nThe Opportunity The Behavioral Data Science lab at UTS has several openings for PhD students to research the Future of Work and labour market networks. The topics of interest are job transitions, skills analysis, and the impacts of technology on labour markets. However, we’re open to other related areas, such as impacts of personality profiles on job choice, quantifying cultural differences between labour markets, and competitive dynamics of occupations and industries. The research will apply data science and machine learning techniques to labour market data, such as job ads and employment statistics. \nWhat We Offer This project is part of a wider collaboration with US-based universities and companies. This can open unique opportunities to research internships in both overseas academia and industry. Furthermore, you will be supported throughout your doctorate to become a first rate researcher. This will include:\n access to rich datasets to derive insights and real-world problems and know-how; exposure to industry networks to apply your work, and possible career paths upon graduation; collaboration with top international universities from our networks; and a supportive peer-group of doctoral researchers with regular events and reading groups. weekly meetings with supervisors to advance your research;  Once you graduate, you will have developed in-demand skills, published peer-review papers, broadened your professional network, and established yourself as an expert in the Future of Work. \nAbout You To be successful in this role, you will either have:\n a strong background in data science and applied machine learning; an interest labour markets; knowledge of at least one programming language, such as Python and R;  AND/OR\n A strong background in economics and are willing to develop your technical skills;  You will also likely have an Honours or Masters degree. However, equivalent experience coupled with a relevant Bachelor’s Degree could also be sufficient.\nMost importantly, you will show a great deal of initiative, thrive under autonomy, and have a passion for applying rigorous research to help people understand real problems. \nFunding Scholarships are available but will be awarded through a competitive process. Stipend top-ups and paid research work might be available throughout your thesis. \nHow to Apply If you are interested, please send your CV and a cover letter to Marian-Andrei.Rizoiu@uts.edu.au. The cover letter should detail your academic track record, why you are interested in this topic and why you are a good match for the subject. \nReferences [Moro et al, Nat.Comm.’21] Moro, E., Frank, M. R., Pentland, A., Rutherford, A., Cebrian, M., \u0026amp; Rahwan, I. (2021). Universal resilience patterns in labor markets. Nature Communications, 12(1), 1972. https://doi.org/10.1038/s41467-021-22086-3  [Dawson et al, PLOS ONE ‘21] Dawson, N., Williams, M.-A., \u0026amp; Rizoiu, M.-A. (2021). Skill-driven Recommendations for Job Transition Pathways. PLOS ONE. Retrieved from http://arxiv.org/abs/2011.11801  [Kern et al, PNAS’19] Kern, M. L., McCarthy, P. X., Chakrabarty, D., \u0026amp; Rizoiu, M.-A. (2019). Social media-predicted personality traits and values can help match people to their ideal jobs. Proceedings of the National Academy of Sciences, 116(52), 26459–26464. https://doi.org/10.1073/pnas.1917942116\n","date":1627948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627948800,"objectID":"2f54d159cf3edd4aca2788a09e92c871","permalink":"https://www.behavioral-ds.science/extra/phdopportunity/","publishdate":"2021-08-03T00:00:00Z","relpermalink":"/extra/phdopportunity/","section":"extra","summary":"Summary  Rare PhD opportunity for research on the Future of Work and Labour Markets applying Data Science and Machine Learning techniques Mentored by leading researchers and industrials in this space Access to datasets, strong industry partnerships, and international collaborations with top universities   The Research Topic Advances in automation technologies and labour market shocks, such as COVID-19, have elevated the importance of labour mobility issues. Therefore, identifying viable and desirable job transition pathways for individuals has been a growing area of interest.","tags":null,"title":"PhD Opportunity","type":"extra"},{"authors":["Paul X. McCarthy, Marian-Andrei Rizoiu"],"categories":["Research"],"content":"The online world is continuously expanding — always aggregating more services, more users and more activity. Last year, the number of websites registered on the \u0026ldquo;.com\u0026rdquo; domain surpassed 150,000,000.\nHowever, more than a quarter of a century since its first commercial use, the growth of the online world is now slowing down in some key categories.\nWe conducted a multi-year research project analysing global trends in online diversity and dominance. Our research, published in Public Library of Science, is the first to reveal some long-term trends in how businesses compete in the age of the web.\nWe saw a dramatic consolidation of attention towards a shrinking (but increasingly dominant) group of online organisations. So, while there is still growth in the functions, features and applications offered on the web, the number of entities providing these functions is shrinking.\nWeb diversity nosedives We analysed more than six billion user comments from the social media website Reddit dating back to 2006, as well as 11.8 billion Twitter posts from as far back as 2011. In total, our research used a massive 5.6Tb trove of data from more than a decade of global activity.\nThis dataset was more than four times the size of the original data from the Hubble Space Telescope, which helped Brian Schmidt and colleagues do their Nobel-prize winning work in 1998 to prove the universe's expansion is accelerating.\nWith the Reddit posts, we analysed all the links to other sites and online services — more than one billion in total — to understand the dynamics of link growth, dominance and diversity through the decade.\nWe used a measure of link \u0026ldquo;uniqueness\u0026rdquo;. On this scale, 1 represents maximum diversity (all links have their own domain) and 0 is minimum diversity (all links are on one domain, such as \u0026ldquo;youtube.com\u0026rdquo;).\nA decade ago, there was a much greater variety of domains within links posted by users of Reddit, with more than 20 different domains for every 100 random links users posted. Now there are only about five different domains for every 100 links posted.\nIn fact, between 60-70% of all attention on key social media platforms is focused towards just ten popular domains.\nBeyond social media platforms, we also studied linkage patterns across the web, looking at almost 20 billion links over three years. These results reinforced the \u0026ldquo;rich are getting richer\u0026rdquo; online.\nThe authority, influence and visibility of the top 1,000 global websites (as measured by network centrality or PageRank) is growing every month, at the expense of all other sites.\nApp diversity is on the rise The web started as a source of innovation, new ideas and inspiration — a technology that opened up the playing field. It's now also becoming a medium that actually stifles competition and promotes monopolies and the dominance of a few players.\nOur findings resolve a long-running paradox about the nature of the web: does it help grow businesses, jobs and investment? Or does it make it harder to get ahead by letting anyone and everyone join the game? The answer, it turns out, is it does both.\nWhile the diversity of sources is in decline, there is a countervailing force of continually increasing functionality with new services, products and applications — such as music streaming services (Spotify), file sharing programs (Dropbox) and messaging platforms (Messenger, Whatsapp and Snapchat).\nWebsite ‘infant mortality\u0026rsquo; Another major finding was the dramatic increase in the \u0026ldquo;infant mortality\u0026rdquo; rate of websites — with the big kids on the block guarding their turf more staunchly than ever.\nWe examined new domains that were continually referenced or linked-to in social media after their first appearance. We found that while almost 40% of the domains created 2006 were active five years on, only a little more than 3% of those created in 2015 remain active today.\nThe dynamics of online competition are becoming clearer and clearer. And the loss of diversity is concerning. Unlike the natural world, there are no sanctuaries; competition is part of both nature and business.\nOur study has profound implications for business leaders, investors and governments everywhere. It shows the network effects of the web don't just apply to online businesses. They have permeated the entire economy and are rewriting many previously accepted rules of economics.\nFor example, the idea that businesses can maintain a competitive advantage based on where they are physically located is increasingly tenuous. Meanwhile, there's new opportunities for companies to set up shop from anywhere in the world and serve a global customer base that's both mainstream and niche.\nThe best way to encourage diversity is to have more global online businesses focused on providing diverse services, by addressing consumers\u0026rsquo; increasingly niche needs.\nIn Australia, we're starting to see this through homegrown companies such as Canva, SafetyCulture and iWonder. Hopefully many more will appear in the decade ahead.\nThis article was first published by the authors on The Conversation.\n","date":1625184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625184000,"objectID":"7de58c534e5d77d2f09366caed0cae14","permalink":"https://www.behavioral-ds.science/blogpost/online_diversity/","publishdate":"2021-07-02T00:00:00Z","relpermalink":"/blogpost/online_diversity/","section":"blogpost","summary":"More than a quarter of a century since the first commercial use of the online world, its growth is now slowing down in some key categories.","tags":null,"title":"We spent six years scouring billions of links, and found the web is both expanding and shrinking ","type":"blogpost"},{"authors":["Paul X. McCarthy, Marian-Andrei Rizoiu"],"categories":["Research"],"content":"The online world is continuously expanding — always aggregating more services, more users and more activity. Last year, the number of websites registered on the \u0026ldquo;.com\u0026rdquo; domain surpassed 150,000,000.\nHowever, more than a quarter of a century since its first commercial use, the growth of the online world is now slowing down in some key categories.\nWe conducted a multi-year research project analysing global trends in online diversity and dominance. Our research, published in Public Library of Science, is the first to reveal some long-term trends in how businesses compete in the age of the web.\nWe saw a dramatic consolidation of attention towards a shrinking (but increasingly dominant) group of online organisations. So, while there is still growth in the functions, features and applications offered on the web, the number of entities providing these functions is shrinking.\nWeb diversity nosedives We analysed more than six billion user comments from the social media website Reddit dating back to 2006, as well as 11.8 billion Twitter posts from as far back as 2011. In total, our research used a massive 5.6Tb trove of data from more than a decade of global activity.\nThis dataset was more than four times the size of the original data from the Hubble Space Telescope, which helped Brian Schmidt and colleagues do their Nobel-prize winning work in 1998 to prove the universe's expansion is accelerating.\nWith the Reddit posts, we analysed all the links to other sites and online services — more than one billion in total — to understand the dynamics of link growth, dominance and diversity through the decade.\nWe used a measure of link \u0026ldquo;uniqueness\u0026rdquo;. On this scale, 1 represents maximum diversity (all links have their own domain) and 0 is minimum diversity (all links are on one domain, such as \u0026ldquo;youtube.com\u0026rdquo;).\nA decade ago, there was a much greater variety of domains within links posted by users of Reddit, with more than 20 different domains for every 100 random links users posted. Now there are only about five different domains for every 100 links posted.\nIn fact, between 60-70% of all attention on key social media platforms is focused towards just ten popular domains.\nBeyond social media platforms, we also studied linkage patterns across the web, looking at almost 20 billion links over three years. These results reinforced the \u0026ldquo;rich are getting richer\u0026rdquo; online.\nThe authority, influence and visibility of the top 1,000 global websites (as measured by network centrality or PageRank) is growing every month, at the expense of all other sites.\nApp diversity is on the rise The web started as a source of innovation, new ideas and inspiration — a technology that opened up the playing field. It's now also becoming a medium that actually stifles competition and promotes monopolies and the dominance of a few players.\nOur findings resolve a long-running paradox about the nature of the web: does it help grow businesses, jobs and investment? Or does it make it harder to get ahead by letting anyone and everyone join the game? The answer, it turns out, is it does both.\nWhile the diversity of sources is in decline, there is a countervailing force of continually increasing functionality with new services, products and applications — such as music streaming services (Spotify), file sharing programs (Dropbox) and messaging platforms (Messenger, Whatsapp and Snapchat).\nWebsite ‘infant mortality\u0026rsquo; Another major finding was the dramatic increase in the \u0026ldquo;infant mortality\u0026rdquo; rate of websites — with the big kids on the block guarding their turf more staunchly than ever.\nWe examined new domains that were continually referenced or linked-to in social media after their first appearance. We found that while almost 40% of the domains created 2006 were active five years on, only a little more than 3% of those created in 2015 remain active today.\nThe dynamics of online competition are becoming clearer and clearer. And the loss of diversity is concerning. Unlike the natural world, there are no sanctuaries; competition is part of both nature and business.\nOur study has profound implications for business leaders, investors and governments everywhere. It shows the network effects of the web don't just apply to online businesses. They have permeated the entire economy and are rewriting many previously accepted rules of economics.\nFor example, the idea that businesses can maintain a competitive advantage based on where they are physically located is increasingly tenuous. Meanwhile, there's new opportunities for companies to set up shop from anywhere in the world and serve a global customer base that's both mainstream and niche.\nThe best way to encourage diversity is to have more global online businesses focused on providing diverse services, by addressing consumers\u0026rsquo; increasingly niche needs.\nIn Australia, we're starting to see this through homegrown companies such as Canva, SafetyCulture and iWonder. Hopefully many more will appear in the decade ahead.\nThis article was first published by the authors on The Conversation.\n","date":1625184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625184000,"objectID":"5403cd8a8010a78a17fdce661d172816","permalink":"https://www.behavioral-ds.science/theme1_content/online_diversity/","publishdate":"2021-07-02T00:00:00Z","relpermalink":"/theme1_content/online_diversity/","section":"theme1_content","summary":"More than a quarter of a century since the first commercial use of the online world, its growth is now slowing down in some key categories.","tags":null,"title":"We spent six years scouring billions of links, and found the web is both expanding and shrinking ","type":"theme1_content"},{"authors":["Rohit Ram"],"categories":["Software"],"content":" -- Solution -- Social media platforms, although relatively new, host millions of users and billions of interactions daily. As tied as we are to these platforms, they profoundly impact our social institutions through phenomena such as disinformation, political polarization, and social bots.\nResearchers are increasingly interested in trying to form an understanding of phenomena and their implications. Social scientists, political scientists, and data practitioners alike curate expansive datasets to combat these potentially adverse effects on our society; however, they lack the appropriate tooling.\nbirdspotter is an easy-to-use tool that models Twitter users\u0026rsquo; attributes and labels them. It comes prepackaged with a state-of-the-art bot detector and an influence quantification system based on tweet dynamics. birdspotter features a generalized user labeler, which can be retrained easily with the engineered features to address a variety of use cases. Also, birdspotter.ml is a web application that can be utilized to explore datasets and derive a narrative around a dataset.\nIn this post, I'll showcase the basic usage of birdspotter and birdspotter.ml.\nInstallation The package can be installed in the canonical python way:\npip install birdspotter  Getting a dataset The Twitter T\u0026amp;Cs restrict the sharing of tweet data directly online; however, they do allow the sharing of tweet-ids, which can be converted to full tweet data through a process called hydration. Tools like twarc can be used to hydrate a Tweet ID dataset. The resulting dataset will be in jsonl (line delimited json) format, which birdspotter accepts directly.\nIn the below examples, we use two datasets; a collection of COVID-19 related tweets from January 31st, 2020 [1], and a collection of tweets about politicians on Twitter [2].\nThe politicians\u0026rsquo; dataset was acquired through the following process (and a similar process was taken for the COVID-19 dataset):\npip install twarc wget http://twitterpoliticians.org./downloads/base/all_tweet_ids.csv twarc hydrate all_tweet_ids.csv \u0026gt; tweets.jsonl  Basic Usage The code below imports the main class Birdspotter, extracts the tweets from their standard format, labels the users with the default bot detector and influence, and reformats the retweet cascades into a tidier format.\n## Import birdspotter from birdspotter import BirdSpotter ## Extracts the tweets from the raw jsonl [https://github.com/echen102/COVID-19-TweetIDs] bs = BirdSpotter('covid19.jsonl') ## Uses the default bot labeller and influence quantification systems bs.getLabeledUsers() ## Formats the retweet cascades, such that expected retweet structures can extracted bs.getCascadesDataFrame() ## Access the botness labels and influence scores bs.featureDataframe[['botness', 'influence']]  From here, the dataset is readily profile-able:\nbotness_dist = sns.histplot(data=bs.featureDataframe, x=\u0026quot;botness\u0026quot;) influence_eccdf = sns.ecdfplot(data=bs.featureDataframe, x=\u0026quot;influence\u0026quot;, complementary=True).set(xscale=\u0026quot;log\u0026quot;, yscale=\u0026quot;log\u0026quot;)  The visualizer An alternative way to profile a dataset is the use birdspotter.ml, which facilitates dataset exploration and narrative construction.\nThe visualizer features a scatterplot (on the left) of influence and botness for a sample of users and the population density. The colors represent the hashtags (a proxy for the topic) that the users most tweet about in the dataset. Users within the scatterplot are hoverable and selectable, and their information populates in the components on the right.\nThe top right component shows information and metrics about the selected user and links the user's profile.\nThe bottom right component shows the retweet cascades where a user has participated and highlights their participation. The points represent the follower counts (social capital) of users and their retweets/tweets\u0026rsquo; timing. The points are also hoverable and selectable.\nCustomising the labeller By default, the labeler is trained as a bot detection system, comparable to the state-of-the-art botometer. Notable, birdspotter is provided in an offline package and can be applied at scale, while botometer is accessible only via an online API, which is often prohibitively rate-limited.\nbirdspotter is a versatile tool and can be utilized by practitioners for a variety of use-cases. For example, we could train the labeler to identify political leaning. This process is a bit involved, so we summarise it below;\n We hydrate some tweets from the Twitter Parlimentarian Database We filter the tweets to include only Australian Politicians. We label right-wing partied politicians positively, and others negatively (with bs_pol.getBotAnnotationTemplate for example) We retrain birdspotter with these new labels and label all users (i.e., including users the politicians retweeted) using the new model  bs_pol = BirdSpotter('aus_tweets.jsonl') bs_pol.trainClassifierModel('pol_training_data.pickle') bs_pol.getLabeledUsers()  On this limited of Australian politicians dataset, a 10-fold CV of birdspotter garners an average AUC (Area under ROC) of 0.986.\nConclusion birdspotter aims to democratize social analyzes that were once the domain of machine learning experts, generating insights and understanding of online phenomena and mitigating their potentially adverse effects on our society. This post shows how birdspotter can be used in both a simple and advanced way to recover such insights.\nReferences [1] Chen, E. et al. 2020. Tracking social media discourse about the covid-19 pandemic: Development of a public coronavirus twitter data set. JMIR Public Health and Surveillance. 6, 2 (2020), e19273.\n[2] Vliet, L. van et al. 2020. The twitter parliamentarian database: Analyzing twitter politics across 26 countries. PloS one. 15, 9 (2020), e0237073.\n","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"ca0702d4d1b26a8e5d6ff0c1727ceb36","permalink":"https://www.behavioral-ds.science/theme2_content/birdspotter/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/theme2_content/birdspotter/","section":"theme2_content","summary":"We introduce `birdspotter`, an R tool that models Twitter users' attributes and labels them.","tags":["python"],"title":"birdspotter: A toolkit for analyzing and labelling Twitter users","type":"theme2_content"},{"authors":["Nik Dawson (edited by Marian-Andrei Rizoiu)"],"categories":["blogpost"],"content":"Summary:\n We build a machine learning-based Job Transitions Recommender System that can accurately predict the probability of transitioning between occupations. We showcase the system for workers forced to transition between jobs. The system is based on a novel data-driven method to measure the similarity between occupations based on their underlying skill profiles and real-time job ads. We also build a leading indicator of Artificial Intelligence adoption in Australian industries, outlining gaps, opportunities, and trends. For full technical details, please read the pre-print.  People are forced to change jobs as new technologies automate labour, production is moved abroad, and economic crises unfold. Successfully transitioning between jobs, however, requires leveraging current skills and acquiring others, which can falter if the skills gap is too large.\nIn our latest research, Nik Dawson and Marian-Andrei Rizoiu (together with collaborator Mary-Anne Williams) and I developed a novel method to measure the \u0026lsquo;distance\u0026rsquo; between sets of skills using real-time job ads data. We then use these measures to build a recommender system that accurately predicts the probability of transitioning from one occupation to every other possible occupation. Intuitively, two occupations have a high probability of successfully transitioning when their skill sets are highly similar (i.e. the distance is small). For example, an Accountant has a high probability of transitioning to become a Financial Analyst because their skill sets are similar; whereas a Speech Therapist has a low transition probability to becoming a Financial Analyst as their skill sets are very different. This isn't to say that it's not possible. Rather, the skills gap is large, so the probability of successfully transitioning is diminished.\nThe SKILL SPACE Method Distance between skills\nIn order to measure the distance between occupations from their underlying skill sets, we first measure the distance between individual skills in job ads for each calendar year from 2012-2020. To achieve this bottom-up approach, we first use a method from Trade Economics, called ‘Revealed Comparative Advantage\u0026rsquo; (RCA), to identify how important an individual skill is to a job ad (i.e. comparative advantage). Then, after some normalisation, we calculate the pairwise similarity between every skill for each year. The image below shows the skill distance embeddings for the top 500 skills by posting frequency in 2018.\nHere, each marker represents an individual skill that is coloured according to one of 13 clusters of highly similar skills. As seen in the Software Development cluster (see inset), highly similar skills cluster closely together, such as ‘Python\u0026rsquo; and ‘C++\u0026rsquo;. The skills map also provides useful insights, highlighting that specialised skills (such as ‘Software Development\u0026rsquo; and ‘Healthcare\u0026rsquo;) tend to lay toward the edges of the embedding, whereas more general and transferable skills lay toward the middle, acting as a ‘bridge\u0026rsquo; to specialist skills.\nDistance between occupations\nNext, we use the pairwise skill distances to measure the distance between sets of skills. In this example, we define sets of skills by their occupational groupings. But they can just as easily be defined by other groupings, such as companies, industries, or personalised skill sets. We calculate the distance between skill sets as the weighted average similarity between the individual skills in each set, where the weights correspond to the skill importance in their respective sets. The figure below visualises the distance between Australian occupations in 2018.\nEach occupation is represented by a marker and coloured on a scale according to their automation susceptibility, as calculated by Frey and Osborne - dark blue represents low-risk probability and dark red shows high risk probability over the coming two decades. As seen in the magnified inset, similar occupations lie close together on the map. Further, occupations in low risk of automation tend to be characterised by non-routine, interpersonal, and/or high cognitive labour tasks; whereas occupations in high risk of automation tend to require routine, manual, and/or low cognitive labour tasks. For example, in the inset of the Figure above, a ‘Sheetmetal Trades Worker\u0026rsquo; is deemed to be at high risk of labour automation (82% probability) due to high levels of routine and manual labour tasks required by the occupation. However, the skill set demands of a ‘Sheetmetal Trades Worker\u0026rsquo; are highly similar to an ‘Industrial Designer\u0026rsquo;, which is considered at low risk of labour automation over the coming two decades (4% probability). Therefore, an ‘Industrial Designer\u0026rsquo; represents a transition opportunity for a ‘Sheetmetal Trades Worker\u0026rsquo; that leverages existing skills and protects against potential risks of technological labour automation.\nConstructing a Job Transitions Recommender System The SKILL SPACE Method described above achieves high levels of accuracy in predicting job transitions. However, these are symmetric measures and we know that job transitions are asymmetric - it is more difficult to transition between jobs in one direction than the other. Therefore, transitions are determined by more than the symmetric distance between skill sets; other factors, such as educational requirements and experience demands, contribute to these asymmetries.\nWe account for the asymmetries between job transitions by training a machine learning classifier model that combines the SKILL SPACE distance measures with other labour market variables from job ads data and employment statistics (see the pre-print for full details). Our machine learning model is trained against a dataset of ‘actual\u0026rsquo; (i.e. real life) job transitions from an Australian longitudinal household survey. We then apply the trained model to predict the probability for every possible occupational transition in the dataset - described as the transition probability between a ‘source\u0026rsquo; and a ‘target\u0026rsquo; occupation. This creates the Transitions Map, for which a subset of 20 occupations can be seen in the figure below.\nThe coloured heatmap shows the transition probabilities (‘source\u0026rsquo; occupations are in columns and ‘targets\u0026rsquo; are in rows). Dark blue represents higher transition probabilities and lighter blue shows lower probabilities, where the asymmetries between occupation pairs are clearly observed. For example, a ‘Finance Manager\u0026rsquo; has a higher probability of transitioning to become an ‘Accounting Clerk\u0026rsquo; than the reverse direction. Moreover, transitioning to some occupations is generally easier (for example, ‘Bar Attendants and Baristas\u0026rsquo;) than others (‘Life Scientists\u0026rsquo;). The dendrogram (the lines on the left and top of the chart) illustrates the hierarchical clusters of occupations where there is a clear divide between service-oriented professions and manual labour occupations.\nOur model achieves high levels of performance, accurately predicting 76% of occupational transitions.\nRecommending Jobs and Skills The Transitions Map provides the basis for making qualified job transition recommendations. We call this the Job Transitions Recommender System. In the figure below, we highlight the example of `Domestic Cleaners\u0026rsquo;, an occupation that has experienced significant declines in labour demand and employment levels during COVID-19 in Australia.\nFirst, we use the Transitions Map to recommend the occupations with the highest transition probabilities; these are the named occupations on the right side of the flow diagram, ordered in descending order of transition probability. Segment widths show the labour demand for each of the recommended occupations during the beginning of the COVID-19 period (measured by posting frequency). The segment colours represent the percentage change of posting frequency during March and April 2019 compared to the same months in 2020; dark red indicates a big decrease in job ad posts and dark blue indicates a big increase. The first six transition recommendations for ‘Domestic Cleaners\u0026rsquo; all experienced negative demand, which is unsurprising given that ‘non-essential\u0026rsquo; services were restricted in Australia during this period. However, the seventh recommendation, ‘Aged and Disabled Carers\u0026rsquo;, had significantly grown in demand during the beginning of the COVID-19 period and there was a high number of jobs advertised. Given that it is generally favorable to transition to high demand jobs, we selected ‘Aged and Disabled Carers\u0026rsquo; as the target occupation for this example.\nSkill recommendations for target occupations\nWe then take the Job Transitions Recommender System a step further by incorporating skill recommendations. Transitioning to a new occupation generally requires developing new skills under time and resource constraints. Therefore, workers must prioritise which skills to develop. We argue that a worker should invest in developing a skill when (1) the skill is important to the target occupation and (2) the distance to acquire the skill is large (that is, it is relatively difficult to acquire). In the case of the ‘Domestic Cleaner\u0026rsquo; in the figure above, the top recommended skills to assist in the transition to the ‘Aged and Disabled Carer\u0026rsquo; occupation are specialised patient care skills, such as ‘Patient Hygiene Assistance\u0026rsquo;. Conversely, the reasons not to develop a skill are when (1) the skill is not important or (2) the distance is small to the target occupation. The figure shows that while some ‘Aged and Disabled Carer\u0026rsquo; jobs require ‘Business Analysis\u0026rsquo; and ‘Finance\u0026rsquo; skills, these skills are of low importance for the ‘Aged and Disabled Carer\u0026rsquo; occupation, so they should not be prioritised. Similarly, skills such as ‘Ironing\u0026rsquo; and ‘Laundry\u0026rsquo; are required by ‘Aged and Disabled Carer\u0026rsquo; jobs but the distance is small, so it is likely that either a ‘Domestic Cleaner\u0026rsquo; already possesses these skills or they can easily acquire them.\nDeveloping a Leading Indicator of AI Adoption The SKILL SPACE method can also be flexibly applied to other adjacent problems, such as identifying the extent of specific skill gaps of firms within industries and potential adoption of new technologies. Here, we develop a leading indicator for emerging technology adoption and potential labour market disruptions based on skills data, using Artificial Intelligence (AI) as an example. We select AI because of its potential impacts on transforming labour tasks and accelerating job transitions. By applying SKILL SPACE, we were able to measure the yearly similarities between an adaptive set of AI skills against each of the 19 Australian industries from 2013-2019 (see the pre-print for technical details). As industry skill sets become more similar to AI skills, the skills gap is diminished and firms within industries are more likely to possess the skills to make productive use of AI technologies. The growth of AI skill similarity within industries is shown by the coloured areas of the radar chart below.\nAll industries have increased their similarity levels to AI skills. This highlights the growing importance of AI skills across the Australian labour market. However, the rates of similarity are unequally distributed. Some industries, such as ‘Finance and Insurance Services\u0026rsquo; and ‘Information Media and Telecommunications\u0026rsquo; command much higher rates of AI skill similarity. This indicates that not only are firms within these industries increasingly demanding AI skills but also that the AI skills gaps within these industries are much smaller. Therefore, it is likely that firms within these industries are adopting AI and making productive use of these technologies.\nAlso noteworthy are the differences in growth rates toward AI skill similarity. As the figure above clearly shows, AI skill similarity has rapidly grown for some industries and more modestly for others. For instance, ‘Retail Trade\u0026rsquo; has experienced the highest levels of growth in similarity to AI skills, increasing by 407% from 2013 to 2019. The majority of this growth has occurred recently, which coincides with the launch of Amazon Australia in 2017. Since then, Amazon has swiftly hired thousands in Australia. This indicator can assist policy-makers and businesses to robustly monitor the growth of AI skills (or other emerging technologies), which acts as a proxy for AI adoption within industries.\nConclusion While the future of work remains unclear, change is inevitable. New technologies, economic crises, and other factors will continue to shift labour demands causing workers to move between jobs. If labour transitions occur efficiently, significant productivity and equity benefits arise at all levels of the labour market; if transitions are slow, or fail, significant costs are borne to both the State and the individual. Therefore, it's in the interests of workers, firms, and governments that labour transitions are efficient and effective. The methods and systems we put forward here could significantly improve the achievement of these goals.\nFor the full technical details, please read the pre-print.\nAcknowledgements\nWe thank Bledi Taska and Davor Miskulin from Burning Glass Technologies for generously providing the job advertisements data for this research and for their valuable feedback. We also thank Stijn Broecke and other colleagues from the OECD for their ongoing input and guidance in the development of this work.\n","date":1612396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612396800,"objectID":"281b03802d57d936edbbd9eec3588fbc","permalink":"https://www.behavioral-ds.science/blogpost/job_transitions/","publishdate":"2021-02-04T00:00:00Z","relpermalink":"/blogpost/job_transitions/","section":"blogpost","summary":"Summary:\n We build a machine learning-based Job Transitions Recommender System that can accurately predict the probability of transitioning between occupations. We showcase the system for workers forced to transition between jobs. The system is based on a novel data-driven method to measure the similarity between occupations based on their underlying skill profiles and real-time job ads. We also build a leading indicator of Artificial Intelligence adoption in Australian industries, outlining gaps, opportunities, and trends.","tags":null,"title":"Job Transitions in a Time of Automation and Labour Market Crises","type":"blogpost"},{"authors":["Nik Dawson (edited by Marian-Andrei Rizoiu)"],"categories":["Paper"],"content":"Summary:\n We build a machine learning-based Job Transitions Recommender System that can accurately predict the probability of transitioning between occupations. We showcase the system for workers forced to transition between jobs. The system is based on a novel data-driven method to measure the similarity between occupations based on their underlying skill profiles and real-time job ads. We also build a leading indicator of Artificial Intelligence adoption in Australian industries, outlining gaps, opportunities, and trends. For full technical details, please read the pre-print.  People are forced to change jobs as new technologies automate labour, production is moved abroad, and economic crises unfold. Successfully transitioning between jobs, however, requires leveraging current skills and acquiring others, which can falter if the skills gap is too large.\nIn our latest research, Nik Dawson and Marian-Andrei Rizoiu (together with collaborator Mary-Anne Williams) and I developed a novel method to measure the \u0026lsquo;distance\u0026rsquo; between sets of skills using real-time job ads data. We then use these measures to build a recommender system that accurately predicts the probability of transitioning from one occupation to every other possible occupation. Intuitively, two occupations have a high probability of successfully transitioning when their skill sets are highly similar (i.e. the distance is small). For example, an Accountant has a high probability of transitioning to become a Financial Analyst because their skill sets are similar; whereas a Speech Therapist has a low transition probability to becoming a Financial Analyst as their skill sets are very different. This isn't to say that it's not possible. Rather, the skills gap is large, so the probability of successfully transitioning is diminished.\nThe SKILL SPACE Method Distance between skills\nIn order to measure the distance between occupations from their underlying skill sets, we first measure the distance between individual skills in job ads for each calendar year from 2012-2020. To achieve this bottom-up approach, we first use a method from Trade Economics, called ‘Revealed Comparative Advantage\u0026rsquo; (RCA), to identify how important an individual skill is to a job ad (i.e. comparative advantage). Then, after some normalisation, we calculate the pairwise similarity between every skill for each year. The image below shows the skill distance embeddings for the top 500 skills by posting frequency in 2018.\nHere, each marker represents an individual skill that is coloured according to one of 13 clusters of highly similar skills. As seen in the Software Development cluster (see inset), highly similar skills cluster closely together, such as ‘Python\u0026rsquo; and ‘C++\u0026rsquo;. The skills map also provides useful insights, highlighting that specialised skills (such as ‘Software Development\u0026rsquo; and ‘Healthcare\u0026rsquo;) tend to lay toward the edges of the embedding, whereas more general and transferable skills lay toward the middle, acting as a ‘bridge\u0026rsquo; to specialist skills.\nDistance between occupations\nNext, we use the pairwise skill distances to measure the distance between sets of skills. In this example, we define sets of skills by their occupational groupings. But they can just as easily be defined by other groupings, such as companies, industries, or personalised skill sets. We calculate the distance between skill sets as the weighted average similarity between the individual skills in each set, where the weights correspond to the skill importance in their respective sets. The figure below visualises the distance between Australian occupations in 2018.\nEach occupation is represented by a marker and coloured on a scale according to their automation susceptibility, as calculated by Frey and Osborne - dark blue represents low-risk probability and dark red shows high risk probability over the coming two decades. As seen in the magnified inset, similar occupations lie close together on the map. Further, occupations in low risk of automation tend to be characterised by non-routine, interpersonal, and/or high cognitive labour tasks; whereas occupations in high risk of automation tend to require routine, manual, and/or low cognitive labour tasks. For example, in the inset of the Figure above, a ‘Sheetmetal Trades Worker\u0026rsquo; is deemed to be at high risk of labour automation (82% probability) due to high levels of routine and manual labour tasks required by the occupation. However, the skill set demands of a ‘Sheetmetal Trades Worker\u0026rsquo; are highly similar to an ‘Industrial Designer\u0026rsquo;, which is considered at low risk of labour automation over the coming two decades (4% probability). Therefore, an ‘Industrial Designer\u0026rsquo; represents a transition opportunity for a ‘Sheetmetal Trades Worker\u0026rsquo; that leverages existing skills and protects against potential risks of technological labour automation.\nConstructing a Job Transitions Recommender System The SKILL SPACE Method described above achieves high levels of accuracy in predicting job transitions. However, these are symmetric measures and we know that job transitions are asymmetric - it is more difficult to transition between jobs in one direction than the other. Therefore, transitions are determined by more than the symmetric distance between skill sets; other factors, such as educational requirements and experience demands, contribute to these asymmetries.\nWe account for the asymmetries between job transitions by training a machine learning classifier model that combines the SKILL SPACE distance measures with other labour market variables from job ads data and employment statistics (see the pre-print for full details). Our machine learning model is trained against a dataset of ‘actual\u0026rsquo; (i.e. real life) job transitions from an Australian longitudinal household survey. We then apply the trained model to predict the probability for every possible occupational transition in the dataset - described as the transition probability between a ‘source\u0026rsquo; and a ‘target\u0026rsquo; occupation. This creates the Transitions Map, for which a subset of 20 occupations can be seen in the figure below.\nThe coloured heatmap shows the transition probabilities (‘source\u0026rsquo; occupations are in columns and ‘targets\u0026rsquo; are in rows). Dark blue represents higher transition probabilities and lighter blue shows lower probabilities, where the asymmetries between occupation pairs are clearly observed. For example, a ‘Finance Manager\u0026rsquo; has a higher probability of transitioning to become an ‘Accounting Clerk\u0026rsquo; than the reverse direction. Moreover, transitioning to some occupations is generally easier (for example, ‘Bar Attendants and Baristas\u0026rsquo;) than others (‘Life Scientists\u0026rsquo;). The dendrogram (the lines on the left and top of the chart) illustrates the hierarchical clusters of occupations where there is a clear divide between service-oriented professions and manual labour occupations.\nOur model achieves high levels of performance, accurately predicting 76% of occupational transitions.\nRecommending Jobs and Skills The Transitions Map provides the basis for making qualified job transition recommendations. We call this the Job Transitions Recommender System. In the figure below, we highlight the example of `Domestic Cleaners\u0026rsquo;, an occupation that has experienced significant declines in labour demand and employment levels during COVID-19 in Australia.\nFirst, we use the Transitions Map to recommend the occupations with the highest transition probabilities; these are the named occupations on the right side of the flow diagram, ordered in descending order of transition probability. Segment widths show the labour demand for each of the recommended occupations during the beginning of the COVID-19 period (measured by posting frequency). The segment colours represent the percentage change of posting frequency during March and April 2019 compared to the same months in 2020; dark red indicates a big decrease in job ad posts and dark blue indicates a big increase. The first six transition recommendations for ‘Domestic Cleaners\u0026rsquo; all experienced negative demand, which is unsurprising given that ‘non-essential\u0026rsquo; services were restricted in Australia during this period. However, the seventh recommendation, ‘Aged and Disabled Carers\u0026rsquo;, had significantly grown in demand during the beginning of the COVID-19 period and there was a high number of jobs advertised. Given that it is generally favorable to transition to high demand jobs, we selected ‘Aged and Disabled Carers\u0026rsquo; as the target occupation for this example.\nSkill recommendations for target occupations\nWe then take the Job Transitions Recommender System a step further by incorporating skill recommendations. Transitioning to a new occupation generally requires developing new skills under time and resource constraints. Therefore, workers must prioritise which skills to develop. We argue that a worker should invest in developing a skill when (1) the skill is important to the target occupation and (2) the distance to acquire the skill is large (that is, it is relatively difficult to acquire). In the case of the ‘Domestic Cleaner\u0026rsquo; in the figure above, the top recommended skills to assist in the transition to the ‘Aged and Disabled Carer\u0026rsquo; occupation are specialised patient care skills, such as ‘Patient Hygiene Assistance\u0026rsquo;. Conversely, the reasons not to develop a skill are when (1) the skill is not important or (2) the distance is small to the target occupation. The figure shows that while some ‘Aged and Disabled Carer\u0026rsquo; jobs require ‘Business Analysis\u0026rsquo; and ‘Finance\u0026rsquo; skills, these skills are of low importance for the ‘Aged and Disabled Carer\u0026rsquo; occupation, so they should not be prioritised. Similarly, skills such as ‘Ironing\u0026rsquo; and ‘Laundry\u0026rsquo; are required by ‘Aged and Disabled Carer\u0026rsquo; jobs but the distance is small, so it is likely that either a ‘Domestic Cleaner\u0026rsquo; already possesses these skills or they can easily acquire them.\nDeveloping a Leading Indicator of AI Adoption The SKILL SPACE method can also be flexibly applied to other adjacent problems, such as identifying the extent of specific skill gaps of firms within industries and potential adoption of new technologies. Here, we develop a leading indicator for emerging technology adoption and potential labour market disruptions based on skills data, using Artificial Intelligence (AI) as an example. We select AI because of its potential impacts on transforming labour tasks and accelerating job transitions. By applying SKILL SPACE, we were able to measure the yearly similarities between an adaptive set of AI skills against each of the 19 Australian industries from 2013-2019 (see the pre-print for technical details). As industry skill sets become more similar to AI skills, the skills gap is diminished and firms within industries are more likely to possess the skills to make productive use of AI technologies. The growth of AI skill similarity within industries is shown by the coloured areas of the radar chart below.\nAll industries have increased their similarity levels to AI skills. This highlights the growing importance of AI skills across the Australian labour market. However, the rates of similarity are unequally distributed. Some industries, such as ‘Finance and Insurance Services\u0026rsquo; and ‘Information Media and Telecommunications\u0026rsquo; command much higher rates of AI skill similarity. This indicates that not only are firms within these industries increasingly demanding AI skills but also that the AI skills gaps within these industries are much smaller. Therefore, it is likely that firms within these industries are adopting AI and making productive use of these technologies.\nAlso noteworthy are the differences in growth rates toward AI skill similarity. As the figure above clearly shows, AI skill similarity has rapidly grown for some industries and more modestly for others. For instance, ‘Retail Trade\u0026rsquo; has experienced the highest levels of growth in similarity to AI skills, increasing by 407% from 2013 to 2019. The majority of this growth has occurred recently, which coincides with the launch of Amazon Australia in 2017. Since then, Amazon has swiftly hired thousands in Australia. This indicator can assist policy-makers and businesses to robustly monitor the growth of AI skills (or other emerging technologies), which acts as a proxy for AI adoption within industries.\nConclusion While the future of work remains unclear, change is inevitable. New technologies, economic crises, and other factors will continue to shift labour demands causing workers to move between jobs. If labour transitions occur efficiently, significant productivity and equity benefits arise at all levels of the labour market; if transitions are slow, or fail, significant costs are borne to both the State and the individual. Therefore, it's in the interests of workers, firms, and governments that labour transitions are efficient and effective. The methods and systems we put forward here could significantly improve the achievement of these goals.\nFor the full technical details, please read the pre-print.\nAcknowledgements\nWe thank Bledi Taska and Davor Miskulin from Burning Glass Technologies for generously providing the job advertisements data for this research and for their valuable feedback. We also thank Stijn Broecke and other colleagues from the OECD for their ongoing input and guidance in the development of this work.\n","date":1612396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612396800,"objectID":"a007e4b789a62fad0b9f52133f72c42b","permalink":"https://www.behavioral-ds.science/theme3_content/job_transitions/","publishdate":"2021-02-04T00:00:00Z","relpermalink":"/theme3_content/job_transitions/","section":"theme3_content","summary":"We build a machine learning-based recommender system that can accurately predict the probability of transitioning between occupations.","tags":null,"title":"Job Transitions in a Time of Automation and Labour Market Crises","type":"theme3_content"},{"authors":["Quyu Kong, Rohit Ram"],"categories":["Software"],"content":"We demonstrate in this blog post a tutorial on applying the tools for analyzing online information diffusions about Twitter users, birdspotter and evently.\nDataset In this tutorial, we apply two tools for analyzing Twitter users, on a COVID-19 retweet dataset. The dataset is curated by Chen, et al. One can obtain a copy of the tweet IDs from their project. We only use the 31st of Janury sample of the whole dataset for demonstration purpose. The tweets can be recovered by hydration from their IDs. We note that some tweets might have been deleted and in the end we manage to get 69.2% (1,489,877) of the original tweets.\nTools While BirdSpotter captures the social influence and botness of Twitter users, evently specifically models the temporal dynamics of online information diffusion. We leverage information provided by the tools to study the users in the COVID19 dataset.\nlibrary(evently) library(reticulate) birdspotter \u0026lt;- import('birdspotter')  Preprocessing tweets At this step, we seek to extract diffusion cascades from the COVID-19 dataset for analyzing user influence and botness. A diffusion cascade consist of an initial tweet posted by a Twitter user and followed then by a sereis of retweets. A function provided by evently allows one to obtain cascades from JSON formatted raw tweets. On the other hand, we initialize a BirdSpotter instance and compute the influence and botness scores for all users in the dataset.\ncascades \u0026lt;- parse_raw_tweets_to_cascades('corona_2020_01_31.jsonl', keep_user = T, keep_absolute_time = T) bs \u0026lt;- birdspotter$BirdSpotter('corona_2020_01_31.jsonl') labeled_users \u0026lt;- bs$getLabeledUsers()[, c('user_id', 'botness', 'influence')]  As we cannot publish corona_2020_01_31.jsonl due to Twitter TOC, we have stored the results and load them below\nload('corona_2020_01_31.rda') labeled_users \u0026lt;- read.csv('corona_31_botness_influence.csv', stringsAsFactors = F, colClasses=c(\u0026quot;character\u0026quot;,rep(\u0026quot;numeric\u0026quot;,3)))  We note that all user IDs have been encrypted. After obtaining the results, let’s first conduct some simple measurements on users and cascades.\nlibrary(ggplot2) # check the density of these two values mean_bot \u0026lt;- mean(labeled_users$botness, na.rm = T) ggplot(labeled_users, aes(botness)) + stat_density(geom = 'line') + geom_vline(xintercept = mean_bot, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_bot, y = 2, label= sprintf('mean: %s', round(mean_bot, 2))), color= 'red', angle=90, vjust=-0.11)  mean_inf \u0026lt;- mean(labeled_users$influence) ggplot(labeled_users) + stat_ecdf(aes(influence, 1 - ..y..)) + scale_x_log10() + scale_y_log10() + ylab('CCDF') + geom_vline(xintercept = mean_inf, linetype=2, color = 'red') +geom_text(data=data.frame(), aes(x = mean_inf, y = 1e-3, label= sprintf('mean: %s', round(mean_inf, 2))), color= 'red', angle=90, vjust=-0.11)  ## Warning: Transformation introduced infinite values in continuous y-axis  mean_value \u0026lt;- mean(sapply(cascades, nrow)) ggplot(data.frame(size = sapply(cascades, nrow))) + stat_ecdf(aes(size, 1 - ..y..)) + scale_x_log10() + scale_y_log10() + geom_vline(xintercept = mean_value, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('cascade size') + ylab('CCDF')  ## Warning: Transformation introduced infinite values in continuous y-axis  mean_value2 \u0026lt;- mean(sapply(cascades, function(c) c$time[nrow(c)])) ggplot(data.frame(time = sapply(cascades, function(c) c$time[nrow(c)]))) + stat_ecdf(aes(time, 1 - ..y..)) + scale_x_continuous(trans = 'log1p', breaks = c(0, 100, 10000, 1000000), labels = c('0', '1e2', '1e4', '1e6')) + scale_y_log10() + geom_vline(xintercept = mean_value2, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_value2, y = 1e-3, label= sprintf('mean: %s', round(mean_value2, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('cascade final event time')+ ylab('CCDF')  ## Warning: Transformation introduced infinite values in continuous y-axis  mean_value \u0026lt;- mean(labeled_users$activity) ggplot(data.frame(size = labeled_users$activity)) + stat_ecdf(aes(size, 1 - ..y..)) + scale_x_log10() + scale_y_log10() + geom_vline(xintercept = mean_value, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('user activity')+ ylab('CCDF')  ## Warning: Transformation introduced infinite values in continuous y-axis  Retrain the bot detector If one find the botness scores are not accurate, birdspotter provides a relabeling tool and a retrain API to learn from the given relabeled dataset\n# output a file for mannual labeling bs$getBotAnnotationTemplate('users_to_label.csv') # Once annotated the botness detector can be trained with bs$trainClassifierModel('users_to_label.csv')  Fit user posted cacsades with evently We model a group of cascades initiated by a particular user jointly and treat the fitted model as a characterization of the user. In this example, we select two users for comparison.\nselected_users \u0026lt;- c('369686755237813560', '174266868073402929') # fit Hawkes process on cascades initiated by the selected users user_cascades_fitted \u0026lt;- lapply(selected_users, function(user) { # select cascades that are initiated by the \u0026quot;selected_user\u0026quot; selected_cascades \u0026lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades) # obtain the observation times; # note 1580515200 is 1st Feb when the observation stopped # as we only observed until the end of 31st Jan times \u0026lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1]) # fit a model on the selected cascades; fit_series(data = selected_cascades, model_type = 'mPL', observation_time = times, cores = 10) }) user_cascades_SEISMIC_fitted \u0026lt;- lapply(selected_users, function(user) { selected_cascades \u0026lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades) times \u0026lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1]) fit_series(data = selected_cascades, model_type = 'SEISMIC', observation_time = times) }) # check the fitted kernel functions plot_kernel_function(user_cascades_fitted) + scale_color_discrete(labels = c(\u0026quot;@BobOngHugots\u0026quot;, \u0026quot;@Jaefans_Global\u0026quot;))  The plot shows the fitted kernel functions of these two users which reflect their time-decaying influence of attracting followers to reshare their posts. We then demonstrate how to simulate new cascades\nset.seed(134841) user_magnitude \u0026lt;- Filter(function(cascade) cascade$user[[1]] == selected_users[[1]], cascades)[[1]]$magnitude[1] # simulate a new cascade from @BobOngHugots sim_cascade \u0026lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude) plot_event_series(cascade = sim_cascade, model = user_cascades_fitted[[1]])  selected_cascade \u0026lt;- Filter(function(cascade) cascade$user[1] == selected_users[[1]], cascades)[[1]] selected_time \u0026lt;- user_cascades_fitted[[1]]$observation_time[1] # simulate a cascade with a \u0026quot;selected_cascade\u0026quot; from @BobOngHugots sim_cascade \u0026lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude, init_history = selected_cascade) sprintf('%s new events simulated after cascade', nrow(sim_cascade[[1]]) - nrow(selected_cascade))  ## [1] \u0026quot;25 new events simulated after cascade\u0026quot;  predict_final_popularity(user_cascades_fitted[[1]], selected_cascade, selected_time)  ## [1] 458.303  # predict with SEISMIC model, assume we have fitted the SEISMIC model predict_final_popularity(user_cascades_SEISMIC_fitted[[1]], selected_cascade, selected_time)  ## [1] 729.923  get_branching_factor(user_cascades_fitted[[1]])  ## [1] 0.7681281  get_viral_score(user_cascades_fitted[[1]])  ## [1] 7.407763  Visualize users in a latent space We show a visualization of top 300 users posted most tweets using the features returned by evently along with the botness and influence scores from birdspotter.\n# obtain observation times here again times \u0026lt;- 1580515200 - sapply(cascades, function(cas) cas$absolute_time[1]) # indicate the grouping of each cascade with the user who started the cascade names(cascades) \u0026lt;- sapply(cascades, function(cas) cas$user[1]) # fit Hawkes processes on all cascades first fitted_corona \u0026lt;- group_fit_series(cascades, model_type = 'mPL', observation_time = times)  The fitting procedure takes quite long so we again load the pre-fitted models here\nload('fitted_models.rda') # choose the top 300 users who started most cacsades selected_users \u0026lt;- labeled_users$user_id[labeled_users$user_id %in% names(sort(sapply(fitted_corona, length), decreasing = T)[seq(300)])] # gather the stats for these users user_influences \u0026lt;- labeled_users$influence[labeled_users$user_id %in% selected_users] user_botness \u0026lt;- labeled_users$botness[labeled_users$user_id %in% selected_users] fitted_corona_selected \u0026lt;- fitted_corona[selected_users] # get the features features \u0026lt;- generate_features(fitted_corona_selected) # compute distances between users using manhattan distance features \u0026lt;- features[, -1] # remove the user id column distances \u0026lt;- dist(features, method = 'manhattan') library(tsne) positions \u0026lt;- tsne(distances, k = 2)  ## sigma summary: Min. : 0.34223375605395 |1st Qu. : 0.457223801885988 |Median : 0.489891425900637 |Mean : 0.500483006369232 |3rd Qu. : 0.538593613780411 |Max. : 0.676779919259545 | ## Epoch: Iteration #100 error is: 14.1961110881254 ## Epoch: Iteration #200 error is: 0.490122133064818 ## Epoch: Iteration #300 error is: 0.474257867010761 ## Epoch: Iteration #400 error is: 0.472067779170087 ## Epoch: Iteration #500 error is: 0.471844181155159 ## Epoch: Iteration #600 error is: 0.471798834134577 ## Epoch: Iteration #700 error is: 0.471783207059971 ## Epoch: Iteration #800 error is: 0.471632929621924 ## Epoch: Iteration #900 error is: 0.47087861882558 ## Epoch: Iteration #1000 error is: 0.470873765976829  df \u0026lt;- data.frame(x = positions[,1], y = positions[,2], influence = user_influences, botness = user_botness) df \u0026lt;- cbind(df, data.frame(botornot = ifelse(df$botness \u0026gt; 0.6, 'Bot', 'Not Bot'))) ggplot(df, aes(x, y, color = influence, shape = botornot, size = botornot)) + geom_point() + scale_shape_manual(values = c(15,1)) + scale_size_manual(values = c(1.5, 1.2)) + scale_color_gradient(low = '#56B1F7', high = '#132B43', trans = 'log10') + theme_void() + labs(size = NULL, shape = NULL) + theme(legend.direction = 'horizontal', legend.position = c(0.8, 0.2), legend.key.size = unit(.3, 'cm'), legend.text = element_text(size = 6), legend.title = element_text(size = 6), legend.spacing = unit(.05, 'cm'))  ","date":1606953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606953600,"objectID":"842675823f109d82050a247e820bb1b0","permalink":"https://www.behavioral-ds.science/theme2_content/user_analysis/","publishdate":"2020-12-03T00:00:00Z","relpermalink":"/theme2_content/user_analysis/","section":"theme2_content","summary":"In this tutorial, we apply two novel tools (BirdSpotter and Evently) for analyzing Twitter users on a COVID-19 retweet dataset.","tags":null,"title":"User Analysis on reshare cascades about COVID-19","type":"theme2_content"},{"authors":["Rohit Ram"],"categories":["Research"],"content":"   Most statistics students will be familiar with the phrase “correlation isn’t causation,” however, this doesn’t feature strongly in the remainder of their educations. To overcome this hurdle, the researchers’ best practice in experimental design is the randomized controlled trial. However, there are only specific experiments that we can perform. For example, to test the whether smoking causes cancer, we can’t force subjects to smoke. ⊕In the 1950s the tobacco companies argued that there could be some confounding factor (a gene) which smokers and lung cancer patients shared. In general, restricting ourselves to experimental studies to determine causation is incredibly limiting (especially for data scientists). We want to make the same causal conclusions from observational studies and those from experimental studies. We can do that by studying causal inference.\nSimpson’s Paradox An example of the importance of understanding causal relationships is given by Simpson’s Paradox (Simpson 1951), which describes a peculiar phenomenon that can present in data sets, where a correlation between two variables is present in one direction but reverses in each stratum of the data. The paradox expressed best through an example: ⊕This appears to suggest that the more someone exercises, the higher their cholesterol is! This is absurd!  Figure 1: The results of an experiment, where x-axis represents how much exercise an individual does in hours, and y-axis represents cholestral measurment for the same individual.  Figure 1 shows a positive correlation in an experiment that measures individuals’ exercise per week and cholesterol. At first glance, this seems absurd, but when we partition the data by another causal variable, this seems reasonable:  Figure 2: The same results as the experiment above, partioned by age  ⊕(Also note, we have fabricated the data, although these relationships are quite plausible) Understanding the full causal story is essential. Without an entire causal narrative, we might recommend inappropriate interventions; for example, a doctor might prescribe less exercise to reduce cholesterol in the case above.\nTo deduce such causal stories, we need to apply the methodology of causal inference.\n Structural Equation Models and Causal Graphs A structural equation model (SEM) is a set of equations representing the relationship between variables. For example, the equations which generated the data from the Simpson’s paradox example, are given as: \\[ \\begin{align*} age \u0026amp;= U_1 \\\\ exercise \u0026amp;= \\frac{1}{13}*age + U_2 \\\\ cholesteral \u0026amp;= -4*exercise + age + U_3 \\end{align*} \\] We can think of \\(U_1\\), \\(U_2\\), and \\(U_3\\) as specific unobserved exogenous variables of an individual, which generate their endogenous variables (something like error terms).\nA causal graph is a DAG which describes the existence of relationships between variables in a model. An edge x -\u0026gt; y represents the relationship x directly causes y. Consequently, causal graphs can represent SEMs: Indeed this graph shows how age confounds the effect of exercise on cholesterol.\n Do-calculus Pearl (1995) outline a method to remove this confounding (and other similar scenarios) using do-calculus. Outlining the specifics of do-calculus is beyond the scope of this blog post (but for interested readers, we suggest (Pearl, Glymour, and Jewell 2016)). In brief, do-calculus introduces the \\(do()\\) operator, which acts as an intervention and fixes a variable to a particular constant. For example, consider a similar binary situation to the Simpson’s paradox example, where exer is a binary variable true if the individual is active, chol is a binary variable true if the individual has high cholesterol, and age is a binary variable true if the individual is over 60.\nbin_simpsons_data \u0026lt;- simpsons_data %\u0026gt;% mutate(age = age \u0026gt; 60) %\u0026gt;% # Binarize the age, so those over 60 are True, and under 60 are False mutate(exer = exercise\u0026gt;mean(exercise)) %\u0026gt;% # Binarize the exercise level, so those above the average are True, and under are False mutate(chol = cholesteral\u0026gt;mean(cholesteral)) # Binarize the cholesteral level, so those above the average are True, and under are False We ask the same experimental question; does exercise reduce cholesterol. A naive approach would be to compute the effect as \\(P(chol | exer = 1) - P(chol | exer = 0)=\\) 0.168, where \\(P(chol | exer)\\) is computed by filtering the data according to exer. Taking this approach, we would erroneously observe that the effect was positive since those who exercise more are also old and more likely to have high cholesterol.\nThe experimental best practice approach would be to perform a randomized controlled trial (RCT). A random selection of individuals are assigned to do a high exercise regiment and the others do a low exercise regiment (regardless of age). The RCT implicitly removes the natural tendency of exercise to vary with age and allows researchers to observe the causal effect of exercise on cholesterol. When using data generated in such a fashion, increases/decreases in the probability of having high cholesterol caused by exercise are given by \\(P_{RCT}(chol | exer = 1) - P_{RCT}(chol | exer = 0)\\). This metric is known as the Average Causal Effect (ACE), sometimes called the Average Treatment Effect. Note that by conditioning on \\(exer=x\\), with data generated by an RCT, researchers are essentially limiting the data used to estimate \\(P_{RCT}(chol | exer = x)\\), to individuals who were forced to do an exercise regiment \\(x\\). The do here represents forcing individuals to take an intervention value, regardless of their natural tendency, and this is captured by the \\(do()\\) operator. In this case, \\(P(chol | do(exer = x)) = P_{RCT}(chol | exer = x)\\), since the data was generated with an RCT. However, RCTs can be prohibitively expensive (both in time and money) and might not be necessary to tease out a causal effect.\nWe would still like to estimate the ACE, \\(P(chol | do(exer = 1)) - P(chol | do(exer = 0))\\), by using data that wasn’t generated from an RCT. By using the \\(do()\\) operator here, we aim to disassociate exer from its natural tendency with age and effectively perform a graph surgery:\nPearl, Glymour, and Jewell (2016) provide an adjustment formula for just this scenario: \\[ P(y|do(x)) = \\sum_z \\frac{P(X=x, Y=y, PA=z)}{P(X=x| PA=z)} \\] where \\(X\\) represents the variable we are acting on, \\(Y\\) the variable we measure results from, and \\(PA\\) the parents of \\(X\\) and \\(Y\\) or more generally any nodes that satisfy the back-door criterion (which we will introduce later). Note this allows us to derive the causal effect, as if we had generated data with an RCT, using only probabilities estimated from data not generated by an RCT.\nAs such we compute our ACE for the binary scenario:\n# The Joint Distribution P(age, exer, chol) i.e. P(x,y,z) p_aec \u0026lt;- bin_simpsons_data %\u0026gt;% count(age, exer, chol) %\u0026gt;% mutate(freq = n/sum(n)) # The Marginal Distribution P(age) i.e. P(z) p_a \u0026lt;- bin_simpsons_data %\u0026gt;% count(age) %\u0026gt;% mutate(freq = n/sum(n)) # The Marginal Distribution P(age, exer) i.e. P(x, z) p_ea \u0026lt;- bin_simpsons_data %\u0026gt;% count(age, exer) %\u0026gt;% mutate(freq = n/sum(n)) # The Conditional Mariginal Distribution P(exer | age) i.e. P(x | z) p_e_a \u0026lt;- p_a %\u0026gt;% right_join(p_ea, by=\u0026quot;age\u0026quot;) %\u0026gt;% mutate(freq = freq.y/freq.x) %\u0026gt;% select(age, exer, freq) # The Intervention Distribution P(chol | do(exer)) i.e. P(y | do(x)) probabilities \u0026lt;- data.table(p_aec %\u0026gt;% left_join(p_e_a, by=c(\u0026quot;age\u0026quot;, \u0026quot;exer\u0026quot;)) %\u0026gt;% mutate(freq = freq.x/freq.y) %\u0026gt;% select(age, exer, chol, freq) %\u0026gt;% filter(chol) # We are only concerned with what cause high cholestral ) # The average causal effect of exer on chol ACE \u0026lt;- sum(probabilities[exer==T, freq]) - sum(probabilities[exer==F, freq])  This procedure leads to a negative ACE of -0.175, which shows the causal effect of going from high to low exercise on the probability of getting high cholesterol.\nA natural question that follows from this example is, under what conditions can we use such adjustments to achieve an identifiable causal effect.\n d-seperation To understand common scenarios where the effect of variable \\(X\\) on \\(Y\\) is identifiable within a causal graph, we must first introduce the concept of d-separation, also known as blocking. A pair of variable \\(X\\) and \\(Y\\) are said to be blocked if they are conditionally independent, given a set of nodes \\(Z\\). There are three graph types, which are essential for blocking:\nIn the chain scenario, \\(X \\sim Y\\) is blocked by conditioning on \\(Z={M}\\). This is sometimes refered to as the mediation scenario, which we will address further in the front-door criterion.\nIn the fork scenario, \\(X \\sim Y\\) is blocked by conditioning on \\(Z={Z}\\). This is sometimes refered to as the confounder scenario, which is the situation in the simpson’s paradox example.\nFinally, in the collider scenario, \\(X \\sim Y\\) is blocked by not conditioning on \\(Z={M}\\). The idea that \\(X\\) and \\(Y\\), which are independent, to begin with, can become conditionally dependant is unintuitive. One way to think about this is that we are sharing information received from $ Y $ with $ X $ through $ M $ when we condition on $ M $. For a more thorough investigation into this phenomenon, refer to (Pearl, Glymour, and Jewell 2016).\nA path is said to be blocked by \\(Z\\) if it contains a chain or fork with its middle node in \\(Z\\) or a collider with its middle node not in \\(Z\\).\nWe are now ready to introduce the main criteria for which we can perform adjustments.\n The Backdoor Definition 1 (The Backdoor Criterion) A set of nodes \\(Z\\), given a DAG \\(G\\) and a pair of nodes \\((X,Y)\\), is said to satisfy the backdoor criterion if no node in \\(Z\\) is a descendant of \\(X\\), and \\(Z\\) blocks all paths between \\(X\\) and \\(Y\\), which contain arrows into \\(X\\).  If there exists are set of nodes why satisfy the backdoor criterion, then the effect of \\(X\\) on \\(Y\\) is identifiable and given by: \\[ P(y|do(x)) = \\sum_z \\frac{P(X=x, Y=y, Z=z)}{P(X=x| Z=z)} \\]\nThe backdoor criterion stops undue influence through the backdoor paths; it leaves direct paths between \\(X\\) and \\(Y\\), and it blocks spurious paths.\nIt is clear that { age } satisfies these conditions to be a backdoor adjustment set in the example above.\n The Front-door There are notably common scenarios where this doesn’t work. For example, consider a constructed causal mediation situation, as follows: In this case we cannot use the backdoor criterion, to detect the effect of smoking on cancer because tar is a descendant of smoking, and there exists no direct link between smoking and cancer. We must use instead the frontdoor criterion:\nDefinition 2 (The Frontdoor Criterion) A set of nodes \\(Z\\), given a DAG \\(G\\) and a pair of nodes \\((X,Y)\\), is said to satisfy the frontdoor criterion if; \\(Z\\) intercepts all direct paths from \\(X\\) to \\(Y\\), all paths between \\(X\\) and \\(Z\\) are blocked, and all backdoor paths between \\(Y\\) and \\(Z\\) are blocked by \\(X\\).  If there exists are set of nodes \\(Z\\) which satisfy the frontdoor criterion, and \\(P(x, z)\u0026gt;0\\), then the effect of \\(X\\) on \\(Y\\) is identifiable and given by: \\[ P(y|do(x)) = \\sum_z P(z|x) \\sum_{x^\\prime} P(y|x^\\prime, z)P(x^\\prime) \\] In our smoking scenario, we see that by adjusting for tar , we can observe the effect of smoking on cancer.\n Conclusion The above briefly outlines a core motivation for studying causal inference and causal stories. We summarise some of the underlying theory of causal inference and show practical methodology through the frontdoor and backdoor criterion for determining causal effects through entirely observational studies.\nThere are notable aspects of causal inference we have omitted from this taster. The most gaping is the lack of an explanation for the powerful tool of counterfactuals. We have only presented binary examples here (aside from our motivating example); however, perhaps the most common and useful causal inference application is to continuous examples using regression with linear models. Ultimately, we decided this was beyond causal inference taster’s scope and were more deserving of their own articles. Again, for the interested reader, we recommend Pearl, Glymour, and Jewell (2016), which adds links to many other resources.\nPearl, Judea. 1995. “Causal Diagrams for Empirical Research.” Biometrika 82 (4): 669–88.  Pearl, Judea, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal Inference in Statistics: A Primer. John Wiley \u0026amp; Sons.  Simpson, Edward H. 1951. “The Interpretation of Interaction in Contingency Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41.    ","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605139200,"objectID":"32ffef7d79751599d38a48f3770fa188","permalink":"https://www.behavioral-ds.science/theme1_content/causal_inference_taster/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/theme1_content/causal_inference_taster/","section":"theme1_content","summary":"An introduction to the core motivation, underlying theory and practical methodology of causal inference through examples.","tags":null,"title":"Causal Inference: A basic taster","type":"theme1_content"},{"authors":["Quyu Kong"],"categories":["project"],"content":"This is an example project description\n","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"33abb12721d64552d355551625cd27ba","permalink":"https://www.behavioral-ds.science/researchproject/project1/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/researchproject/project1/","section":"researchproject","summary":"This is an example project description","tags":["R"],"title":"Epidemic Hawkes: an example project","type":"researchproject"},{"authors":["Quyu Kong"],"categories":["package"],"content":"Introduction This package is designed for simulating and fitting the Hawkes processes and the HawkesN processes with several options of kernel functions. Currently, it assumes univariate processes without background event rates. Prior knowledge about the models is assumed in the following tutorial and please refer to [1] and [2] for details about the models.\nlibrary(evently)  Installation and dependencies Several dependencies (poweRlaw, AMPL, Ipopt) are required for running this package. These dependencies will be installed automatically by R or by following instructions upon package load.\nInstall the package by executing\nif (!require('devtools')) install.packages('devtools') devtools::install_github('behavioral-ds/evently')  Simulating cascades Let’s first simulate 100 event cascades of the Hawkes process with an exponential kernel function (please refer to the Available models for models and their abbreviations in the package) with a given parameter set, . For each simulation, we only simulate until 5 seconds. The resulted cascades are placed in a single list where each cascade is a data.frame.\nset.seed(4) sim_no \u0026lt;- 100 data \u0026lt;- generate_hawkes_event_series(par = c(K = 0.9, theta = 1), model_type = 'EXP', Tmax = 5, sim_no = sim_no) # alternatively, `generate_hawkes_event_series` also accepts a model class object # e.g. # model \u0026lt;- new_hawkes_model(par = c(K = 0.9, theta = 1), model_type = 'EXP') # generate_hawkes_event_series(model = model, Tmax = 5, sim_no = sim_no) head(data[[1]])  ## magnitude time ## 1 1 0.0000000 ## 2 1 0.5941959 ## 3 1 1.4712411 ## 4 1 1.6105430 ## 5 1 1.7855535 ## 6 1 1.8883869  A simulated process is represented by a data.frame where each row is an event. time indicates the event happening time, while magnitude is the event mark information which is always 1 if model_type is an unmarked model. In the context of retweet diffusion cascades, the first row is the original tweet and all following events are its retweets. time records the relative time (in second) of each retweet to the original tweet and magnitude is the follows’ count of the user who retweeted.\nFitting a model on data We can then fit on the cascades simulated in the previous section. After providing the data and model_type, the fitting procedure will spawn 10 AMPL optimization procedures with different parameter inistializations due to the non-convexity of some likelihood functions. Among the 10 fitted model, the one giving the best likelihood value will be returned. To make the fitting procedure faster, we can specify the number of cores to be used for fitting them in parallel.\nfitted_model \u0026lt;- fit_series(data, model_type = 'EXP', observation_time = 5, cores = 10) fitted_model  ## Model: EXP ## No. of cascades: 100 ## init_par ## K 7.92e+00; theta 1.32e+00 ## par ## K 8.51e-01; theta 1.06e+00 ## Neg Log Likelihood: 285.488 ## lower_bound ## K 1.00e-100; theta 1.00e-100 ## upper_bound ## K 1.00e+04; theta 3.00e+02 ## convergence: 0  Available models There are 8 models available so far in this package:\n   Model Abbreviation (model_type) Intensity Function Parameters     Hawkes process with an exponential kernel function EXP  K,theta   Hawkes process with a power-law kernel function PL  K,c,theta   HawkesN process with an exponential kernel function EXPN  K,theta,N   HawkesN process with a power-law kernel function PLN  K,c,theta,N   Marked Hawkes process with an exponential kernel function mEXP  K,beta,theta   Marked Hawkes process with a power-law kernel function mPL  K,beta,c,theta   Marked HawkesN process with an exponential kernel function mEXPN  K,beta,theta,N   Marked HawkesN process with a power-law kernel function mPLN  K,beta,c,theta,N    Acknowledgement The development of this package is supported by the Green Policy grant from the National Security College, Crawford School, ANU.\nLicense Both dataset and code are distributed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. If you require a different license, please contact us at Quyu.Kong@anu.edu.au or Marian-Andrei@rizoiu.eu.\nReference [1] Rizoiu, M. A., Lee, Y., Mishra, S., \u0026amp; Xie, L. (2017, December). Hawkes processes for events in social media. In Frontiers of Multimedia Research (pp. 191-218). Association for Computing Machinery and Morgan \u0026amp; Claypool.\n[2] Rizoiu, M. A., Mishra, S., Kong, Q., Carman, M., \u0026amp; Xie, L. (2018, April). SIR-Hawkes: Linking epidemic models and Hawkes processes to model diffusions in finite populations. In Proceedings of the 2018 World Wide Web Conference (pp. 419-428). International World Wide Web Conferences Steering Committee.\n[3] Mishra, S., Rizoiu, M. A., \u0026amp; Xie, L. (2016, October). Feature driven and point process approaches for popularity prediction. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 1069-1078). ACM.\n[4] Kong, Q., Rizoiu, M. A., \u0026amp; Xie, L. (2019). Modeling Information Cascades with Self-exciting Processes via Generalized Epidemic Models. arXiv preprint arXiv:1910.05451.\n","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"b32db4bbb114e0b6d8e0361c344237f6","permalink":"https://www.behavioral-ds.science/softwaretool/evently/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/softwaretool/evently/","section":"softwaretool","summary":"Introduction This package is designed for simulating and fitting the Hawkes processes and the HawkesN processes with several options of kernel functions. Currently, it assumes univariate processes without background event rates. Prior knowledge about the models is assumed in the following tutorial and please refer to [1] and [2] for details about the models.\nlibrary(evently)  Installation and dependencies Several dependencies (poweRlaw, AMPL, Ipopt) are required for running this package. These dependencies will be installed automatically by R or by following instructions upon package load.","tags":["R"],"title":"evently: simulation, fitting of Hawkes processes","type":"softwaretool"},{"authors":["Quyu Kong"],"categories":["Software"],"content":" Introduction This package is designed for simulating and fitting the Hawkes processes and the HawkesN processes with several options of kernel functions. Currently, it assumes univariate processes without background event rates. Prior knowledge about the models is assumed in the following tutorial and please refer to [1] and [2] for details about the models.\nlibrary(evently)  Installation and dependencies Several dependencies (poweRlaw, AMPL, Ipopt) are required for running this package. These dependencies will be installed automatically by R or by following instructions upon package load.\nInstall the package by executing\nif (!require('devtools')) install.packages('devtools') devtools::install_github('behavioral-ds/evently')  Simulating cascades Let’s first simulate 100 event cascades of the Hawkes process with an exponential kernel function (please refer to the Available models for models and their abbreviations in the package) with a given parameter set, . For each simulation, we only simulate until 5 seconds. The resulted cascades are placed in a single list where each cascade is a data.frame.\nset.seed(4) sim_no \u0026lt;- 100 data \u0026lt;- generate_hawkes_event_series(par = c(K = 0.9, theta = 1), model_type = 'EXP', Tmax = 5, sim_no = sim_no) # alternatively, `generate_hawkes_event_series` also accepts a model class object # e.g. # model \u0026lt;- new_hawkes_model(par = c(K = 0.9, theta = 1), model_type = 'EXP') # generate_hawkes_event_series(model = model, Tmax = 5, sim_no = sim_no) head(data[[1]])  ## magnitude time ## 1 1 0.0000000 ## 2 1 0.5941959 ## 3 1 1.4712411 ## 4 1 1.6105430 ## 5 1 1.7855535 ## 6 1 1.8883869  A simulated process is represented by a data.frame where each row is an event. time indicates the event happening time, while magnitude is the event mark information which is always 1 if model_type is an unmarked model. In the context of retweet diffusion cascades, the first row is the original tweet and all following events are its retweets. time records the relative time (in second) of each retweet to the original tweet and magnitude is the follows’ count of the user who retweeted.\nFitting a model on data We can then fit on the cascades simulated in the previous section. After providing the data and model_type, the fitting procedure will spawn 10 AMPL optimization procedures with different parameter inistializations due to the non-convexity of some likelihood functions. Among the 10 fitted model, the one giving the best likelihood value will be returned. To make the fitting procedure faster, we can specify the number of cores to be used for fitting them in parallel.\nfitted_model \u0026lt;- fit_series(data, model_type = 'EXP', observation_time = 5, cores = 10) fitted_model  ## Model: EXP ## No. of cascades: 100 ## init_par ## K 7.92e+00; theta 1.32e+00 ## par ## K 8.51e-01; theta 1.06e+00 ## Neg Log Likelihood: 285.488 ## lower_bound ## K 1.00e-100; theta 1.00e-100 ## upper_bound ## K 1.00e+04; theta 3.00e+02 ## convergence: 0  Available models There are 8 models available so far in this package:\n   Model Abbreviation (model_type) Intensity Function Parameters     Hawkes process with an exponential kernel function EXP  K,theta   Hawkes process with a power-law kernel function PL  K,c,theta   HawkesN process with an exponential kernel function EXPN  K,theta,N   HawkesN process with a power-law kernel function PLN  K,c,theta,N   Marked Hawkes process with an exponential kernel function mEXP  K,beta,theta   Marked Hawkes process with a power-law kernel function mPL  K,beta,c,theta   Marked HawkesN process with an exponential kernel function mEXPN  K,beta,theta,N   Marked HawkesN process with a power-law kernel function mPLN  K,beta,c,theta,N    Acknowledgement The development of this package is supported by the Green Policy grant from the National Security College, Crawford School, ANU.\nLicense Both dataset and code are distributed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. If you require a different license, please contact us at Quyu.Kong@anu.edu.au or Marian-Andrei@rizoiu.eu.\nReference [1] Rizoiu, M. A., Lee, Y., Mishra, S., \u0026amp; Xie, L. (2017, December). Hawkes processes for events in social media. In Frontiers of Multimedia Research (pp. 191-218). Association for Computing Machinery and Morgan \u0026amp; Claypool.\n[2] Rizoiu, M. A., Mishra, S., Kong, Q., Carman, M., \u0026amp; Xie, L. (2018, April). SIR-Hawkes: Linking epidemic models and Hawkes processes to model diffusions in finite populations. In Proceedings of the 2018 World Wide Web Conference (pp. 419-428). International World Wide Web Conferences Steering Committee.\n[3] Mishra, S., Rizoiu, M. A., \u0026amp; Xie, L. (2016, October). Feature driven and point process approaches for popularity prediction. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 1069-1078). ACM.\n[4] Kong, Q., Rizoiu, M. A., \u0026amp; Xie, L. (2019). Modeling Information Cascades with Self-exciting Processes via Generalized Epidemic Models. arXiv preprint arXiv:1910.05451.\n","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"1f20e4c8d4a01bccf09b080c3e76b409","permalink":"https://www.behavioral-ds.science/theme1_content/evently/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/theme1_content/evently/","section":"theme1_content","summary":"We introduce `evently`, an R package designed for simulating and fitting the Hawkes processes and the HawkesN processes.","tags":["R"],"title":"evently: simulation, fitting of Hawkes processes","type":"theme1_content"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://www.behavioral-ds.science/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9eb50f9088083bebcb7e4cf99e22b9ed","permalink":"https://www.behavioral-ds.science/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"","tags":null,"title":"News","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://www.behavioral-ds.science/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"my_widget_page"},{"authors":null,"categories":null,"content":"  \n           , , ,  ,                    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40ea366a28f9524de71378c3212c5489","permalink":"https://www.behavioral-ds.science/publication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/","section":"","summary":"  \n           , , ,  ,                    ","tags":null,"title":"Publications","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6da302e29c14c780e10955c05f7c11ad","permalink":"https://www.behavioral-ds.science/reading/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reading/","section":"","summary":"","tags":null,"title":"Reading","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://www.behavioral-ds.science/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research","type":"my_widget_page"}]